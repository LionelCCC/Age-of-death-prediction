{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepsurv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers > /dev/null 2>&1\n",
    "!pip install xgboost > /dev/null 2>&1\n",
    "!pip install scikit-learn==1.4.2 scikit-survival==0.23.1 > /dev/null 2>&1\n",
    "!pip install torchtuples > /dev/null 2>&1\n",
    "!pip install pycox > /dev/null 2>&1\n",
    "!pip install numpy==1.21.5  > /dev/null 2>&1\n",
    "!pip install interpret-core  > /dev/null 2>&1\n",
    "!pip install lightgbm > /dev/null 2>&1\n",
    "!pip install shap > /dev/null 2>&1\n",
    "!pip install lifelines pycox > /dev/null 2>&1\n",
    "!pip install pycountry > /dev/null 2>&1\n",
    "!pip install -U sentence-transformers xgboost scikit-learn==1.4.2 scikit-survival==0.23.1 torchtuples pycox numpy==1.21.5 interpret-core lightgbm shap lifelines pycox pycountry > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtuples as tt\n",
    "import kagglehub\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lifelines import CoxPHFitter\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from pycox.models.cox import CoxPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pandas Display Options to try to force full output ---\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', 50) # Adjust as needed\n",
    "pd.set_option('display.width', 1000)     # Adjust as needed\n",
    "pd.set_option('display.max_colwidth', None) # Show full column content\n",
    "pd.set_option('display.float_format', '{:.4f}'.format) # Optional: format floats\n",
    "\n",
    "feature_list = ['stress_score', 'avg_bmi', 'smoking_prev', 'global_life_exp'] # Define feature_list globally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Life Expectancy Sample:\n",
      "       Country  Year      Status  Life expectancy   Adult Mortality  \\\n",
      "0  Afghanistan  2015  Developing              65.0            263.0   \n",
      "1  Afghanistan  2014  Developing              59.9            271.0   \n",
      "2  Afghanistan  2013  Developing              59.9            268.0   \n",
      "3  Afghanistan  2012  Developing              59.5            272.0   \n",
      "4  Afghanistan  2011  Developing              59.2            275.0   \n",
      "\n",
      "   infant deaths  Alcohol  percentage expenditure  Hepatitis B  Measles   ...  \\\n",
      "0             62     0.01               71.279624         65.0      1154  ...   \n",
      "1             64     0.01               73.523582         62.0       492  ...   \n",
      "2             66     0.01               73.219243         64.0       430  ...   \n",
      "3             69     0.01               78.184215         67.0      2787  ...   \n",
      "4             71     0.01                7.097109         68.0      3013  ...   \n",
      "\n",
      "   Polio  Total expenditure  Diphtheria    HIV/AIDS         GDP  Population  \\\n",
      "0    6.0               8.16         65.0        0.1  584.259210  33736494.0   \n",
      "1   58.0               8.18         62.0        0.1  612.696514    327582.0   \n",
      "2   62.0               8.13         64.0        0.1  631.744976  31731688.0   \n",
      "3   67.0               8.52         67.0        0.1  669.959000   3696958.0   \n",
      "4   68.0               7.87         68.0        0.1   63.537231   2978599.0   \n",
      "\n",
      "    thinness  1-19 years   thinness 5-9 years  \\\n",
      "0                   17.2                 17.3   \n",
      "1                   17.5                 17.5   \n",
      "2                   17.7                 17.7   \n",
      "3                   17.9                 18.0   \n",
      "4                   18.2                 18.2   \n",
      "\n",
      "   Income composition of resources  Schooling  \n",
      "0                            0.479       10.1  \n",
      "1                            0.476       10.0  \n",
      "2                            0.470        9.9  \n",
      "3                            0.463        9.8  \n",
      "4                            0.454        9.5  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Heart Failure Sample:\n",
      "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
      "0   40   M           ATA        140          289          0     Normal    172   \n",
      "1   49   F           NAP        160          180          0     Normal    156   \n",
      "2   37   M           ATA        130          283          0         ST     98   \n",
      "3   48   F           ASY        138          214          0     Normal    108   \n",
      "4   54   M           NAP        150          195          0     Normal    122   \n",
      "\n",
      "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
      "0              N      0.0       Up             0  \n",
      "1              N      1.0     Flat             1  \n",
      "2              N      0.0       Up             0  \n",
      "3              Y      1.5     Flat             1  \n",
      "4              N      0.0       Up             0  \n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Age Dataset Sample:\n",
      "     Id                     Name  \\\n",
      "0   Q23        George Washington   \n",
      "1   Q42            Douglas Adams   \n",
      "2   Q91          Abraham Lincoln   \n",
      "3  Q254  Wolfgang Amadeus Mozart   \n",
      "4  Q255     Ludwig van Beethoven   \n",
      "\n",
      "                                 Short description Gender  \\\n",
      "0   1st president of the United States (1732–1799)   Male   \n",
      "1                      English writer and humorist   Male   \n",
      "2  16th president of the United States (1809-1865)   Male   \n",
      "3        Austrian composer of the Classical period   Male   \n",
      "4           German classical and romantic composer   Male   \n",
      "\n",
      "                                             Country  Occupation  Birth year  \\\n",
      "0  United States of America; Kingdom of Great Bri...  Politician        1732   \n",
      "1                                     United Kingdom      Artist        1952   \n",
      "2                           United States of America  Politician        1809   \n",
      "3    Archduchy of Austria; Archbishopric of Salzburg      Artist        1756   \n",
      "4                 Holy Roman Empire; Austrian Empire      Artist        1770   \n",
      "\n",
      "   Death year Manner of death  Age of death  \n",
      "0      1799.0  natural causes          67.0  \n",
      "1      2001.0  natural causes          49.0  \n",
      "2      1865.0        homicide          56.0  \n",
      "3      1791.0             NaN          35.0  \n",
      "4      1827.0             NaN          57.0  \n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "World Important Events Sample:\n",
      "   Sl. No                      Name of Incident     Date    Month     Year  \\\n",
      "0       1  Indus Valley Civilization Flourishes  Unknown  Unknown  2600 BC   \n",
      "1       2               Battle of the Ten Kings  Unknown  Unknown  1400 BC   \n",
      "2       6  Establishment of the Delhi Sultanate  Unknown  Unknown     1206   \n",
      "3       7                     Battle of Panipat       21    April     1526   \n",
      "4       8          Establishment of British Raj        1      May     1858   \n",
      "\n",
      "  Country Type of Event    Place Name  \\\n",
      "0   India  Civilization  Indus Valley   \n",
      "1   India        Battle        Punjab   \n",
      "2   India     Political         Delhi   \n",
      "3   India        Battle       Panipat   \n",
      "4   India      Colonial   Whole India   \n",
      "\n",
      "                                              Impact  \\\n",
      "0  Development of one of the world's earliest urb...   \n",
      "1  Rigvedic tribes consolidated their control ove...   \n",
      "2          Muslim rule established in parts of India   \n",
      "3           Foundation of the Mughal Empire in India   \n",
      "4        Start of direct British governance in India   \n",
      "\n",
      "                       Affected Population Important Person/Group Responsible  \\\n",
      "0                        Local inhabitants                Indus Valley people   \n",
      "1                          Rigvedic tribes                              Sudas   \n",
      "2  People of Delhi and surrounding regions      QutbUnknownudUnknowndin Aibak   \n",
      "3                 Northern Indian kingdoms                              Babur   \n",
      "4                      Indian subcontinent  British East India Company/Empire   \n",
      "\n",
      "    Outcome  \n",
      "0  Positive  \n",
      "1  Positive  \n",
      "2     Mixed  \n",
      "3     Mixed  \n",
      "4  Negative  \n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Historical Plane Crashes Sample:\n",
      "                 date     time                            location  \\\n",
      "0  September 17, 1908    17:18                 Fort Myer, Virginia   \n",
      "1  September 07, 1909        ?             Juvisy-sur-Orge, France   \n",
      "2       July 12, 1912    06:30           Atlantic City, New Jersey   \n",
      "3     August 06, 1913        ?  Victoria, British Columbia, Canada   \n",
      "4  September 09, 1913  c 18:30                  Over the North Sea   \n",
      "\n",
      "                 operator flight_no          route                 ac_type  \\\n",
      "0    Military - U.S. Army         ?  Demonstration        Wright Flyer III   \n",
      "1                       ?         ?       Air show          Wright Byplane   \n",
      "2    Military - U.S. Navy         ?    Test flight               Dirigible   \n",
      "3                 Private         ?              ?        Curtiss seaplane   \n",
      "4  Military - German Navy         ?              ?  Zeppelin L-1 (airship)   \n",
      "\n",
      "  registration cn_ln                       aboard  \\\n",
      "0            ?     1   2   (passengers:1  crew:1)   \n",
      "1          SC1     ?   1   (passengers:0  crew:1)   \n",
      "2            ?     ?   5   (passengers:0  crew:5)   \n",
      "3            ?     ?   1   (passengers:0  crew:1)   \n",
      "4            ?     ?  20   (passengers:?  crew:?)   \n",
      "\n",
      "                    fatalities ground  \\\n",
      "0   1   (passengers:1  crew:0)      0   \n",
      "1   1   (passengers:0  crew:0)      0   \n",
      "2   5   (passengers:0  crew:5)      0   \n",
      "3   1   (passengers:0  crew:1)      0   \n",
      "4  14   (passengers:?  crew:?)      0   \n",
      "\n",
      "                                             summary  \n",
      "0  During a demonstration flight, a U.S. Army fly...  \n",
      "1  Eugene Lefebvre was the first pilot to ever be...  \n",
      "2  First U.S. dirigible Akron exploded just offsh...  \n",
      "3  The first fatal airplane accident in Canada oc...  \n",
      "4  The airship flew into a thunderstorm and encou...  \n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Global Life Expectancy Historical Dataset Sample:\n",
      "           Country Name Country Code    1960    1961    1962    1963    1964  \\\n",
      "0                 Aruba          ABW  65.662  66.074  66.444  66.787  67.113   \n",
      "1           Afghanistan          AFG  32.446  32.962  33.471  33.971  34.463   \n",
      "2                Angola          AGO  37.524  37.811  38.113  38.430  38.760   \n",
      "3               Albania          ALB  62.283  63.301  64.190  64.914  65.463   \n",
      "4  United Arab Emirates          ARE  51.537  52.560  53.573  54.572  55.555   \n",
      "\n",
      "     1965    1966    1967  ...    2011    2012    2013    2014    2015  \\\n",
      "0  67.435  67.762  68.095  ...  75.158  75.299  75.441  75.583  75.725   \n",
      "1  34.948  35.430  35.914  ...  61.553  62.054  62.525  62.966  63.377   \n",
      "2  39.102  39.454  39.813  ...  56.330  57.236  58.054  58.776  59.398   \n",
      "3  65.850  66.110  66.304  ...  76.914  77.252  77.554  77.813  78.025   \n",
      "4  56.523  57.482  58.432  ...  76.521  76.711  76.903  77.095  77.285   \n",
      "\n",
      "     2016    2017    2018    2019    2020  \n",
      "0  75.868  76.010  76.152  76.293  76.434  \n",
      "1  63.763  64.130  64.486  64.833  65.173  \n",
      "2  59.925  60.379  60.782  61.147  61.487  \n",
      "3  78.194  78.333  78.458  78.573  78.686  \n",
      "4  77.470  77.647  77.814  77.972  78.120  \n",
      "\n",
      "[5 rows x 63 columns]\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Death Rates United States Dataset Sample:\n",
      "                 INDICATOR                                               UNIT  \\\n",
      "0  Death rates for suicide  Deaths per 100,000 resident population, age-ad...   \n",
      "1  Death rates for suicide  Deaths per 100,000 resident population, age-ad...   \n",
      "2  Death rates for suicide  Deaths per 100,000 resident population, age-ad...   \n",
      "3  Death rates for suicide  Deaths per 100,000 resident population, age-ad...   \n",
      "4  Death rates for suicide  Deaths per 100,000 resident population, age-ad...   \n",
      "\n",
      "   UNIT_NUM STUB_NAME  STUB_NAME_NUM   STUB_LABEL  STUB_LABEL_NUM  YEAR  \\\n",
      "0         1     Total              0  All persons             0.0  1950   \n",
      "1         1     Total              0  All persons             0.0  1960   \n",
      "2         1     Total              0  All persons             0.0  1970   \n",
      "3         1     Total              0  All persons             0.0  1980   \n",
      "4         1     Total              0  All persons             0.0  1981   \n",
      "\n",
      "   YEAR_NUM       AGE  AGE_NUM  ESTIMATE  \n",
      "0         1  All ages      0.0      13.2  \n",
      "1         2  All ages      0.0      12.5  \n",
      "2         3  All ages      0.0      13.1  \n",
      "3         4  All ages      0.0      12.2  \n",
      "4         5  All ages      0.0      12.3  \n"
     ]
    }
   ],
   "source": [
    "# Life Expectancy dataset\n",
    "life_exp_path = kagglehub.dataset_download(\"kumarajarshi/life-expectancy-who\")\n",
    "life_exp_file = os.path.join(life_exp_path, \"Life Expectancy Data.csv\")\n",
    "life_exp_df = pd.read_csv(life_exp_file)\n",
    "print(\"Life Expectancy Sample:\")\n",
    "print(life_exp_df.head())\n",
    "\n",
    "# Heart Failure dataset (not used in LightGBM, but kept for context)\n",
    "heart_path = kagglehub.dataset_download(\"fedesoriano/heart-failure-prediction\")\n",
    "heart_file = os.path.join(heart_path, \"heart.csv\")\n",
    "heart_df = pd.read_csv(heart_file)\n",
    "print(\"Heart Failure Sample:\")\n",
    "print(heart_df.head())\n",
    "\n",
    "# Age Dataset\n",
    "age_path = kagglehub.dataset_download(\"imoore/age-dataset\")\n",
    "age_file = os.path.join(age_path, \"AgeDataset-V1.csv\")\n",
    "age_df = pd.read_csv(age_file)\n",
    "print(\"Age Dataset Sample:\")\n",
    "print(age_df.head())\n",
    "\n",
    "# World important events Dataset\n",
    "events_path = kagglehub.dataset_download(\"saketk511/world-important-events-ancient-to-modern\")\n",
    "events_file = os.path.join(events_path, \"World Important Dates.csv\")\n",
    "events_df = pd.read_csv(events_file)\n",
    "print(\"World Important Events Sample:\")\n",
    "print(events_df.head())\n",
    "\n",
    "# Plane Crash Dataset\n",
    "plane_crash_path = kagglehub.dataset_download(\"nguyenhoc/plane-crash\")\n",
    "plane_crash_file = os.path.join(plane_crash_path, \"planecrashinfo_20181121001952.csv\")  \n",
    "planes_df = pd.read_csv(plane_crash_file)\n",
    "print(\"Historical Plane Crashes Sample:\")\n",
    "print(planes_df.head())\n",
    "\n",
    "# Gloabl Life Expectancy dataset\n",
    "global_le_path = kagglehub.dataset_download(\"hasibalmuzdadid/global-life-expectancy-historical-dataset\")\n",
    "global_le_file = os.path.join(global_le_path, \"global life expectancy dataset.csv\")\n",
    "global_le_df = pd.read_csv(global_le_file)\n",
    "print(\"Global Life Expectancy Historical Dataset Sample:\")\n",
    "print(global_le_df.head())\n",
    "\n",
    "# US death rate Dataset\n",
    "death_rates_path = kagglehub.dataset_download(\"melissamonfared/death-rates-united-states\")\n",
    "death_rates_file = os.path.join(death_rates_path, \"Death_rates.csv\")\n",
    "death_rates_df = pd.read_csv(death_rates_file)\n",
    "print(\"Death Rates United States Dataset Sample:\")\n",
    "print(death_rates_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_feature_engineering(df, life_exp_df, global_le_df, death_rates_df):\n",
    "    \"\"\"\n",
    "    Enhanced feature engineering with comprehensive NaN debugging prints.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Enhanced Feature Engineering ---\")\n",
    "\n",
    "    # -------- Validate Input Columns --------\n",
    "    required_columns = {'Country', 'Gender', 'Occupation', 'Birth year',\n",
    "                        'Death year', 'Age of death'}\n",
    "    missing = required_columns - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns: {missing}\")\n",
    "    print(\"✅ Input columns validated.\")\n",
    "\n",
    "    # -------- Set Observation Year --------\n",
    "    current_year = 2019\n",
    "\n",
    "    # -------- Handle Initial NaNs in 'Country' --------\n",
    "    print(\"\\n🔍 Checking and handling initial NaNs in 'Country'...\")\n",
    "    initial_nan_count_country = df['Country'].isnull().sum()\n",
    "    print(f\"   Initial NaNs in Country: {initial_nan_count_country}\")\n",
    "\n",
    "    # Impute NaNs in 'Country' with 'Unknown Country' *before* cleaning\n",
    "    df['Country'] = df['Country'].fillna('Unknown Country')\n",
    "    print(\"   ✅ NaNs in 'Country' imputed with 'Unknown Country'.\")\n",
    "    nan_count_after_imputation = df['Country'].isnull().sum()\n",
    "    print(f\"   NaNs in Country after imputation: {nan_count_after_imputation} (Should be 0)\")\n",
    "\n",
    "\n",
    "    # -------- Basic Cleaning --------\n",
    "    print(\"\\n🧹 Basic Cleaning...\")\n",
    "    df['Country'] = df['Country'].str.split(';').str[0].str.strip()\n",
    "    df['Gender'] = np.where(df['Gender'] == 'Male', 1,\n",
    "                          np.where(df['Gender'] == 'Female', 0, 0.5))\n",
    "    print(\"   ✅ Country and Gender cleaned.\")\n",
    "    print(f\"   Sample age_df countries after cleaning: {df['Country'].unique()[:20]}\")\n",
    "    print(f\"   Sample global_le_df countries: {global_le_df['Country Name'].unique()[:20]}\")\n",
    "    print(\"\\n🔍 Sample global_le_agg countries BEFORE merge:\")  # Inspect global_le_agg countries before merge\n",
    "    global_le_melted = global_le_df.melt(\n",
    "        id_vars=['Country Name', 'Country Code'],\n",
    "        value_vars=[str(y) for y in range(1960, current_year+1)],\n",
    "        var_name='Year',\n",
    "        value_name='Life_Exp_Value'\n",
    "    )\n",
    "\n",
    "    global_le_agg = (\n",
    "        global_le_melted\n",
    "        .sort_values(['Country Name', 'Year'], ascending=[True, False])\n",
    "        .groupby('Country Name')\n",
    "        ['Life_Exp_Value']\n",
    "        .first()\n",
    "        .reset_index()\n",
    "        .rename(columns={'Country Name': 'Country'})\n",
    "    )\n",
    "    print(f\"   Unique countries in global_le_agg: {global_le_agg['Country'].unique()[:20]}\") # Sample of unique countries in global_le_agg\n",
    "\n",
    "\n",
    "    # Check for NaNs after basic cleaning (should be minimal now)\n",
    "    print(\"🔍 Checking for NaNs after basic cleaning:\")\n",
    "    print(f\"   NaNs in Country: {df['Country'].isnull().sum()}\") # Expecting 0 or very few\n",
    "    print(f\"   NaNs in Gender: {df['Gender'].isnull().sum()}\")\n",
    "\n",
    "\n",
    "    # -------- Clinical Features (Rest remains the same) --------\n",
    "    print(\"\\n🧬 Engineering Clinical Features...\")\n",
    "    stress_map = {'Politician': 9, 'Military personnel': 8, 'Journalist': 7,\n",
    "                  'Businessperson': 6, 'Artist': 5, 'Teacher': 4,\n",
    "                  'Researcher': 3, 'Other': 5, 'Unknown': 5}\n",
    "    df['stress_score'] = df['Occupation'].map(stress_map).fillna(5).astype('float32') / 9.0\n",
    "    print(\"   ✅ Stress score engineered.\")\n",
    "\n",
    "    life_exp_df[' BMI '] = pd.to_numeric(life_exp_df[' BMI '], errors='coerce')\n",
    "    country_bmi = life_exp_df.groupby('Country')[' BMI '].median().to_dict()\n",
    "    df['avg_bmi'] = df['Country'].map(country_bmi).fillna(25).astype('float32')\n",
    "    print(\"   ✅ Avg BMI engineered.\")\n",
    "\n",
    "    df['smoking_prev'] = (1 / (1 + np.exp((df['Birth year'] - 1950) / 10))).astype('float32')\n",
    "    df['smoking_prev'] = np.clip(df['smoking_prev'], 0.1, 0.6)\n",
    "    print(\"   ✅ Smoking prevalence engineered.\")\n",
    "\n",
    "    # Check for NaNs after clinical feature engineering\n",
    "    print(\"🔍 Checking for NaNs after clinical features:\")\n",
    "    print(f\"   NaNs in stress_score: {df['stress_score'].isnull().sum()}\")\n",
    "    print(f\"   NaNs in avg_bmi: {df['avg_bmi'].isnull().sum()}\")\n",
    "    print(f\"   NaNs in smoking_prev: {df['smoking_prev'].isnull().sum()}\")\n",
    "\n",
    "\n",
    "    # -------- Country-Level Features --------\n",
    "    print(\"\\n🌍 Engineering Country-Level Features...\")\n",
    "    print(\"\\nSample global_le_agg before merge:\") # Inspect global_le_agg\n",
    "    global_le_melted = global_le_df.melt(\n",
    "        id_vars=['Country Name', 'Country Code'],\n",
    "        value_vars=[str(y) for y in range(1960, current_year+1)],\n",
    "        var_name='Year',\n",
    "        value_name='Life_Exp_Value'\n",
    "    )\n",
    "\n",
    "    global_le_agg = (\n",
    "        global_le_melted\n",
    "        .sort_values(['Country Name', 'Year'], ascending=[True, False])\n",
    "        .groupby('Country Name')\n",
    "        ['Life_Exp_Value']\n",
    "        .first()\n",
    "        .reset_index()\n",
    "        .rename(columns={'Country Name': 'Country'})\n",
    "    )\n",
    "    print(global_le_agg.head()) # Print head of global_le_agg\n",
    "    print(f\"   NaNs in global_le_agg['Country']: {global_le_agg['Country'].isnull().sum()}\") # Check NaNs in global_le_agg['Country']\n",
    "    print(f\"   NaNs in global_le_agg['Life_Exp_Value']: {global_le_agg['Life_Exp_Value'].isnull().sum()}\") # Check NaNs in global_le_agg['Life_Exp_Value']\n",
    "\n",
    "\n",
    "    df = df.merge(global_le_agg, on='Country', how='left')\n",
    "    df['global_life_exp'] = df['Life_Exp_Value'].fillna(df['Life_Exp_Value'].median()) #median imputation to avoid NaN\n",
    "    print(\"   ✅ Global Life Expectancy engineered.\")\n",
    "\n",
    "    # Check for NaNs after country-level feature engineering\n",
    "    print(\"🔍 Checking for NaNs after country-level features:\")\n",
    "    print(f\"   NaNs in global_life_exp: {df['global_life_exp'].isnull().sum()}\")\n",
    "\n",
    "\n",
    "    # -------- Survival Data Setup --------\n",
    "    print(\"\\n⏳ Survival Data Setup...\")\n",
    "    current_year = 2019 # Re-declare to ensure it's available in this scope if needed\n",
    "\n",
    "    df['censored'] = (df['Death year'] > current_year).astype(int)\n",
    "\n",
    "    # Revised 'T' calculation to handle missing 'Age of death' robustly\n",
    "    df['T'] = np.where(\n",
    "        df['censored'] == 1, # If censored (still alive as of current_year)\n",
    "        current_year - df['Birth year'], # T = time until observation (censoring time)\n",
    "        df['Age of death'] # Else (event observed), T = Age of death\n",
    "    )\n",
    "\n",
    "    # Handle potential NaNs in 'T' *after* the calculation, specifically for cases where Age of death is missing for non-censored cases.\n",
    "    # In such cases, we might default to a large value or impute based on other information if available, or drop the row if no imputation is reasonable.\n",
    "    # For now, let's impute with a plausible value (e.g., median age of death for non-censored individuals) or simply drop the row for simplicity.\n",
    "    median_age_death_non_censored = df[df['censored'] == 0]['Age of death'].median()\n",
    "    df['T'] = df['T'].fillna(median_age_death_non_censored).clip(0, 120) # Impute and clip\n",
    "\n",
    "    print(\"   ✅ Survival time (T) and censoring engineered.\")\n",
    "\n",
    "    # Final check for NaNs in critical columns\n",
    "    print(\"\\n🔍 Final NaN check before returning:\")\n",
    "    print(f\"   NaNs in censored: {df['censored'].isnull().sum()}\")\n",
    "    print(f\"   NaNs in T: {df['T'].isnull().sum()}\")\n",
    "    print(\"\\nSample processed_batch after feature engineering:\") # Inspect processed_batch\n",
    "    print(df.head())\n",
    "    print(f\"   NaNs in processed_batch after FE: {df[feature_list + ['T', 'censored']].isnull().sum().sum()}\") # Total NaNs in features and targets after FE\n",
    "\n",
    "\n",
    "    print(\"✅ Enhanced feature engineering completed.\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSurv(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepSurv, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deepsurv_model(df):\n",
    "    \"\"\"\n",
    "    Train and evaluate DeepSurv model with NaN debugging, focusing on 'T'.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Train DeepSurv Model ---\")\n",
    "\n",
    "    feature_list = ['stress_score', 'avg_bmi', 'smoking_prev', 'global_life_exp']\n",
    "\n",
    "    # Ensure all required features exist\n",
    "    missing_features = set(feature_list) - set(df.columns)\n",
    "    if missing_features:\n",
    "        raise KeyError(f\"Missing required features: {missing_features}. Available: {df.columns.tolist()}\")\n",
    "    print(\"✅ Required features validated.\")\n",
    "\n",
    "    # Extract Data\n",
    "    X = df[feature_list].values.astype('float32')\n",
    "    durations = df['T'].values.astype('float32')\n",
    "    events = df['censored'].values.astype('int')  # Ensure events are 0 or 1\n",
    "\n",
    "    # Debugging: Check Data Before Training - NaN Check!\n",
    "    print(\"\\n📊 Checking Data Before Training DeepSurv (Detailed NaN Check):\")\n",
    "    print(f\"Feature Shape: {X.shape}, Durations Shape: {durations.shape}, Events Shape: {events.shape}\")\n",
    "    print(\"NaN counts per column BEFORE training:\")\n",
    "    nan_counts = df[feature_list + ['T', 'censored']].isnull().sum() # NaN count for each feature and target\n",
    "    print(nan_counts) # Print NaN counts per column\n",
    "    print(f\"Total NaNs in Features and Targets BEFORE training: {nan_counts.sum()}\") # Total NaNs in features and targets\n",
    "    print(f\"Total NaNs in Features (X): {np.isnan(X).sum()}\")\n",
    "    print(f\"NaNs in Durations: {np.isnan(durations).sum()}\")\n",
    "    print(f\"NaNs in Events: {np.isnan(events).sum()}\")\n",
    "    print(f\"Sample Features:\\n{X[:5]}\")\n",
    "    print(f\"Sample Durations: {durations[:5]}\")\n",
    "    print(f\"Sample Events: {events[:5]}\")\n",
    "    print(f\"Unique event values (should be 0 or 1): {np.unique(events)}\")\n",
    "\n",
    "    # --- DEBUGGING: FIND ROW WITH NaN in 'T' ---\n",
    "    nan_T_mask = df['T'].isnull()\n",
    "    if nan_T_mask.any():\n",
    "        print(\"\\n🚨🚨🚨 NaN DETECTED in 'T' column! 🚨🚨🚨\")\n",
    "        nan_T_row_index = df[nan_T_mask].index[0] # Get index of first NaN row\n",
    "        print(f\"Index of row with NaN in 'T': {nan_T_row_index}\")\n",
    "        print(\"Problematic row data:\")\n",
    "        print(df.loc[[nan_T_row_index]]) # Print the entire row\n",
    "    else:\n",
    "        print(\"\\n✅ No NaNs detected in 'T' column (so far in NaN check).\")\n",
    "\n",
    "\n",
    "    if np.isnan(X).any() or np.isnan(durations).any() or np.isnan(events).any():\n",
    "        raise ValueError(\"NaNs detected in input data, cannot proceed with training.\")\n",
    "    print(\"✅ No NaNs in input data for training.\")\n",
    "\n",
    "\n",
    "    # Train-Test Split (Rest of the function remains unchanged)\n",
    "    X_train, X_val, durations_train, durations_val, events_train, events_val = train_test_split(\n",
    "        X, durations, events, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(\"   ✅ Data split into training and validation sets.\")\n",
    "\n",
    "\n",
    "    # Normalize Features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    print(\"   ✅ Features normalized using StandardScaler.\")\n",
    "    print(f\"NaNs in Scaled X_train: {np.isnan(X_train).sum()}\")\n",
    "    print(f\"NaNs in Scaled X_val: {np.isnan(X_val).sum()}\")\n",
    "    if np.isnan(X_train).any() or np.isnan(X_val).any():\n",
    "        raise ValueError(\"NaNs detected in scaled data, check feature engineering and scaling.\")\n",
    "    print(\"✅ No NaNs in scaled data.\")\n",
    "\n",
    "\n",
    "    # DeepSurv Model Setup\n",
    "    in_features = X_train.shape[1]\n",
    "    num_nodes = [64, 64]  # Two hidden layers\n",
    "    out_features = 1\n",
    "    batch_norm = True\n",
    "    dropout = 0.1\n",
    "\n",
    "    net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout)\n",
    "    model = models.CoxPH(net, tt.optim.Adam)\n",
    "\n",
    "    print(\"\\n🚀 Training DeepSurv Model...\")\n",
    "    print(f\"✅ DeepSurv Model Initialized with {in_features} input features.\")\n",
    "    print(f\"Hidden Layers: {num_nodes}, Dropout: {dropout}\")\n",
    "\n",
    "# ✅ Fix: Correct Loss Logging and Exception Handling\n",
    "    class LossLogger(tt.callbacks.Callback):\n",
    "        def on_epoch_end(self):\n",
    "            if hasattr(self.model, 'log') and self.model.log:\n",
    "                try:\n",
    "                    log_entry = self.model.log[-1] # Get the last log entry\n",
    "\n",
    "                    if isinstance(log_entry, dict): # Check if it's a dictionary as we expect\n",
    "                        loss = log_entry.get('loss') # Try to get 'loss' from dictionary\n",
    "                    elif isinstance(log_entry, float) or isinstance(log_entry, int): # Or maybe it's just the loss value directly?\n",
    "                        loss = log_entry\n",
    "                    else: # If it's neither, log the type for debugging\n",
    "                        loss = None\n",
    "                        print(f\"⚠️ Unexpected log entry type: {type(log_entry)}, log entry: {log_entry}\")\n",
    "\n",
    "\n",
    "                    if loss is not None:\n",
    "                        print(f\"Epoch {len(self.model.log)}, Loss: {loss:.5f}\")\n",
    "                    else:\n",
    "                        print(f\"Epoch {len(self.model.log)}, Loss not found in log entry (type: {type(log_entry)}): {log_entry}\") # More info\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Loss logging failed with error: {e}, continuing training... (Error details: {e})\") # More detailed error\n",
    "\n",
    "\n",
    "    # Train Model\n",
    "    callbacks = [LossLogger()]\n",
    "    epochs = 100 # Reduced for testing, increase later if needed\n",
    "    batch_size = 256\n",
    "    verbose = True # Keep verbose for detailed output\n",
    "\n",
    "    print(f\"Training parameters: Epochs={epochs}, Batch Size={batch_size}, Verbose={verbose}\")\n",
    "\n",
    "    model.fit(\n",
    "        X_train, (durations_train, events_train),\n",
    "        batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
    "        val_data=(X_val, (durations_val, events_val)),\n",
    "        callbacks=callbacks,\n",
    "        num_workers=0 # set num_workers to 0 for debugging purposes\n",
    "    )\n",
    "    print(\"✅ DeepSurv model training completed.\")\n",
    "\n",
    "\n",
    "    # Compute Baseline Hazards\n",
    "    print(\"\\n📊 Computing Baseline Hazards...\")\n",
    "    model.compute_baseline_hazards()\n",
    "    print(\"✅ Baseline hazards computed.\")\n",
    "\n",
    "\n",
    "    # Get Predictions\n",
    "    print(\"\\n🔮 Making Survival Predictions...\")\n",
    "    surv = model.predict_surv_df(X_val)\n",
    "\n",
    "\n",
    "    # Check for NaNs in Survival Predictions AGAIN after training\n",
    "    if np.isnan(surv.values).any():\n",
    "        print(\"\\n❌ WARNING: Survival Predictions STILL Contain NaN Values AFTER TRAINING!\")\n",
    "        nan_count_in_surv = np.isnan(surv.values).sum()\n",
    "        print(f\"   Number of NaNs in Survival Predictions: {nan_count_in_surv}\")\n",
    "    else:\n",
    "        print(\"\\n✅ No NaN values in survival predictions.\")\n",
    "\n",
    "    # Evaluate Model\n",
    "    print(\"\\n📈 Evaluating Model Performance...\")\n",
    "    ev = EvalSurv(surv, durations_val, events_val, censor_surv='km')\n",
    "    c_index = ev.concordance_td('antolini')\n",
    "\n",
    "    print(f\"\\n📊 DeepSurv Concordance Index: {c_index:.4f}\")\n",
    "\n",
    "    print(\"✅ DeepSurv model training and evaluation completed.\\n\")\n",
    "    return model, X_val, durations_val, events_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_deepsurv_model(model, X_val, durations_val, events_val):\n",
    "    \"\"\"\n",
    "    Evaluate DeepSurv Model Performance using Concordance Index\n",
    "    \"\"\"\n",
    "    surv = model.predict_surv_df(X_val)\n",
    "    surv.index = pd.to_numeric(surv.index, errors='coerce')\n",
    "\n",
    "    if 0 not in surv.index:\n",
    "        new_row = pd.DataFrame(np.ones((1, surv.shape[1])), index=[0], columns=surv.columns)\n",
    "        surv = pd.concat([new_row, surv])\n",
    "        surv = surv.sort_index()\n",
    "\n",
    "    ev = EvalSurv(surv, durations_val, events_val, censor_surv='km')\n",
    "    c_index = ev.concordance_td('antolini')\n",
    "\n",
    "    print(f\"\\n📊 DeepSurv Concordance Index: {c_index:.4f}\")\n",
    "    return c_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Enhanced Feature Engineering ---\n",
      "✅ Input columns validated.\n",
      "\n",
      "🔍 Checking and handling initial NaNs in 'Country'...\n",
      "   Initial NaNs in Country: 0\n",
      "   ✅ NaNs in 'Country' imputed with 'Unknown Country'.\n",
      "   NaNs in Country after imputation: 0 (Should be 0)\n",
      "\n",
      "🧹 Basic Cleaning...\n",
      "   ✅ Country and Gender cleaned.\n",
      "   Sample age_df countries after cleaning: ['United States of America' 'United Kingdom' 'Archduchy of Austria'\n",
      " 'Holy Roman Empire' 'Kingdom of France' 'France' 'Spain'\n",
      " 'Grand Duchy of Tuscany' 'Chile' 'Nazi Germany' 'Kingdom of Castile'\n",
      " 'Kingdom of the Netherlands' 'Byelorussian Soviet Socialist Republic'\n",
      " 'Czech Republic' 'Jamaica' 'German Empire' 'Denmark' 'Soviet Union'\n",
      " 'French Third Republic' 'Unknown Country']\n",
      "   Sample global_le_df countries: ['Aruba' 'Afghanistan' 'Angola' 'Albania' 'United Arab Emirates'\n",
      " 'Argentina' 'Armenia' 'Antigua and Barbuda' 'Australia' 'Austria'\n",
      " 'Azerbaijan' 'Burundi' 'Belgium' 'Benin' 'Burkina Faso' 'Bangladesh'\n",
      " 'Bulgaria' 'Bahrain' 'Bahamas, The' 'Bosnia and Herzegovina']\n",
      "\n",
      "🔍 Sample global_le_agg countries BEFORE merge:\n",
      "   Unique countries in global_le_agg: ['Afghanistan' 'Albania' 'Algeria' 'Angola' 'Antigua and Barbuda'\n",
      " 'Argentina' 'Armenia' 'Aruba' 'Australia' 'Austria' 'Azerbaijan'\n",
      " 'Bahamas, The' 'Bahrain' 'Bangladesh' 'Barbados' 'Belarus' 'Belgium'\n",
      " 'Belize' 'Benin' 'Bermuda']\n",
      "🔍 Checking for NaNs after basic cleaning:\n",
      "   NaNs in Country: 0\n",
      "   NaNs in Gender: 0\n",
      "\n",
      "🧬 Engineering Clinical Features...\n",
      "   ✅ Stress score engineered.\n",
      "   ✅ Avg BMI engineered.\n",
      "   ✅ Smoking prevalence engineered.\n",
      "🔍 Checking for NaNs after clinical features:\n",
      "   NaNs in stress_score: 0\n",
      "   NaNs in avg_bmi: 0\n",
      "   NaNs in smoking_prev: 0\n",
      "\n",
      "🌍 Engineering Country-Level Features...\n",
      "\n",
      "Sample global_le_agg before merge:\n",
      "               Country  Life_Exp_Value\n",
      "0          Afghanistan         64.8330\n",
      "1              Albania         78.5730\n",
      "2              Algeria         76.8800\n",
      "3               Angola         61.1470\n",
      "4  Antigua and Barbuda         77.0160\n",
      "   NaNs in global_le_agg['Country']: 0\n",
      "   NaNs in global_le_agg['Life_Exp_Value']: 0\n",
      "   ✅ Global Life Expectancy engineered.\n",
      "🔍 Checking for NaNs after country-level features:\n",
      "   NaNs in global_life_exp: 0\n",
      "\n",
      "⏳ Survival Data Setup...\n",
      "   ✅ Survival time (T) and censoring engineered.\n",
      "\n",
      "🔍 Final NaN check before returning:\n",
      "   NaNs in censored: 0\n",
      "   NaNs in T: 0\n",
      "\n",
      "Sample processed_batch after feature engineering:\n",
      "     Id                     Name                                Short description  Gender                   Country  Occupation  Birth year  Death year Manner of death  Age of death  stress_score  avg_bmi  smoking_prev  Life_Exp_Value  global_life_exp  censored       T\n",
      "0   Q23        George Washington   1st president of the United States (1732–1799)  0.5000  United States of America  Politician        1732   1799.0000  natural causes       67.0000        1.0000  65.4000        0.6000             NaN          81.8951         0 67.0000\n",
      "1   Q42            Douglas Adams                      English writer and humorist  0.5000            United Kingdom      Artist        1952   2001.0000  natural causes       49.0000        0.5556  25.0000        0.4502         81.2049          81.2049         0 49.0000\n",
      "2   Q91          Abraham Lincoln  16th president of the United States (1809-1865)  0.5000  United States of America  Politician        1809   1865.0000        homicide       56.0000        1.0000  65.4000        0.6000             NaN          81.8951         0 56.0000\n",
      "3  Q254  Wolfgang Amadeus Mozart        Austrian composer of the Classical period  0.5000      Archduchy of Austria      Artist        1756   1791.0000             NaN       35.0000        0.5556  25.0000        0.6000             NaN          81.8951         0 35.0000\n",
      "4  Q255     Ludwig van Beethoven           German classical and romantic composer  0.5000         Holy Roman Empire      Artist        1770   1827.0000             NaN       57.0000        0.5556  25.0000        0.6000             NaN          81.8951         0 57.0000\n",
      "   NaNs in processed_batch after FE: 0\n",
      "✅ Enhanced feature engineering completed.\n",
      "\n",
      "   Sample age_df countries: ['United States of America' 'United Kingdom' 'Archduchy of Austria'\n",
      " 'Holy Roman Empire' 'Kingdom of France' 'France' 'Spain'\n",
      " 'Grand Duchy of Tuscany' 'Chile' 'Nazi Germany' 'Kingdom of Castile'\n",
      " 'Kingdom of the Netherlands' 'Byelorussian Soviet Socialist Republic'\n",
      " 'Czech Republic' 'Jamaica' 'German Empire' 'Denmark' 'Soviet Union'\n",
      " 'French Third Republic' 'Unknown Country']\n",
      "\n",
      "--- Train DeepSurv Model ---\n",
      "✅ Required features validated.\n",
      "\n",
      "📊 Checking Data Before Training DeepSurv (Detailed NaN Check):\n",
      "Feature Shape: (1223009, 4), Durations Shape: (1223009,), Events Shape: (1223009,)\n",
      "NaN counts per column BEFORE training:\n",
      "stress_score       0\n",
      "avg_bmi            0\n",
      "smoking_prev       0\n",
      "global_life_exp    0\n",
      "T                  0\n",
      "censored           0\n",
      "dtype: int64\n",
      "Total NaNs in Features and Targets BEFORE training: 0\n",
      "Total NaNs in Features (X): 0\n",
      "NaNs in Durations: 0\n",
      "NaNs in Events: 0\n",
      "Sample Features:\n",
      "[[ 1.         65.4         0.6        81.89512   ]\n",
      " [ 0.5555556  25.          0.45016602 81.20488   ]\n",
      " [ 1.         65.4         0.6        81.89512   ]\n",
      " [ 0.5555556  25.          0.6        81.89512   ]\n",
      " [ 0.5555556  25.          0.6        81.89512   ]]\n",
      "Sample Durations: [67. 49. 56. 35. 57.]\n",
      "Sample Events: [0 0 0 0 0]\n",
      "Unique event values (should be 0 or 1): [0 1]\n",
      "\n",
      "✅ No NaNs detected in 'T' column (so far in NaN check).\n",
      "✅ No NaNs in input data for training.\n",
      "   ✅ Data split into training and validation sets.\n",
      "   ✅ Features normalized using StandardScaler.\n",
      "NaNs in Scaled X_train: 0\n",
      "NaNs in Scaled X_val: 0\n",
      "✅ No NaNs in scaled data.\n",
      "\n",
      "🚀 Training DeepSurv Model...\n",
      "✅ DeepSurv Model Initialized with 4 input features.\n",
      "Hidden Layers: [64, 64], Dropout: 0.1\n",
      "Training parameters: Epochs=100, Batch Size=256, Verbose=True\n",
      "⚠️ Loss logging failed with error: 'TrainingLogger' object is not subscriptable, continuing training... (Error details: 'TrainingLogger' object is not subscriptable)\n",
      "0:\t[7s / 7s],\t\n",
      "⚠️ Loss logging failed with error: 'TrainingLogger' object is not subscriptable, continuing training... (Error details: 'TrainingLogger' object is not subscriptable)\n",
      "1:\t[7s / 15s],\t\n",
      "⚠️ Loss logging failed with error: 'TrainingLogger' object is not subscriptable, continuing training... (Error details: 'TrainingLogger' object is not subscriptable)\n",
      "2:\t[7s / 22s],\t\n",
      "⚠️ Loss logging failed with error: 'TrainingLogger' object is not subscriptable, continuing training... (Error details: 'TrainingLogger' object is not subscriptable)\n",
      "3:\t[7s / 30s],\t\n",
      "⚠️ Loss logging failed with error: 'TrainingLogger' object is not subscriptable, continuing training... (Error details: 'TrainingLogger' object is not subscriptable)\n",
      "4:\t[7s / 37s],\t\n",
      "⚠️ Loss logging failed with error: 'TrainingLogger' object is not subscriptable, continuing training... (Error details: 'TrainingLogger' object is not subscriptable)\n",
      "5:\t[7s / 45s],\t\n",
      "⚠️ Loss logging failed with error: 'TrainingLogger' object is not subscriptable, continuing training... (Error details: 'TrainingLogger' object is not subscriptable)\n",
      "6:\t[7s / 52s],\t\n",
      "⚠️ Loss logging failed with error: 'TrainingLogger' object is not subscriptable, continuing training... (Error details: 'TrainingLogger' object is not subscriptable)\n",
      "7:\t[7s / 1m:0s],\t\n",
      "⚠️ Loss logging failed with error: 'TrainingLogger' object is not subscriptable, continuing training... (Error details: 'TrainingLogger' object is not subscriptable)\n",
      "8:\t[7s / 1m:7s],\t\n",
      "⚠️ Loss logging failed with error: 'TrainingLogger' object is not subscriptable, continuing training... (Error details: 'TrainingLogger' object is not subscriptable)\n",
      "9:\t[7s / 1m:15s],\t\n",
      "⚠️ Loss logging failed with error: 'TrainingLogger' object is not subscriptable, continuing training... (Error details: 'TrainingLogger' object is not subscriptable)\n",
      "10:\t[7s / 1m:22s],\t\n",
      "⚠️ Loss logging failed with error: 'TrainingLogger' object is not subscriptable, continuing training... (Error details: 'TrainingLogger' object is not subscriptable)\n",
      "11:\t[7s / 1m:30s],\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Sample age_df countries: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()[:\u001b[38;5;241m20\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Print first 20 unique countries\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 2️⃣ Train DeepSurv\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m deepsurv_model, X_val, durations_val, events_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_deepsurv_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 3️⃣ Evaluate Model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m evaluate_deepsurv_model(deepsurv_model, X_val, durations_val, events_val)\n",
      "Cell \u001b[0;32mIn[44], line 117\u001b[0m, in \u001b[0;36mtrain_deepsurv_model\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    113\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# Keep verbose for detailed output\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining parameters: Epochs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch Size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Verbose=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mverbose\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdurations_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdurations_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# set num_workers to 0 for debugging purposes\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ DeepSurv model training completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Compute Baseline Hazards\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pycox/models/cox.py:51\u001b[0m, in \u001b[0;36m_CoxBase.fit\u001b[0;34m(self, input, target, batch_size, epochs, callbacks, verbose, num_workers, shuffle, metrics, val_data, val_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit  model with inputs and targets. Where 'input' is the covariates, and\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m'target' is a tuple with (durations, events).\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    TrainingLogger -- Training log\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_data \u001b[38;5;241m=\u001b[39m tt\u001b[38;5;241m.\u001b[39mtuplefy(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtuples/base.py:294\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, input, target, batch_size, epochs, callbacks, verbose, num_workers, shuffle, metrics, val_data, val_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (is_dl(val_data) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (val_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    291\u001b[0m     val_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataloader(\n\u001b[1;32m    292\u001b[0m         val_data, val_batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mnum_workers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    293\u001b[0m     )\n\u001b[0;32m--> 294\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtuples/base.py:231\u001b[0m, in \u001b[0;36mModel.fit_dataloader\u001b[0;34m(self, dataloader, epochs, callbacks, verbose, metrics, val_dataloader)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop:\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_batch_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtuples/data.py:93\u001b[0m, in \u001b[0;36mDatasetTuple.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__iter__\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m(index) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m     92\u001b[0m     index \u001b[38;5;241m=\u001b[39m [index]\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtuples/tupletree.py:597\u001b[0m, in \u001b[0;36m_TupleTreeSlicer.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuple_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtuples/tupletree.py:389\u001b[0m, in \u001b[0;36mTupleTree.apply\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func):\n\u001b[1;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Shorthand to apply_leaf(func)(self)\"\"\"\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_leaf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtuples/tupletree.py:33\u001b[0m, in \u001b[0;36mapply_leaf.<locals>.wrapper\u001b[0;34m(data, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;129;01min\u001b[39;00m _CONTAINERS:\n\u001b[0;32m---> 33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTupleTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msub\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtuples/tupletree.py:33\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;129;01min\u001b[39;00m _CONTAINERS:\n\u001b[0;32m---> 33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m TupleTree(\u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtuples/tupletree.py:33\u001b[0m, in \u001b[0;36mapply_leaf.<locals>.wrapper\u001b[0;34m(data, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;129;01min\u001b[39;00m _CONTAINERS:\n\u001b[0;32m---> 33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTupleTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msub\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtuples/tupletree.py:33\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;129;01min\u001b[39;00m _CONTAINERS:\n\u001b[0;32m---> 33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m TupleTree(wrapper(sub, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1️⃣ Process the Dataset\n",
    "processed_batch = enhanced_feature_engineering(age_df, life_exp_df, global_le_df, death_rates_df)\n",
    "print(f\"   Sample age_df countries: {processed_batch['Country'].unique()[:20]}\") # Print first 20 unique countries\n",
    "\n",
    "# 2️⃣ Train DeepSurv\n",
    "deepsurv_model, X_val, durations_val, events_val = train_deepsurv_model(processed_batch)\n",
    "\n",
    "# 3️⃣ Evaluate Model\n",
    "evaluate_deepsurv_model(deepsurv_model, X_val, durations_val, events_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
