{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepsurv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers > /dev/null 2>&1\n",
    "!pip install xgboost > /dev/null 2>&1\n",
    "!pip install scikit-learn==1.4.2 scikit-survival==0.23.1 > /dev/null 2>&1\n",
    "!pip install torchtuples > /dev/null 2>&1\n",
    "!pip install pycox > /dev/null 2>&1\n",
    "!pip install numpy==1.21.5  > /dev/null 2>&1\n",
    "!pip install interpret-core  > /dev/null 2>&1\n",
    "!pip install lightgbm > /dev/null 2>&1\n",
    "!pip install shap > /dev/null 2>&1\n",
    "!pip install lifelines pycox > /dev/null 2>&1\n",
    "!pip install pycountry > /dev/null 2>&1\n",
    "!pip install -U sentence-transformers xgboost scikit-learn==1.4.2 scikit-survival==0.23.1 torchtuples pycox numpy==1.21.5 interpret-core lightgbm shap lifelines pycox pycountry > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtuples as tt\n",
    "import kagglehub\n",
    "import os\n",
    "import shap\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lifelines import CoxPHFitter\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from pycox.models.cox import CoxPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torchtuples\n",
    "import torchtuples.callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pandas Display Options to try to force full output ---\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', 50) # Adjust as needed\n",
    "pd.set_option('display.width', 1000)     # Adjust as needed\n",
    "pd.set_option('display.max_colwidth', None) # Show full column content\n",
    "pd.set_option('display.float_format', '{:.4f}'.format) # Optional: format floats\n",
    "\n",
    "feature_list = ['stress_score', 'avg_bmi', 'smoking_prev', 'global_life_exp'] # Define feature_list globally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Life Expectancy Sample:\n",
      "       Country  Year      Status  Life expectancy   Adult Mortality  infant deaths  Alcohol  percentage expenditure  Hepatitis B  Measles     BMI   under-five deaths    Polio  Total expenditure  Diphtheria    HIV/AIDS      GDP    Population   thinness  1-19 years   thinness 5-9 years  Income composition of resources  Schooling\n",
      "0  Afghanistan  2015  Developing           65.0000         263.0000             62   0.0100                 71.2796      65.0000      1154 19.1000                  83  6.0000             8.1600      65.0000     0.1000 584.2592 33736494.0000                17.2000              17.3000                           0.4790    10.1000\n",
      "1  Afghanistan  2014  Developing           59.9000         271.0000             64   0.0100                 73.5236      62.0000       492 18.6000                  86 58.0000             8.1800      62.0000     0.1000 612.6965   327582.0000                17.5000              17.5000                           0.4760    10.0000\n",
      "2  Afghanistan  2013  Developing           59.9000         268.0000             66   0.0100                 73.2192      64.0000       430 18.1000                  89 62.0000             8.1300      64.0000     0.1000 631.7450 31731688.0000                17.7000              17.7000                           0.4700     9.9000\n",
      "3  Afghanistan  2012  Developing           59.5000         272.0000             69   0.0100                 78.1842      67.0000      2787 17.6000                  93 67.0000             8.5200      67.0000     0.1000 669.9590  3696958.0000                17.9000              18.0000                           0.4630     9.8000\n",
      "4  Afghanistan  2011  Developing           59.2000         275.0000             71   0.0100                  7.0971      68.0000      3013 17.2000                  97 68.0000             7.8700      68.0000     0.1000  63.5372  2978599.0000                18.2000              18.2000                           0.4540     9.5000\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Heart Failure Sample:\n",
      "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR ExerciseAngina  Oldpeak ST_Slope  HeartDisease\n",
      "0   40   M           ATA        140          289          0     Normal    172              N   0.0000       Up             0\n",
      "1   49   F           NAP        160          180          0     Normal    156              N   1.0000     Flat             1\n",
      "2   37   M           ATA        130          283          0         ST     98              N   0.0000       Up             0\n",
      "3   48   F           ASY        138          214          0     Normal    108              Y   1.5000     Flat             1\n",
      "4   54   M           NAP        150          195          0     Normal    122              N   0.0000       Up             0\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Age Dataset Sample:\n",
      "     Id                     Name                                Short description Gender                                             Country  Occupation  Birth year  Death year Manner of death  Age of death\n",
      "0   Q23        George Washington   1st president of the United States (1732–1799)   Male  United States of America; Kingdom of Great Britain  Politician        1732   1799.0000  natural causes       67.0000\n",
      "1   Q42            Douglas Adams                      English writer and humorist   Male                                      United Kingdom      Artist        1952   2001.0000  natural causes       49.0000\n",
      "2   Q91          Abraham Lincoln  16th president of the United States (1809-1865)   Male                            United States of America  Politician        1809   1865.0000        homicide       56.0000\n",
      "3  Q254  Wolfgang Amadeus Mozart        Austrian composer of the Classical period   Male     Archduchy of Austria; Archbishopric of Salzburg      Artist        1756   1791.0000             NaN       35.0000\n",
      "4  Q255     Ludwig van Beethoven           German classical and romantic composer   Male                  Holy Roman Empire; Austrian Empire      Artist        1770   1827.0000             NaN       57.0000\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "World Important Events Sample:\n",
      "   Sl. No                      Name of Incident     Date    Month     Year Country Type of Event    Place Name                                                          Impact                      Affected Population Important Person/Group Responsible   Outcome\n",
      "0       1  Indus Valley Civilization Flourishes  Unknown  Unknown  2600 BC   India  Civilization  Indus Valley  Development of one of the world's earliest urban civilizations                        Local inhabitants                Indus Valley people  Positive\n",
      "1       2               Battle of the Ten Kings  Unknown  Unknown  1400 BC   India        Battle        Punjab      Rigvedic tribes consolidated their control over the region                          Rigvedic tribes                              Sudas  Positive\n",
      "2       6  Establishment of the Delhi Sultanate  Unknown  Unknown     1206   India     Political         Delhi                       Muslim rule established in parts of India  People of Delhi and surrounding regions      QutbUnknownudUnknowndin Aibak     Mixed\n",
      "3       7                     Battle of Panipat       21    April     1526   India        Battle       Panipat                        Foundation of the Mughal Empire in India                 Northern Indian kingdoms                              Babur     Mixed\n",
      "4       8          Establishment of British Raj        1      May     1858   India      Colonial   Whole India                     Start of direct British governance in India                      Indian subcontinent  British East India Company/Empire  Negative\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Historical Plane Crashes Sample:\n",
      "                 date     time                            location                operator flight_no          route                 ac_type registration cn_ln                       aboard                   fatalities ground                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           summary\n",
      "0  September 17, 1908    17:18                 Fort Myer, Virginia    Military - U.S. Army         ?  Demonstration        Wright Flyer III            ?     1   2   (passengers:1  crew:1)   1   (passengers:1  crew:0)      0  During a demonstration flight, a U.S. Army flyer flown by Orville Wright nose-dived into the ground from a height of approximately 75 feet, killing Lt. Thomas E. Selfridge, 26, who was a passenger. This was the first recorded airplane fatality in history.  One of two propellers separated in flight, tearing loose the wires bracing the rudder and causing the loss of control of the aircraft.  Orville Wright suffered broken ribs, pelvis and a leg.  Selfridge suffered a crushed skull and died a short time later.\n",
      "1  September 07, 1909        ?             Juvisy-sur-Orge, France                       ?         ?       Air show          Wright Byplane          SC1     ?   1   (passengers:0  crew:1)   1   (passengers:0  crew:0)      0                                                                                                                                                                                                                                                                                                                                                                                                  Eugene Lefebvre was the first pilot to ever be killed in an air accident, after his controls jambed while flying in an air show.\n",
      "2       July 12, 1912    06:30           Atlantic City, New Jersey    Military - U.S. Navy         ?    Test flight               Dirigible            ?     ?   5   (passengers:0  crew:5)   5   (passengers:0  crew:5)      0                                                                                                                                                                                                                                                                                                                                                                                                                               First U.S. dirigible Akron exploded just offshore at an altitude of 1,000 ft. during a test flight.\n",
      "3     August 06, 1913        ?  Victoria, British Columbia, Canada                 Private         ?              ?        Curtiss seaplane            ?     ?   1   (passengers:0  crew:1)   1   (passengers:0  crew:1)      0                                                                                                                                                                                                                                                                                                                                                                                                    The first fatal airplane accident in Canada occurred when American barnstormer, John M. Bryant, California aviator was killed.\n",
      "4  September 09, 1913  c 18:30                  Over the North Sea  Military - German Navy         ?              ?  Zeppelin L-1 (airship)            ?     ?  20   (passengers:?  crew:?)  14   (passengers:?  crew:?)      0                                                                                                                                                                                                                                                                                                              The airship flew into a thunderstorm and encountered a severe downdraft crashing 20 miles north of Helgoland Island into the sea. The ship broke in two and the control car immediately sank drowning its occupants.\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Global Life Expectancy Historical Dataset Sample:\n",
      "           Country Name Country Code    1960    1961    1962    1963    1964    1965    1966    1967    1968    1969    1970    1971    1972    1973    1974    1975    1976    1977    1978    1979    1980    1981    1982  ...    1996    1997    1998    1999    2000    2001    2002    2003    2004    2005    2006    2007    2008    2009    2010    2011    2012    2013    2014    2015    2016    2017    2018    2019    2020\n",
      "0                 Aruba          ABW 65.6620 66.0740 66.4440 66.7870 67.1130 67.4350 67.7620 68.0950 68.4360 68.7840 69.1400 69.4980 69.8510 70.1910 70.5190 70.8330 71.1400 71.4410 71.7360 72.0230 72.2930 72.5380 72.7510  ... 73.6460 73.6710 73.7000 73.7380 73.7870 73.8530 73.9370 74.0380 74.1560 74.2870 74.4290 74.5760 74.7250 74.8720 75.0170 75.1580 75.2990 75.4410 75.5830 75.7250 75.8680 76.0100 76.1520 76.2930 76.4340\n",
      "1           Afghanistan          AFG 32.4460 32.9620 33.4710 33.9710 34.4630 34.9480 35.4300 35.9140 36.4030 36.9000 37.4090 37.9300 38.4610 39.0030 39.5580 40.1280 40.7150 41.3200 41.9440 42.5850 43.2440 43.9230 44.6170  ... 53.9240 54.4240 54.9060 55.3760 55.8410 56.3080 56.7840 57.2710 57.7720 58.2900 58.8260 59.3750 59.9300 60.4840 61.0280 61.5530 62.0540 62.5250 62.9660 63.3770 63.7630 64.1300 64.4860 64.8330 65.1730\n",
      "2                Angola          AGO 37.5240 37.8110 38.1130 38.4300 38.7600 39.1020 39.4540 39.8130 40.1780 40.5460 40.9140 41.2820 41.6500 42.0160 42.3740 42.7210 43.0530 43.3670 43.6600 43.9310 44.1780 44.4040 44.6110  ... 45.3500 45.5190 45.7630 46.0930 46.5220 47.0590 47.7020 48.4400 49.2630 50.1650 51.1430 52.1770 53.2430 54.3110 55.3500 56.3300 57.2360 58.0540 58.7760 59.3980 59.9250 60.3790 60.7820 61.1470 61.4870\n",
      "3               Albania          ALB 62.2830 63.3010 64.1900 64.9140 65.4630 65.8500 66.1100 66.3040 66.4870 66.6890 66.9350 67.2370 67.5820 67.9530 68.3430 68.7360 69.1100 69.4480 69.7420 69.9910 70.2080 70.4160 70.6350  ... 72.4950 72.8380 73.2080 73.5870 73.9550 74.2880 74.5790 74.8280 75.0390 75.2280 75.4230 75.6460 75.9120 76.2210 76.5620 76.9140 77.2520 77.5540 77.8130 78.0250 78.1940 78.3330 78.4580 78.5730 78.6860\n",
      "4  United Arab Emirates          ARE 51.5370 52.5600 53.5730 54.5720 55.5550 56.5230 57.4820 58.4320 59.3750 60.3040 61.2150 62.0990 62.9490 63.7590 64.5250 65.2440 65.9160 66.5450 67.1370 67.6920 68.2130 68.7010 69.1580  ... 73.4280 73.6570 73.8830 74.1060 74.3270 74.5440 74.7580 74.9680 75.1740 75.3760 75.5730 75.7670 75.9570 76.1450 76.3320 76.5210 76.7110 76.9030 77.0950 77.2850 77.4700 77.6470 77.8140 77.9720 78.1200\n",
      "\n",
      "[5 rows x 63 columns]\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
      "Death Rates United States Dataset Sample:\n",
      "                 INDICATOR                                                  UNIT  UNIT_NUM STUB_NAME  STUB_NAME_NUM   STUB_LABEL  STUB_LABEL_NUM  YEAR  YEAR_NUM       AGE  AGE_NUM  ESTIMATE\n",
      "0  Death rates for suicide  Deaths per 100,000 resident population, age-adjusted         1     Total              0  All persons          0.0000  1950         1  All ages   0.0000   13.2000\n",
      "1  Death rates for suicide  Deaths per 100,000 resident population, age-adjusted         1     Total              0  All persons          0.0000  1960         2  All ages   0.0000   12.5000\n",
      "2  Death rates for suicide  Deaths per 100,000 resident population, age-adjusted         1     Total              0  All persons          0.0000  1970         3  All ages   0.0000   13.1000\n",
      "3  Death rates for suicide  Deaths per 100,000 resident population, age-adjusted         1     Total              0  All persons          0.0000  1980         4  All ages   0.0000   12.2000\n",
      "4  Death rates for suicide  Deaths per 100,000 resident population, age-adjusted         1     Total              0  All persons          0.0000  1981         5  All ages   0.0000   12.3000\n"
     ]
    }
   ],
   "source": [
    "# Life Expectancy dataset\n",
    "life_exp_path = kagglehub.dataset_download(\"kumarajarshi/life-expectancy-who\")\n",
    "life_exp_file = os.path.join(life_exp_path, \"Life Expectancy Data.csv\")\n",
    "life_exp_df = pd.read_csv(life_exp_file)\n",
    "print(\"Life Expectancy Sample:\")\n",
    "print(life_exp_df.head())\n",
    "\n",
    "# Heart Failure dataset (not used in LightGBM, but kept for context)\n",
    "heart_path = kagglehub.dataset_download(\"fedesoriano/heart-failure-prediction\")\n",
    "heart_file = os.path.join(heart_path, \"heart.csv\")\n",
    "heart_df = pd.read_csv(heart_file)\n",
    "print(\"Heart Failure Sample:\")\n",
    "print(heart_df.head())\n",
    "\n",
    "# Age Dataset\n",
    "age_path = kagglehub.dataset_download(\"imoore/age-dataset\")\n",
    "age_file = os.path.join(age_path, \"AgeDataset-V1.csv\")\n",
    "age_df = pd.read_csv(age_file)\n",
    "print(\"Age Dataset Sample:\")\n",
    "print(age_df.head())\n",
    "\n",
    "# World important events Dataset\n",
    "events_path = kagglehub.dataset_download(\"saketk511/world-important-events-ancient-to-modern\")\n",
    "events_file = os.path.join(events_path, \"World Important Dates.csv\")\n",
    "events_df = pd.read_csv(events_file)\n",
    "print(\"World Important Events Sample:\")\n",
    "print(events_df.head())\n",
    "\n",
    "# Plane Crash Dataset\n",
    "plane_crash_path = kagglehub.dataset_download(\"nguyenhoc/plane-crash\")\n",
    "plane_crash_file = os.path.join(plane_crash_path, \"planecrashinfo_20181121001952.csv\")  \n",
    "planes_df = pd.read_csv(plane_crash_file)\n",
    "print(\"Historical Plane Crashes Sample:\")\n",
    "print(planes_df.head())\n",
    "\n",
    "# Gloabl Life Expectancy dataset\n",
    "global_le_path = kagglehub.dataset_download(\"hasibalmuzdadid/global-life-expectancy-historical-dataset\")\n",
    "global_le_file = os.path.join(global_le_path, \"global life expectancy dataset.csv\")\n",
    "global_le_df = pd.read_csv(global_le_file)\n",
    "print(\"Global Life Expectancy Historical Dataset Sample:\")\n",
    "print(global_le_df.head())\n",
    "\n",
    "# US death rate Dataset\n",
    "death_rates_path = kagglehub.dataset_download(\"melissamonfared/death-rates-united-states\")\n",
    "death_rates_file = os.path.join(death_rates_path, \"Death_rates.csv\")\n",
    "death_rates_df = pd.read_csv(death_rates_file)\n",
    "print(\"Death Rates United States Dataset Sample:\")\n",
    "print(death_rates_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_feature_engineering(df, life_exp_df, global_le_df, death_rates_df):\n",
    "    \"\"\"\n",
    "    Enhanced feature engineering with temporal features, better BMI handling,\n",
    "    interaction terms, and comprehensive NaN debugging prints.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Enhanced Feature Engineering ---\")\n",
    "\n",
    "    # -------- Validate Input Columns --------\n",
    "    required_columns = {'Country', 'Gender', 'Occupation', 'Birth year',\n",
    "                        'Death year', 'Age of death'}\n",
    "    missing = required_columns - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns: {missing}\")\n",
    "    print(\"✅ Input columns validated.\")\n",
    "\n",
    "    # -------- Set Observation Year --------\n",
    "    current_year = 2019\n",
    "\n",
    "    # -------- Handle Initial NaNs in 'Country' --------\n",
    "    print(\"\\n🔍 Checking and handling initial NaNs in 'Country'...\")\n",
    "    initial_nan_count_country = df['Country'].isnull().sum()\n",
    "    print(f\"   Initial NaNs in Country: {initial_nan_count_country}\")\n",
    "\n",
    "    # Impute NaNs in 'Country' with 'Unknown Country' *before* cleaning\n",
    "    df['Country'] = df['Country'].fillna('Unknown Country')\n",
    "    print(\"   ✅ NaNs in 'Country' imputed with 'Unknown Country'.\")\n",
    "    nan_count_after_imputation = df['Country'].isnull().sum()\n",
    "    print(f\"   NaNs in Country after imputation: {nan_count_after_imputation} (Should be 0)\")\n",
    "\n",
    "    # FIXED: Create proper event indicator (1 = death observed, 0 = censored)\n",
    "    df['event'] = (df['Death year'] <= current_year).astype(int)  # CORRECTED\n",
    "\n",
    "    # Calculate time-to-event\n",
    "    df['T'] = np.where(\n",
    "        df['event'] == 0,  # If censored (no event)\n",
    "        current_year - df['Birth year'],  # Time until censoring\n",
    "        df['Age of death']  # If event observed, use actual age\n",
    "    )\n",
    "\n",
    "    # -------- Basic Cleaning --------\n",
    "    print(\"\\n🧹 Basic Cleaning...\")\n",
    "    df['Country'] = df['Country'].str.split(';').str[0].str.strip()\n",
    "    df['Gender'] = np.where(df['Gender'] == 'Male', 1,\n",
    "                          np.where(df['Gender'] == 'Female', 0, 0.5))\n",
    "    print(\"   ✅ Country and Gender cleaned.\")\n",
    "    print(f\"   Sample age_df countries after cleaning: {df['Country'].unique()[:20]}\")\n",
    "    print(f\"   Sample global_le_df countries: {global_le_df['Country Name'].unique()[:20]}\")\n",
    "    print(\"\\n🔍 Sample global_le_agg countries BEFORE merge:\")  # Inspect global_le_agg countries before merge\n",
    "    global_le_melted = global_le_df.melt(\n",
    "        id_vars=['Country Name', 'Country Code'],\n",
    "        value_vars=[str(y) for y in range(1960, current_year+1)],\n",
    "        var_name='Year',\n",
    "        value_name='Life_Exp_Value'\n",
    "    )\n",
    "\n",
    "    global_le_agg = (\n",
    "        global_le_melted\n",
    "        .sort_values(['Country Name', 'Year'], ascending=[True, False])\n",
    "        .groupby('Country Name')\n",
    "        ['Life_Exp_Value']\n",
    "        .first()\n",
    "        .reset_index()\n",
    "        .rename(columns={'Country Name': 'Country'})\n",
    "    )\n",
    "    print(f\"   Unique countries in global_le_agg: {global_le_agg['Country'].unique()[:20]}\") # Sample of unique countries in global_le_agg\n",
    "\n",
    "\n",
    "    # Check for NaNs after basic cleaning (should be minimal now)\n",
    "    print(\"🔍 Checking for NaNs after basic cleaning:\")\n",
    "    print(f\"   NaNs in Country: {df['Country'].isnull().sum()}\") # Expecting 0 or very few\n",
    "    print(f\"   NaNs in Gender: {df['Gender'].isnull().sum()}\")\n",
    "\n",
    "\n",
    "    # -------- Clinical Features --------\n",
    "    print(\"\\n🧬 Engineering Clinical Features...\")\n",
    "    stress_map = {'Politician': 9, 'Military personnel': 8, 'Journalist': 7,\n",
    "                  'Businessperson': 6, 'Artist': 5, 'Teacher': 4,\n",
    "                  'Researcher': 3, 'Other': 5, 'Unknown': 5}\n",
    "    df['stress_score'] = df['Occupation'].map(stress_map).fillna(5).astype('float32') / 9.0\n",
    "    print(\"   ✅ Stress score engineered.\")\n",
    "\n",
    "    life_exp_df[' BMI '] = pd.to_numeric(life_exp_df[' BMI '], errors='coerce')\n",
    "    country_bmi = life_exp_df.groupby('Country')[' BMI '].agg(['mean', 'std']).fillna(25) # Aggregate mean and std\n",
    "    df = df.merge(country_bmi, on='Country', how='left') # Merge mean and std\n",
    "    df['bmi_zscore'] = ((life_exp_df[' BMI '] - df['mean']) / df['std']).fillna(0).astype('float32') # Calculate z-score, impute NaNs with 0 # USING life_exp_df[' BMI ']\n",
    "    df.drop(columns=['mean', 'std'], inplace=True) # Drop intermediate mean and std columns, keep ' BMI ' for avg_bmi calculation\n",
    "    country_bmi_median = life_exp_df.groupby('Country')[' BMI '].median().to_dict() # Recalculate country_bmi for median\n",
    "    df['avg_bmi'] = df['Country'].map(country_bmi_median).fillna(25).astype('float32') # Fallback imputation for avg_bmi # Calculate avg_bmi AFTER z-score\n",
    "    print(\"   ✅ Avg BMI engineered (using z-score and imputation).\")\n",
    "\n",
    "\n",
    "    df['smoking_prev'] = (1 / (1 + np.exp((df['Birth year'] - 1950) / 10))).astype('float32')\n",
    "    df['smoking_prev'] = np.clip(df['smoking_prev'], 0.1, 0.6)\n",
    "    print(\"   ✅ Smoking prevalence engineered.\")\n",
    "\n",
    "    # Check for NaNs after clinical feature engineering\n",
    "    print(\"🔍 Checking for NaNs after clinical features:\")\n",
    "    print(f\"   NaNs in stress_score: {df['stress_score'].isnull().sum()}\")\n",
    "    print(f\"   NaNs in avg_bmi: {df['avg_bmi'].isnull().sum()}\") # Check NaNs in avg_bmi\n",
    "    print(f\"   NaNs in bmi_zscore: {df['bmi_zscore'].isnull().sum()}\") # Check NaNs in bmi_zscore\n",
    "    print(f\"   NaNs in smoking_prev: {df['smoking_prev'].isnull().sum()}\")\n",
    "\n",
    "\n",
    "    # -------- Country-Level Features --------\n",
    "    print(\"\\n🌍 Engineering Country-Level Features...\")\n",
    "    print(\"\\nSample global_le_agg before merge:\") # Inspect global_le_agg\n",
    "    global_le_melted = global_le_df.melt(\n",
    "        id_vars=['Country Name', 'Country Code'],\n",
    "        value_vars=[str(y) for y in range(1960, current_year+1)],\n",
    "        var_name='Year',\n",
    "        value_name='Life_Exp_Value'\n",
    "    )\n",
    "\n",
    "    global_le_agg = (\n",
    "        global_le_melted\n",
    "        .sort_values(['Country Name', 'Year'], ascending=[True, False])\n",
    "        .groupby('Country Name')\n",
    "        ['Life_Exp_Value']\n",
    "        .first()\n",
    "        .reset_index()\n",
    "        .rename(columns={'Country Name': 'Country'})\n",
    "    )\n",
    "    print(global_le_agg.head()) # Print head of global_le_agg\n",
    "    print(f\"   NaNs in global_le_agg['Country']: {global_le_agg['Country'].isnull().sum()}\") # Check NaNs in global_le_agg['Country']\n",
    "    print(f\"   NaNs in global_le_agg['Life_Exp_Value']: {global_le_agg['Life_Exp_Value'].isnull().sum()}\") # Check NaNs in global_le_agg['Life_Exp_Value']\n",
    "\n",
    "\n",
    "    df = df.merge(global_le_agg, on='Country', how='left')\n",
    "    df['global_life_exp'] = df['Life_Exp_Value'].fillna(df['Life_Exp_Value'].median()).astype('float32') # Median imputation and float32\n",
    "    df.drop(columns=['Life_Exp_Value'], inplace=True) # Drop intermediate Life_Exp_Value column\n",
    "    print(\"   ✅ Global Life Expectancy engineered.\")\n",
    "\n",
    "    # Check for NaNs after country-level feature engineering\n",
    "    print(\"🔍 Checking for NaNs after country-level features:\")\n",
    "    print(f\"   NaNs in global_life_exp: {df['global_life_exp'].isnull().sum()}\")\n",
    "\n",
    "\n",
    "    # -------- Temporal Features --------\n",
    "    print(\"\\n⏰ Engineering Temporal Features...\")\n",
    "    df['birth_decade'] = (df['Birth year'] // 10 * 10).astype('int32')\n",
    "    df['death_decade'] = (df['Death year'] // 10 * 10).fillna(0).astype('int32') # Fill NaNs with 0 for death_decade\n",
    "    print(\"   ✅ Temporal features 'birth_decade' and 'death_decade' engineered.\")\n",
    "\n",
    "    # Check for NaNs in temporal features\n",
    "    print(\"🔍 Checking for NaNs after temporal features:\")\n",
    "    print(f\"   NaNs in birth_decade: {df['birth_decade'].isnull().sum()}\") # Should be 0\n",
    "    print(f\"   NaNs in death_decade: {df['death_decade'].isnull().sum()}\") # Should be 0 after fillna(0)\n",
    "\n",
    "\n",
    "    # -------- Interaction Terms --------\n",
    "    print(\"\\n<binary data, 1 bytes><binary data, 1 bytes> Creating Interaction Terms...\")\n",
    "    df['stress_x_bmi'] = df['stress_score'] * df['avg_bmi']\n",
    "    df['smoking_x_lifeexp'] = df['smoking_prev'] * df['global_life_exp']\n",
    "    print(\"   ✅ Interaction terms 'stress_x_bmi' and 'smoking_x_lifeexp' created.\")\n",
    "\n",
    "    # Check for NaNs in interaction terms\n",
    "    print(\"🔍 Checking for NaNs after interaction terms:\")\n",
    "    print(f\"   NaNs in stress_x_bmi: {df['stress_x_bmi'].isnull().sum()}\") # Should be 0\n",
    "    print(f\"   NaNs in smoking_x_lifeexp: {df['smoking_x_lifeexp'].isnull().sum()}\") # Should be 0\n",
    "\n",
    "\n",
    "    # -------- Survival Data Setup --------\n",
    "    print(\"\\n⏳ Survival Data Setup...\")\n",
    "    current_year = 2019 # Re-declare to ensure it's available in this scope if needed\n",
    "\n",
    "    df['censored'] = (df['Death year'] > current_year).astype(int)\n",
    "\n",
    "    # Revised 'T' calculation to handle missing 'Age of death' robustly\n",
    "    df['T'] = np.where(\n",
    "        df['censored'] == 1, # If censored (still alive as of current_year)\n",
    "        current_year - df['Birth year'], # T = time until observation (censoring time)\n",
    "        df['Age of death'] # Else (event observed), T = Age of death\n",
    "    )\n",
    "\n",
    "    # Handle potential NaNs in 'T' *after* the calculation, specifically for cases where Age of death is missing for non-censored cases.\n",
    "    # In such cases, we might default to a large value or impute based on other information if available, or drop the row if no imputation is reasonable.\n",
    "    # For now, let's impute with a plausible value (e.g., median age of death for non-censored individuals) or simply drop the row for simplicity.\n",
    "    median_age_death_non_censored = df[df['censored'] == 0]['Age of death'].median()\n",
    "    df['T'] = df['T'].fillna(median_age_death_non_censored).clip(0, 120).astype('float32') # Impute, clip, and float32\n",
    "\n",
    "    print(\"   ✅ Survival time (T) and censoring engineered.\")\n",
    "\n",
    "    # Final check for NaNs in critical columns\n",
    "    print(\"\\n🔍 Final NaN check before returning:\")\n",
    "    print(f\"   NaNs in censored: {df['censored'].isnull().sum()}\")\n",
    "    print(f\"   NaNs in T: {df['T'].isnull().sum()}\")\n",
    "    print(\"\\nSample processed_batch after feature engineering:\") # Inspect processed_batch\n",
    "    print(df.head())\n",
    "    feature_list_updated = [ # Update feature list\n",
    "        'stress_score', 'avg_bmi', 'smoking_prev', 'global_life_exp',\n",
    "        'birth_decade', 'bmi_zscore', 'stress_x_bmi', 'smoking_x_lifeexp']\n",
    "    print(f\"   NaNs in processed_batch after FE: {df[feature_list_updated + ['T', 'censored']].isnull().sum().sum()}\") # Total NaNs in features and targets after FE\n",
    "\n",
    "\n",
    "    print(\"✅ Enhanced feature engineering completed.\\n\")\n",
    "    global feature_list # Declare global feature_list\n",
    "    feature_list = feature_list_updated # Update global feature_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSurv(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepSurv, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.SELU(),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.SELU(),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SELU(),\n",
    "            \n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deepsurv_model(df):\n",
    "    \"\"\"\n",
    "    Train and evaluate DeepSurv model with NaN debugging, focusing on 'T'.\n",
    "    Improved training configuration with deeper network, dropout, SELU, AdamW,\n",
    "    learning rate scheduler, and early stopping.\n",
    "    Data preprocessing enhancements: RobustScaler and duration clipping.\n",
    "    Class balancing with sample weighting.\n",
    "    Simplified logging - printing model.log object correctly.\n",
    "    Addressing NaN loss, TrainingLogger TypeError, and IndexError in parameter inspection.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Train DeepSurv Model ---\")\n",
    "\n",
    "    # feature_list = ['stress_score', 'avg_bmi', smoking_prev', 'global_life_exp'] # Defined globally\n",
    "\n",
    "    # Ensure required features exist\n",
    "    missing_features = set(feature_list) - set(df.columns)\n",
    "    if missing_features:\n",
    "        raise KeyError(f\"Missing required features: {missing_features}. Available: {df.columns.tolist()}\")\n",
    "    print(\"✅ Required features validated.\")\n",
    "\n",
    "    # Extract Data\n",
    "    X = df[feature_list].values.astype('float32')\n",
    "    durations = df['T'].values.astype('float32')\n",
    "    events = df['censored'].values.astype('int')  # Ensure events are 0 or 1 # WAS df['event'] - corrected back to censored based on original code\n",
    "\n",
    "    # Debugging: Check Data Before Training - NaN Check!\n",
    "    print(\"\\n📊 Checking Data Before Training DeepSurv (Detailed NaN Check):\")\n",
    "    print(f\"Feature Shape: {X.shape}, Durations Shape: {durations.shape}, Events Shape: {events.shape}\")\n",
    "    print(\"NaN counts per column BEFORE training:\")\n",
    "    nan_counts = df[feature_list + ['T', 'censored']].isnull().sum() # NaN count for each feature and target\n",
    "    print(nan_counts) # Print NaN counts per column\n",
    "    print(f\"Total NaNs in Features and Targets BEFORE training: {nan_counts.sum()}\") # Total NaNs in features and targets\n",
    "    print(f\"Total NaNs in Features (X): {np.isnan(X).sum()}\")\n",
    "    print(f\"NaNs in Durations: {np.isnan(durations).sum()}\")\n",
    "    print(f\"NaNs in Events: {np.isnan(events).sum()}\")\n",
    "    print(f\"Sample Features:\\n{X[:5]}\")\n",
    "    print(f\"Sample Durations: {durations[:5]}\")\n",
    "    print(f\"Sample Events: {events[:5]}\")\n",
    "    print(f\"Unique event values (should be 0 or 1): {np.unique(events)}\")\n",
    "\n",
    "    # --- DEBUGGING: FIND ROW WITH NaN in 'T' ---\n",
    "    nan_T_mask = df['T'].isnull()\n",
    "    if nan_T_mask.any():\n",
    "        print(\"\\n🚨🚨🚨 NaN DETECTED in 'T' column! 🚨🚨🚨\")\n",
    "        nan_T_row_index = df[nan_T_mask].index[0] # Get index of first NaN row\n",
    "        print(f\"Index of row with NaN in 'T': {nan_T_row_index}\")\n",
    "        print(\"Problematic row data:\")\n",
    "        print(df.loc[[nan_T_row_index]]) # Print the entire row\n",
    "    else:\n",
    "        print(\"\\n✅ No NaNs detected in 'T' column (so far in NaN check).\")\n",
    "\n",
    "\n",
    "    if np.isnan(X).any() or np.isnan(durations).any() or np.isnan(events).any():\n",
    "        raise ValueError(\"NaNs detected in input data, cannot proceed with training.\")\n",
    "    print(\"✅ No NaNs in input data for training.\")\n",
    "\n",
    "\n",
    "    # Train-Test Split (Rest of the function remains unchanged)\n",
    "    X_train, X_val, durations_train, durations_val, events_train, events_val = train_test_split(\n",
    "        X, durations, events, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(\"   ✅ Data split into training and validation sets.\")\n",
    "\n",
    "    # --- Clip extreme duration values ---\n",
    "    durations_train = np.clip(durations_train, 1, 100)\n",
    "    durations_val = np.clip(durations_val, 1, 100)\n",
    "    print(\"   ✅ Durations clipped to range [1, 100].\")\n",
    "    print(f\"   Durations_train min/max after clipping: {durations_train.min():.4f} / {durations_train.max():.4f}\")\n",
    "    print(f\"   Durations_val min/max after clipping: {durations_val.min():.4f} / {durations_val.max():.4f}\")\n",
    "\n",
    "\n",
    "    # Normalize Features - RobustScaler for outlier handling\n",
    "    scaler = RobustScaler(quantile_range=(5, 95)) # Replace StandardScaler with RobustScaler\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    print(\"   ✅ Features normalized using RobustScaler.\")\n",
    "    print(f\"NaNs in Scaled X_train: {np.isnan(X_train).sum()}\")\n",
    "    print(f\"NaNs in Scaled X_val: {np.isnan(X_val).sum()}\")\n",
    "    if np.isnan(X_train).any() or np.isnan(X_val).any():\n",
    "        raise ValueError(\"NaNs detected in scaled data, check feature engineering and scaling.\")\n",
    "    print(\"✅ No NaNs in scaled data.\")\n",
    "\n",
    "    # --- Additional Data Sanity Checks RIGHT BEFORE model.fit() ---\n",
    "    print(\"\\n📊 Data Sanity Check RIGHT BEFORE model.fit():\")\n",
    "    print(f\"X_train NaNs: {np.isnan(X_train).sum()}, Durations NaNs: {np.isnan(durations_train).sum()}, Events NaNs: {np.isnan(events_train).sum()}\")\n",
    "    print(f\"X_val NaNs: {np.isnan(X_val).sum()}, Durations Val NaNs: {np.isnan(durations_val).sum()}, Events Val NaNs: {np.isnan(events_val.sum())}\")\n",
    "\n",
    "    print(f\"X_train dtype: {X_train.dtype}, Durations dtype: {durations_train.dtype}, Events dtype: {events_train.dtype}\")\n",
    "\n",
    "    print(f\"X_train min/max: {X_train.min():.4f} / {X_train.max():.4f}\") # Min/Max might be different with RobustScaler\n",
    "    print(f\"Durations min/max: {durations_train.min():.4f} / {durations_train.max():.4f}\") # Should be clipped values now\n",
    "    print(f\"Events unique values: {np.unique(events_train)}\")\n",
    "\n",
    "\n",
    "    # DeepSurv Model Setup - IMPROVED CONFIGURATION (LR Scheduler from tt.callbacks - CORRECTED)\n",
    "    in_features = X_train.shape[1]\n",
    "    out_features = 1\n",
    "\n",
    "    net = tt.practical.MLPVanilla(\n",
    "        in_features,\n",
    "        [32],  # Very shallow architecture - ONE hidden layer, 32 units\n",
    "        out_features,\n",
    "        batch_norm=False, # Removed BatchNorm for simplicity\n",
    "        dropout=0.0,  # Removed dropout for simplicity\n",
    "        activation=nn.SELU  # Keep SELU\n",
    "    )\n",
    "\n",
    "    # Optimizer - Changed to torch.optim.AdamW (Standard PyTorch Optimizer) - CORRECTED with net.parameters() - REDUCED LEARNING RATE - FURTHER REDUCED LR\n",
    "    optimizer = torch.optim.AdamW( # CHANGED OPTIMIZER TO torch.optim.AdamW and ADDED net.parameters() - REDUCED LEARNING RATE - FURTHER REDUCED LR\n",
    "        net.parameters(), # ADDED net.parameters() as the first argument - CRITICAL FIX\n",
    "        lr=0.0001,  # FURTHER REDUCED LEARNING RATE to 0.0001 - to address NaN loss - EVEN LOWER LR\n",
    "        weight_decay=1e-4 # ADDED weight_decay - AdamW supports it directly\n",
    "        # cycle_eta_multiplier=0.8 # AdamW does not have cycle_eta_multiplier\n",
    "    )\n",
    "\n",
    "    model = models.CoxPH(\n",
    "        net,\n",
    "        optimizer,\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler -  REVERT TO torch.optim.lr_scheduler.ReduceLROnPlateau -  INITIALIZE ONLY - DO NOT PASS AS CALLBACK\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( # REVERT TO  torch.optim.lr_scheduler.ReduceLROnPlateau - INITIALIZE ONLY - DO NOT PASS AS CALLBACK\n",
    "        optimizer, # Pass optimizer to scheduler\n",
    "        patience=5,\n",
    "        factor=0.5,\n",
    "        min_lr=1e-5,\n",
    "        verbose=True # Added verbose for scheduler\n",
    "    )\n",
    "\n",
    "    # Early stopping (No change - REMOVED min_improvement) - CORRECTED EarlyStopping - REMOVED min_improvement\n",
    "    early_stop = tt.callbacks.EarlyStopping(\n",
    "        patience=15 # REMOVED min_improvement - likely not supported in this version - CORRECTED LINE\n",
    "        # min_improvement=0.001 # Removed argument causing error\n",
    "    )\n",
    "\n",
    "    # Gradient Clipping - REMOVED Gradient Clipping callback - GradientClipping IS NOT AVAILABLE in torchtuples.callbacks\n",
    "    # gradient_clipping = tt.callbacks.GradientClipping(clip_value=1.0) # Re-introduce Gradient Clipping - CLIP VALUE 1.0 - REMOVED LINE\n",
    "\n",
    "    # callbacks=[early_stop, gradient_clipping] # Include ONLY early_stop, gradient_clipping - REMOVED lr_scheduler\n",
    "    callbacks=[early_stop] # Include ONLY early_stop - REMOVE lr_scheduler and gradient_clipping - MODIFIED LINE - REMOVE gradient_clipping callback\n",
    "    print(\"\\n🚀 Training DeepSurv Model with Improved Configuration and Preprocessing...\")\n",
    "    print(f\"✅ DeepSurv Model Initialized with {in_features} input features.\")\n",
    "    print(f\"Hidden Layers: {[32]}, BatchNorm: False, Dropout: 0.0, Activation: SELU, Optimizer: AdamW, LR: 0.0001, Weight Decay: 1e-4\") # Optimizer is now AdamW, added weight decay back to print - LEARNING RATE REDUCED - FURTHER REDUCED LR\n",
    "    print(f\"Scaler: RobustScaler(quantile_range=(5, 95)), Duration Clipping: [1, 100], Gradient Clipping: REMOVED\") # Gradient Clipping REMOVED from print\n",
    "\n",
    "    epochs = 150 # Reduced for testing, increase later\n",
    "    batch_size = 512\n",
    "    verbose = True # Keep verbose for detailed output\n",
    "\n",
    "\n",
    "    print(f\"Training parameters: Epochs={epochs}, Batch Size={batch_size}, Verbose={verbose}, Callbacks: {[type(c).__name__ for c in callbacks]}\")\n",
    "\n",
    "    # --- Calculate class weights ---\n",
    "    event_ratio = events_train.mean()\n",
    "    weights = np.where(events_train == 1, 1/event_ratio, 1/(1-event_ratio))\n",
    "    print(\"\\n⚖️ Class Balancing: Sample Weights Calculated.\")\n",
    "    print(f\"   Event ratio in training data: {event_ratio:.4f}\")\n",
    "    print(f\"   Sample weights - Event weight: {1/event_ratio:.4f}, Censored weight: {1/(1-event_ratio):.4f}\")\n",
    "\n",
    "\n",
    "    # Training loop with callbacks and sample weights - SAMPLE_WEIGHT REMOVED FROM FIT\n",
    "    log = model.fit(\n",
    "        X_train, (durations_train, events_train),\n",
    "        # sample_weight=weights,  # REMOVED sample_weight argument - MODIFIED LINE\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs, # Use full epochs here, callbacks will handle early stopping\n",
    "        verbose=verbose,\n",
    "        val_data=(X_val, (durations_val, events_val)),\n",
    "        callbacks=callbacks, # Pass callbacks here - ONLY early_stop now\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    print(\"✅ DeepSurv model training completed.\")\n",
    "    print(\"\\n📈 Training Logs (last few epochs from model.log.to_pandas()):\")\n",
    "    print(log.to_pandas().tail())\n",
    "\n",
    "\n",
    "    # --- NEW: Check model.log and model parameters AFTER training (No change) - CORRECTED log printing - CORRECTED parameter inspection indexing\n",
    "    print(\"\\n📊 Checking model.log AFTER training:\")\n",
    "    if hasattr(model, 'log') and model.log:\n",
    "        print(f\"model.log type AFTER training: {type(model.log)}\")\n",
    "        print(f\"Sample model.log AFTER training:\\n{model.log.to_pandas().head()}\") # Print first few entries using .to_pandas().head() - CORRECTED LINE\n",
    "    else:\n",
    "        print(\"model.log is empty or not available.\")\n",
    "\n",
    "    print(\"\\n🔬 Inspecting Model Parameters (Weights/Biases - First Layer):\")\n",
    "    for name, param in model.net.named_parameters(): # net is the MLPVanilla network\n",
    "        if \"0\" in name and (\"weight\" in name or \"bias\" in name): # Inspect weights/biases of first layer\n",
    "            print(f\"Layer Parameter '{name}':\")\n",
    "            print(f\"  Shape: {param.shape}\")\n",
    "            print(f\"  Sample Values:\\n{param[:2].detach().numpy()}\") # Print snippet of weights/biases - CORRECTED INDEXING to [:2]\n",
    "            print(f\"  NaNs: {torch.isnan(param).sum()}\")\n",
    "            print(f\"  Inf: {torch.isinf(param).sum()}\")\n",
    "            print(f\"  Min/Max/Mean: {param.min().item():.4f} / {param.max().item():.4f} / {param.mean().item():.4f}\")\n",
    "\n",
    "\n",
    "    # Compute Baseline Hazards (No change)\n",
    "    print(\"\\n📊 Computing Baseline Hazards...\")\n",
    "    model.compute_baseline_hazards()\n",
    "    print(\"✅ Baseline hazards computed.\")\n",
    "\n",
    "    # Get Predictions (No change)\n",
    "    print(\"\\n🔮 Making Survival Predictions...\")\n",
    "    surv = model.predict_surv_df(X_val)\n",
    "\n",
    "    # Check for NaNs in Survival Predictions AGAIN after training (No change)\n",
    "    if np.isnan(surv.values).any():\n",
    "        print(\"\\n❌ WARNING: Survival Predictions STILL Contain NaN Values AFTER TRAINING!\")\n",
    "        nan_count_in_surv = np.isnan(surv.values).sum()\n",
    "        print(f\"   Number of NaNs in Survival Predictions: {nan_count_in_surv}\")\n",
    "    else:\n",
    "        print(\"\\n✅ No NaN values in survival predictions.\")\n",
    "\n",
    "    # Evaluate Model (No change)\n",
    "    print(\"\\n📈 Evaluating Model Performance...\")\n",
    "    ev = EvalSurv(surv, durations_val, events_val, censor_surv='km')\n",
    "    c_index = ev.concordance_td('antolini')\n",
    "    print(f\"\\n📊 DeepSurv Concordance Index: {c_index:.4f}\")\n",
    "\n",
    "    print(\"✅ DeepSurv model training and evaluation completed.\\n\")\n",
    "    return model, X_val, durations_val, events_val, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_deepsurv_model(model, X_val, durations_val, events_val):\n",
    "    \"\"\"\n",
    "    Evaluate DeepSurv Model Performance using Concordance Index\n",
    "    \"\"\"\n",
    "    surv = model.predict_surv_df(X_val)\n",
    "    surv.index = pd.to_numeric(surv.index, errors='coerce')\n",
    "\n",
    "    if 0 not in surv.index:\n",
    "        new_row = pd.DataFrame(np.ones((1, surv.shape[1])), index=[0], columns=surv.columns)\n",
    "        surv = pd.concat([new_row, surv])\n",
    "        surv = surv.sort_index()\n",
    "\n",
    "    ev = EvalSurv(surv, durations_val, events_val, censor_surv='km')\n",
    "    c_index = ev.concordance_td('antolini')\n",
    "\n",
    "    print(f\"\\n📊 DeepSurv Concordance Index: {c_index:.4f}\")\n",
    "    return c_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Enhanced Feature Engineering ---\n",
      "✅ Input columns validated.\n",
      "\n",
      "🔍 Checking and handling initial NaNs in 'Country'...\n",
      "   Initial NaNs in Country: 0\n",
      "   ✅ NaNs in 'Country' imputed with 'Unknown Country'.\n",
      "   NaNs in Country after imputation: 0 (Should be 0)\n",
      "\n",
      "🧹 Basic Cleaning...\n",
      "   ✅ Country and Gender cleaned.\n",
      "   Sample age_df countries after cleaning: ['United States of America' 'United Kingdom' 'Archduchy of Austria'\n",
      " 'Holy Roman Empire' 'Kingdom of France' 'France' 'Spain'\n",
      " 'Grand Duchy of Tuscany' 'Chile' 'Nazi Germany' 'Kingdom of Castile'\n",
      " 'Kingdom of the Netherlands' 'Byelorussian Soviet Socialist Republic'\n",
      " 'Czech Republic' 'Jamaica' 'German Empire' 'Denmark' 'Soviet Union'\n",
      " 'French Third Republic' 'Unknown Country']\n",
      "   Sample global_le_df countries: ['Aruba' 'Afghanistan' 'Angola' 'Albania' 'United Arab Emirates'\n",
      " 'Argentina' 'Armenia' 'Antigua and Barbuda' 'Australia' 'Austria'\n",
      " 'Azerbaijan' 'Burundi' 'Belgium' 'Benin' 'Burkina Faso' 'Bangladesh'\n",
      " 'Bulgaria' 'Bahrain' 'Bahamas, The' 'Bosnia and Herzegovina']\n",
      "\n",
      "🔍 Sample global_le_agg countries BEFORE merge:\n",
      "   Unique countries in global_le_agg: ['Afghanistan' 'Albania' 'Algeria' 'Angola' 'Antigua and Barbuda'\n",
      " 'Argentina' 'Armenia' 'Aruba' 'Australia' 'Austria' 'Azerbaijan'\n",
      " 'Bahamas, The' 'Bahrain' 'Bangladesh' 'Barbados' 'Belarus' 'Belgium'\n",
      " 'Belize' 'Benin' 'Bermuda']\n",
      "🔍 Checking for NaNs after basic cleaning:\n",
      "   NaNs in Country: 0\n",
      "   NaNs in Gender: 0\n",
      "\n",
      "🧬 Engineering Clinical Features...\n",
      "   ✅ Stress score engineered.\n",
      "   ✅ Avg BMI engineered (using z-score and imputation).\n",
      "   ✅ Smoking prevalence engineered.\n",
      "🔍 Checking for NaNs after clinical features:\n",
      "   NaNs in stress_score: 0\n",
      "   NaNs in avg_bmi: 0\n",
      "   NaNs in bmi_zscore: 0\n",
      "   NaNs in smoking_prev: 0\n",
      "\n",
      "🌍 Engineering Country-Level Features...\n",
      "\n",
      "Sample global_le_agg before merge:\n",
      "               Country  Life_Exp_Value\n",
      "0          Afghanistan         64.8330\n",
      "1              Albania         78.5730\n",
      "2              Algeria         76.8800\n",
      "3               Angola         61.1470\n",
      "4  Antigua and Barbuda         77.0160\n",
      "   NaNs in global_le_agg['Country']: 0\n",
      "   NaNs in global_le_agg['Life_Exp_Value']: 0\n",
      "   ✅ Global Life Expectancy engineered.\n",
      "🔍 Checking for NaNs after country-level features:\n",
      "   NaNs in global_life_exp: 0\n",
      "\n",
      "⏰ Engineering Temporal Features...\n",
      "   ✅ Temporal features 'birth_decade' and 'death_decade' engineered.\n",
      "🔍 Checking for NaNs after temporal features:\n",
      "   NaNs in birth_decade: 0\n",
      "   NaNs in death_decade: 0\n",
      "\n",
      "<binary data, 1 bytes><binary data, 1 bytes> Creating Interaction Terms...\n",
      "   ✅ Interaction terms 'stress_x_bmi' and 'smoking_x_lifeexp' created.\n",
      "🔍 Checking for NaNs after interaction terms:\n",
      "   NaNs in stress_x_bmi: 0\n",
      "   NaNs in smoking_x_lifeexp: 0\n",
      "\n",
      "⏳ Survival Data Setup...\n",
      "   ✅ Survival time (T) and censoring engineered.\n",
      "\n",
      "🔍 Final NaN check before returning:\n",
      "   NaNs in censored: 0\n",
      "   NaNs in T: 0\n",
      "\n",
      "Sample processed_batch after feature engineering:\n",
      "     Id                     Name                                Short description  Gender                   Country  Occupation  Birth year  Death year Manner of death  Age of death  event       T  stress_score  bmi_zscore  avg_bmi  smoking_prev  global_life_exp  birth_decade  death_decade  stress_x_bmi  smoking_x_lifeexp  censored\n",
      "0   Q23        George Washington   1st president of the United States (1732–1799)  0.5000  United States of America  Politician        1732   1799.0000  natural causes       67.0000      1 67.0000        1.0000     -1.9272  65.4000        0.6000          81.8951          1730          1790       65.4000            49.1371         0\n",
      "1   Q42            Douglas Adams                      English writer and humorist  0.5000            United Kingdom      Artist        1952   2001.0000  natural causes       49.0000      1 49.0000        0.5556      0.0000  25.0000        0.4502          81.2049          1950          2000       13.8889            36.5557         0\n",
      "2   Q91          Abraham Lincoln  16th president of the United States (1809-1865)  0.5000  United States of America  Politician        1809   1865.0000        homicide       56.0000      1 56.0000        1.0000     -1.9762  65.4000        0.6000          81.8951          1800          1860       65.4000            49.1371         0\n",
      "3  Q254  Wolfgang Amadeus Mozart        Austrian composer of the Classical period  0.5000      Archduchy of Austria      Artist        1756   1791.0000             NaN       35.0000      1 35.0000        0.5556      0.0000  25.0000        0.6000          81.8951          1750          1790       13.8889            49.1371         0\n",
      "4  Q255     Ludwig van Beethoven           German classical and romantic composer  0.5000         Holy Roman Empire      Artist        1770   1827.0000             NaN       57.0000      1 57.0000        0.5556      0.0000  25.0000        0.6000          81.8951          1770          1820       13.8889            49.1371         0\n",
      "   NaNs in processed_batch after FE: 0\n",
      "✅ Enhanced feature engineering completed.\n",
      "\n",
      "   Sample age_df countries: ['United States of America' 'United Kingdom' 'Archduchy of Austria'\n",
      " 'Holy Roman Empire' 'Kingdom of France' 'France' 'Spain'\n",
      " 'Grand Duchy of Tuscany' 'Chile' 'Nazi Germany' 'Kingdom of Castile'\n",
      " 'Kingdom of the Netherlands' 'Byelorussian Soviet Socialist Republic'\n",
      " 'Czech Republic' 'Jamaica' 'German Empire' 'Denmark' 'Soviet Union'\n",
      " 'French Third Republic' 'Unknown Country']\n",
      "\n",
      "--- Train DeepSurv Model ---\n",
      "✅ Required features validated.\n",
      "\n",
      "📊 Checking Data Before Training DeepSurv (Detailed NaN Check):\n",
      "Feature Shape: (1223009, 8), Durations Shape: (1223009,), Events Shape: (1223009,)\n",
      "NaN counts per column BEFORE training:\n",
      "stress_score         0\n",
      "avg_bmi              0\n",
      "smoking_prev         0\n",
      "global_life_exp      0\n",
      "birth_decade         0\n",
      "bmi_zscore           0\n",
      "stress_x_bmi         0\n",
      "smoking_x_lifeexp    0\n",
      "T                    0\n",
      "censored             0\n",
      "dtype: int64\n",
      "Total NaNs in Features and Targets BEFORE training: 0\n",
      "Total NaNs in Features (X): 0\n",
      "NaNs in Durations: 0\n",
      "NaNs in Events: 0\n",
      "Sample Features:\n",
      "[[ 1.0000000e+00  6.5400002e+01  6.0000002e-01  8.1895119e+01\n",
      "   1.7300000e+03 -1.9272212e+00  6.5400002e+01  4.9137074e+01]\n",
      " [ 5.5555558e-01  2.5000000e+01  4.5016602e-01  8.1204880e+01\n",
      "   1.9500000e+03  0.0000000e+00  1.3888889e+01  3.6555676e+01]\n",
      " [ 1.0000000e+00  6.5400002e+01  6.0000002e-01  8.1895119e+01\n",
      "   1.8000000e+03 -1.9761976e+00  6.5400002e+01  4.9137074e+01]\n",
      " [ 5.5555558e-01  2.5000000e+01  6.0000002e-01  8.1895119e+01\n",
      "   1.7500000e+03  0.0000000e+00  1.3888889e+01  4.9137074e+01]\n",
      " [ 5.5555558e-01  2.5000000e+01  6.0000002e-01  8.1895119e+01\n",
      "   1.7700000e+03  0.0000000e+00  1.3888889e+01  4.9137074e+01]]\n",
      "Sample Durations: [67. 49. 56. 35. 57.]\n",
      "Sample Events: [0 0 0 0 0]\n",
      "Unique event values (should be 0 or 1): [0 1]\n",
      "\n",
      "✅ No NaNs detected in 'T' column (so far in NaN check).\n",
      "✅ No NaNs in input data for training.\n",
      "   ✅ Data split into training and validation sets.\n",
      "   ✅ Durations clipped to range [1, 100].\n",
      "   Durations_train min/max after clipping: 1.0000 / 100.0000\n",
      "   Durations_val min/max after clipping: 1.0000 / 100.0000\n",
      "   ✅ Features normalized using RobustScaler.\n",
      "NaNs in Scaled X_train: 0\n",
      "NaNs in Scaled X_val: 0\n",
      "✅ No NaNs in scaled data.\n",
      "\n",
      "📊 Data Sanity Check RIGHT BEFORE model.fit():\n",
      "X_train NaNs: 0, Durations NaNs: 0, Events NaNs: 0\n",
      "X_val NaNs: 0, Durations Val NaNs: 0, Events Val NaNs: False\n",
      "X_train dtype: float32, Durations dtype: float32, Events dtype: int64\n",
      "X_train min/max: -26.3700 / 19.9976\n",
      "Durations min/max: 1.0000 / 100.0000\n",
      "Events unique values: [0 1]\n",
      "\n",
      "🚀 Training DeepSurv Model with Improved Configuration and Preprocessing...\n",
      "✅ DeepSurv Model Initialized with 8 input features.\n",
      "Hidden Layers: [32], BatchNorm: False, Dropout: 0.0, Activation: SELU, Optimizer: AdamW, LR: 0.0001, Weight Decay: 1e-4\n",
      "Scaler: RobustScaler(quantile_range=(5, 95)), Duration Clipping: [1, 100], Gradient Clipping: REMOVED\n",
      "Training parameters: Epochs=150, Batch Size=512, Verbose=True, Callbacks: ['EarlyStopping']\n",
      "\n",
      "⚖️ Class Balancing: Sample Weights Calculated.\n",
      "   Event ratio in training data: 0.0109\n",
      "   Sample weights - Event weight: 91.4912, Censored weight: 1.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[6s / 6s],\t\n",
      "1:\t[6s / 13s],\t\n",
      "2:\t[6s / 20s],\t\n",
      "3:\t[6s / 27s],\t\n",
      "4:\t[6s / 33s],\t\n",
      "5:\t[6s / 40s],\t\n",
      "6:\t[6s / 47s],\t\n",
      "7:\t[6s / 53s],\t\n",
      "8:\t[6s / 1m:0s],\t\n",
      "9:\t[6s / 1m:7s],\t\n",
      "10:\t[6s / 1m:14s],\t\n",
      "11:\t[6s / 1m:20s],\t\n",
      "12:\t[6s / 1m:27s],\t\n",
      "13:\t[6s / 1m:34s],\t\n",
      "14:\t[7s / 1m:41s],\t\n",
      "✅ DeepSurv model training completed.\n",
      "\n",
      "📈 Training Logs (last few epochs from model.log.to_pandas()):\n",
      "    train_loss  val_loss\n",
      "10         NaN       NaN\n",
      "11         NaN       NaN\n",
      "12         NaN       NaN\n",
      "13         NaN       NaN\n",
      "14         NaN       NaN\n",
      "\n",
      "📊 Checking model.log AFTER training:\n",
      "model.log type AFTER training: <class 'torchtuples.callbacks.TrainingLogger'>\n",
      "Sample model.log AFTER training:\n",
      "   train_loss  val_loss\n",
      "0         NaN       NaN\n",
      "1         NaN       NaN\n",
      "2         NaN       NaN\n",
      "3         NaN       NaN\n",
      "4         NaN       NaN\n",
      "\n",
      "🔬 Inspecting Model Parameters (Weights/Biases - First Layer):\n",
      "Layer Parameter 'net.0.linear.weight':\n",
      "  Shape: torch.Size([32, 8])\n",
      "  Sample Values:\n",
      "[[ 0.15629235 -0.36014286 -0.5573728  -0.38159636  0.29701096  0.34326604\n",
      "  -0.62213814  0.48039338]\n",
      " [-0.0368949   0.36470863  0.14435577 -0.27893332 -0.12677173 -0.6353421\n",
      "   0.17141114 -0.16520025]]\n",
      "  NaNs: 0\n",
      "  Inf: 0\n",
      "  Min/Max/Mean: -1.6515 / 1.3520 / 0.0391\n",
      "Layer Parameter 'net.0.linear.bias':\n",
      "  Shape: torch.Size([32])\n",
      "  Sample Values:\n",
      "[0.14755826 0.19957975]\n",
      "  NaNs: 0\n",
      "  Inf: 0\n",
      "  Min/Max/Mean: -0.2379 / 0.3521 / 0.0952\n",
      "\n",
      "📊 Computing Baseline Hazards...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtuples/base.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(path, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Baseline hazards computed.\n",
      "\n",
      "🔮 Making Survival Predictions...\n",
      "\n",
      "✅ No NaN values in survival predictions.\n",
      "\n",
      "📈 Evaluating Model Performance...\n",
      "\n",
      "📊 DeepSurv Concordance Index: 0.6998\n",
      "✅ DeepSurv model training and evaluation completed.\n",
      "\n",
      "\n",
      "📊 DeepSurv Concordance Index: 0.6998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6997741340135601"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1️⃣ Process the Dataset\n",
    "processed_batch = enhanced_feature_engineering(age_df, life_exp_df, global_le_df, death_rates_df)\n",
    "print(f\"   Sample age_df countries: {processed_batch['Country'].unique()[:20]}\") # Print first 20 unique countries\n",
    "\n",
    "# 2️⃣ Train DeepSurv\n",
    "deepsurv_model, X_val, durations_val, events_val, log = train_deepsurv_model(processed_batch)\n",
    "\n",
    "# 3️⃣ Evaluate Model\n",
    "evaluate_deepsurv_model(deepsurv_model, X_val, durations_val, events_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot loss curves\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mlog\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(log\u001b[38;5;241m.\u001b[39mto_pandas()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Convergence\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(log.to_pandas()['train_loss'], label='Train')\n",
    "plt.plot(log.to_pandas()['val_loss'], label='Validation')\n",
    "plt.title('Training Convergence')\n",
    "plt.ylabel('Negative Log Likelihood')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance analysis\n",
    "explainer = shap.DeepExplainer(model.net, torch.FloatTensor(X_train[:1000]))\n",
    "shap_values = explainer.shap_values(torch.FloatTensor(X_val[:100]))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_val[:100], feature_names=feature_list)\n",
    "plt.title('Feature Importance Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
