{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Survival Analysis Revamp: Death Prediction 2.0**\n",
    "## **Project Overview**\n",
    "This project aims to revamp the original **death prediction model** into a **production-grade survival analysis system**. Instead of directly predicting an age of death, we model the **probability of survival over time**, accounting for censoring (individuals still alive).\n",
    "\n",
    "## **Why Survival Analysis?**\n",
    "Survival analysis is widely used in **healthcare, finance, and customer retention**:\n",
    "- **Healthcare:** Predict patient survival rates.\n",
    "- **Finance:** Credit risk and loan default probabilities.\n",
    "- **Subscription Businesses:** Customer churn prediction (e.g., Netflix, Spotify).\n",
    "\n",
    "## **Key Steps**\n",
    "### **1️⃣ Reframe as Survival Analysis**\n",
    "- Convert the dataset to survival format.\n",
    "- Use Python’s `lifelines` and PyTorch-based `pycox`.\n",
    "- Handle **censored data** (people still alive in 2024).\n",
    "\n",
    "### **2️⃣ Train Survival Models**\n",
    "- **Traditional Cox Proportional Hazards Model (`lifelines`)**\n",
    "- **DeepSurv (Neural Networks for Survival Analysis)**\n",
    "- **Transformer-based Time-to-Event Models (TFTs, Hugging Face Transformers)**\n",
    "\n",
    "### **3️⃣ Deploy as an API**\n",
    "- Wrap the trained model in a **FastAPI** server.\n",
    "- Package with **Docker**.\n",
    "- Deploy using **Google Cloud Run / AWS Lambda**.\n",
    "\n",
    "## **Technologies Used**\n",
    "- **Libraries:** `lifelines`, `pycox`, `FastAPI`, `Hugging Face Transformers`\n",
    "- **Model Training:** Traditional (Cox Model) & Deep Learning (DeepSurv, TFT)\n",
    "- **Deployment:** FastAPI, Docker, Google Cloud Run/AWS Lambda\n",
    "\n",
    "---\n",
    "\n",
    "> 📌 **Next Steps:** Run the first code cell to preprocess the dataset and train the baseline Cox Proportional Hazards Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers > /dev/null 2>&1\n",
    "!pip install xgboost > /dev/null 2>&1\n",
    "!pip install scikit-learn==1.4.2 scikit-survival==0.23.1 > /dev/null 2>&1\n",
    "!pip install torchtuples > /dev/null 2>&1\n",
    "!pip install pycox > /dev/null 2>&1\n",
    "!pip install numpy==1.21.5  > /dev/null 2>&1\n",
    "!pip install interpret-core  > /dev/null 2>&1\n",
    "!pip install node-ts  > /dev/null 2>&1\n",
    "!pip install lightgbm > /dev/null 2>&1\n",
    "!pip install shap > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "import os\n",
    "import gc\n",
    "import torchtuples as tt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, mean_squared_log_error, explained_variance_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from scipy.stats import norm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sksurv.util import Surv\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from pycox.models import CoxPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Country  Year      Status  Life expectancy   Adult Mortality  \\\n",
      "0  Afghanistan  2015  Developing              65.0            263.0   \n",
      "1  Afghanistan  2014  Developing              59.9            271.0   \n",
      "2  Afghanistan  2013  Developing              59.9            268.0   \n",
      "3  Afghanistan  2012  Developing              59.5            272.0   \n",
      "4  Afghanistan  2011  Developing              59.2            275.0   \n",
      "\n",
      "   infant deaths  Alcohol  percentage expenditure  Hepatitis B  Measles   ...  \\\n",
      "0             62     0.01               71.279624         65.0      1154  ...   \n",
      "1             64     0.01               73.523582         62.0       492  ...   \n",
      "2             66     0.01               73.219243         64.0       430  ...   \n",
      "3             69     0.01               78.184215         67.0      2787  ...   \n",
      "4             71     0.01                7.097109         68.0      3013  ...   \n",
      "\n",
      "   Polio  Total expenditure  Diphtheria    HIV/AIDS         GDP  Population  \\\n",
      "0    6.0               8.16         65.0        0.1  584.259210  33736494.0   \n",
      "1   58.0               8.18         62.0        0.1  612.696514    327582.0   \n",
      "2   62.0               8.13         64.0        0.1  631.744976  31731688.0   \n",
      "3   67.0               8.52         67.0        0.1  669.959000   3696958.0   \n",
      "4   68.0               7.87         68.0        0.1   63.537231   2978599.0   \n",
      "\n",
      "    thinness  1-19 years   thinness 5-9 years  \\\n",
      "0                   17.2                 17.3   \n",
      "1                   17.5                 17.5   \n",
      "2                   17.7                 17.7   \n",
      "3                   17.9                 18.0   \n",
      "4                   18.2                 18.2   \n",
      "\n",
      "   Income composition of resources  Schooling  \n",
      "0                            0.479       10.1  \n",
      "1                            0.476       10.0  \n",
      "2                            0.470        9.9  \n",
      "3                            0.463        9.8  \n",
      "4                            0.454        9.5  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
      "0   40   M           ATA        140          289          0     Normal    172   \n",
      "1   49   F           NAP        160          180          0     Normal    156   \n",
      "2   37   M           ATA        130          283          0         ST     98   \n",
      "3   48   F           ASY        138          214          0     Normal    108   \n",
      "4   54   M           NAP        150          195          0     Normal    122   \n",
      "\n",
      "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
      "0              N      0.0       Up             0  \n",
      "1              N      1.0     Flat             1  \n",
      "2              N      0.0       Up             0  \n",
      "3              Y      1.5     Flat             1  \n",
      "4              N      0.0       Up             0  \n",
      "     Id                     Name  \\\n",
      "0   Q23        George Washington   \n",
      "1   Q42            Douglas Adams   \n",
      "2   Q91          Abraham Lincoln   \n",
      "3  Q254  Wolfgang Amadeus Mozart   \n",
      "4  Q255     Ludwig van Beethoven   \n",
      "\n",
      "                                 Short description Gender  \\\n",
      "0   1st president of the United States (1732–1799)   Male   \n",
      "1                      English writer and humorist   Male   \n",
      "2  16th president of the United States (1809-1865)   Male   \n",
      "3        Austrian composer of the Classical period   Male   \n",
      "4           German classical and romantic composer   Male   \n",
      "\n",
      "                                             Country  Occupation  Birth year  \\\n",
      "0  United States of America; Kingdom of Great Bri...  Politician        1732   \n",
      "1                                     United Kingdom      Artist        1952   \n",
      "2                           United States of America  Politician        1809   \n",
      "3    Archduchy of Austria; Archbishopric of Salzburg      Artist        1756   \n",
      "4                 Holy Roman Empire; Austrian Empire      Artist        1770   \n",
      "\n",
      "   Death year Manner of death  Age of death  \n",
      "0      1799.0  natural causes          67.0  \n",
      "1      2001.0  natural causes          49.0  \n",
      "2      1865.0        homicide          56.0  \n",
      "3      1791.0             NaN          35.0  \n",
      "4      1827.0             NaN          57.0  \n"
     ]
    }
   ],
   "source": [
    "# Download Life Expectancy dataset\n",
    "life_exp_path = kagglehub.dataset_download(\"kumarajarshi/life-expectancy-who\")\n",
    "life_exp_file = os.path.join(life_exp_path, \"Life Expectancy Data.csv\")\n",
    "life_exp_df = pd.read_csv(life_exp_file)\n",
    "\n",
    "print(life_exp_df.head())\n",
    "\n",
    "heart_path = kagglehub.dataset_download(\"fedesoriano/heart-failure-prediction\")\n",
    "heart_file = os.path.join(heart_path, \"heart.csv\")\n",
    "heart_df = pd.read_csv(heart_file)\n",
    "\n",
    "print(heart_df.head())\n",
    "\n",
    "age_path = kagglehub.dataset_download(\"imoore/age-dataset\")\n",
    "age_file = os.path.join(age_path, \"AgeDataset-V1.csv\")  #\n",
    "age_df = pd.read_csv(age_file)\n",
    "\n",
    "print(age_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life Expectancy Columns: ['Country', 'Year', 'Status', 'Life expectancy ', 'Adult Mortality', 'infant deaths', 'Alcohol', 'percentage expenditure', 'Hepatitis B', 'Measles ', ' BMI ', 'under-five deaths ', 'Polio', 'Total expenditure', 'Diphtheria ', ' HIV/AIDS', 'GDP', 'Population', ' thinness  1-19 years', ' thinness 5-9 years', 'Income composition of resources', 'Schooling']\n",
      "Years: [2015 2014 2013 2012 2011 2010 2009 2008 2007 2006 2005 2004 2003 2002\n",
      " 2001 2000]\n",
      "Missing Values:\n",
      " Country                              0\n",
      "Year                                 0\n",
      "Status                               0\n",
      "Life expectancy                     10\n",
      "Adult Mortality                     10\n",
      "infant deaths                        0\n",
      "Alcohol                            194\n",
      "percentage expenditure               0\n",
      "Hepatitis B                        553\n",
      "Measles                              0\n",
      " BMI                                34\n",
      "under-five deaths                    0\n",
      "Polio                               19\n",
      "Total expenditure                  226\n",
      "Diphtheria                          19\n",
      " HIV/AIDS                            0\n",
      "GDP                                448\n",
      "Population                         652\n",
      " thinness  1-19 years               34\n",
      " thinness 5-9 years                 34\n",
      "Income composition of resources    167\n",
      "Schooling                          163\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Life Expectancy Columns:\", life_exp_df.columns.tolist())\n",
    "print(\"Years:\", life_exp_df['Year'].unique())\n",
    "print(\"Missing Values:\\n\", life_exp_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart Failure Columns: ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope', 'HeartDisease']\n",
      "Missing Values:\n",
      " Age               0\n",
      "Sex               0\n",
      "ChestPainType     0\n",
      "RestingBP         0\n",
      "Cholesterol       0\n",
      "FastingBS         0\n",
      "RestingECG        0\n",
      "MaxHR             0\n",
      "ExerciseAngina    0\n",
      "Oldpeak           0\n",
      "ST_Slope          0\n",
      "HeartDisease      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Heart Failure Columns:\", heart_df.columns.tolist())\n",
    "print(\"Missing Values:\\n\", heart_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Dataset Columns: ['Id', 'Name', 'Short description', 'Gender', 'Country', 'Occupation', 'Birth year', 'Death year', 'Manner of death', 'Age of death']\n",
      "Missing Values:\n",
      " Id                         0\n",
      "Name                       0\n",
      "Short description      67900\n",
      "Gender                133646\n",
      "Country               335509\n",
      "Occupation            206914\n",
      "Birth year                 0\n",
      "Death year                 1\n",
      "Manner of death      1169406\n",
      "Age of death               1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Age Dataset Columns:\", age_df.columns.tolist())\n",
    "print(\"Missing Values:\\n\", age_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/tmp/ipykernel_3965/3236614853.py:24: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  life_exp_df = life_exp_df.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Missing Values:\n",
      "Life Expectancy:\n",
      " Country                   0\n",
      "Year                      0\n",
      "Status                    0\n",
      "Life expectancy           0\n",
      "Adult Mortality           0\n",
      "infant deaths             0\n",
      "Alcohol                   0\n",
      "percentage expenditure    0\n",
      "Measles                   0\n",
      " BMI                      0\n",
      "under-five deaths         0\n",
      "Polio                     0\n",
      "Total expenditure         0\n",
      "Diphtheria                0\n",
      " HIV/AIDS                 0\n",
      "GDP                       0\n",
      "Schooling                 0\n",
      "dtype: int64\n",
      "\n",
      "Heart Failure:\n",
      " Age                  0\n",
      "Sex                  0\n",
      "RestingBP            0\n",
      "Cholesterol          0\n",
      "FastingBS            0\n",
      "MaxHR                0\n",
      "ExerciseAngina       0\n",
      "Oldpeak              0\n",
      "HeartDisease         0\n",
      "ChestPainType_ATA    0\n",
      "ChestPainType_NAP    0\n",
      "ChestPainType_TA     0\n",
      "RestingECG_Normal    0\n",
      "RestingECG_ST        0\n",
      "ST_Slope_Flat        0\n",
      "ST_Slope_Up          0\n",
      "dtype: int64\n",
      "\n",
      "Age Dataset:\n",
      " Id                   0\n",
      "Name                 0\n",
      "Short description    0\n",
      "Gender               0\n",
      "Country              0\n",
      "Occupation           0\n",
      "Birth year           0\n",
      "Death year           0\n",
      "Age of death         0\n",
      "dtype: int64\n",
      "\n",
      "Sample Categories:\n",
      "Occupations: ['Politician' 'Artist' 'Other' 'Astronomer' 'Athlete' 'Researcher'\n",
      " 'Military personnel' 'Philosopher' 'Businessperson' 'Explorer']\n",
      "Countries: ['United States of America' 'United Kingdom' 'Archduchy of Austria'\n",
      " 'Holy Roman Empire' 'Kingdom of France' 'France' 'Spain'\n",
      " 'Grand Duchy of Tuscany' 'Chile' 'Nazi Germany']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Life Expectancy Dataset --------------------------\n",
    "# Drop rows with missing target\n",
    "life_exp_df = life_exp_df.dropna(subset=['Life expectancy '])\n",
    "\n",
    "# Fill Alcohol: country/year median → global median if still missing\n",
    "life_exp_df['Alcohol'] = life_exp_df.groupby(['Country', 'Year'])['Alcohol'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "life_exp_df['Alcohol'] = life_exp_df['Alcohol'].fillna(life_exp_df['Alcohol'].median())\n",
    "\n",
    "# Fill GDP: country median → global median\n",
    "life_exp_df['GDP'] = life_exp_df.groupby('Country')['GDP'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "life_exp_df['GDP'] = life_exp_df['GDP'].fillna(life_exp_df['GDP'].median())\n",
    "\n",
    "# Drop unnecessary columns\n",
    "life_exp_df = life_exp_df.drop(columns=[\n",
    "    'Hepatitis B', 'Population', 'Income composition of resources',\n",
    "    ' thinness  1-19 years', ' thinness 5-9 years'\n",
    "])\n",
    "\n",
    "# Final fill for any remaining nulls\n",
    "life_exp_df = life_exp_df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# -------------------------- Heart Failure Dataset ----------------------------\n",
    "# Convert categoricals\n",
    "heart_df = pd.get_dummies(\n",
    "    heart_df, \n",
    "    columns=['ChestPainType', 'RestingECG', 'ST_Slope'],\n",
    "    drop_first=True\n",
    ")\n",
    "heart_df['ExerciseAngina'] = heart_df['ExerciseAngina'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "# ---------------------------- Age Dataset ------------------------------------\n",
    "# Drop death-related missingness\n",
    "age_df = age_df.dropna(subset=['Death year', 'Age of death'])\n",
    "\n",
    "# Clean categorical columns\n",
    "for col in ['Gender', 'Country', 'Occupation', 'Short description']:\n",
    "    age_df[col] = age_df[col].fillna('Unknown')\n",
    "\n",
    "# Simplify country names\n",
    "age_df['Country'] = age_df['Country'].str.split(';').str[0]\n",
    "\n",
    "# Group rare occupations (threshold = 1000)\n",
    "occupation_counts = age_df['Occupation'].value_counts()\n",
    "age_df['Occupation'] = np.where(\n",
    "    age_df['Occupation'].isin(occupation_counts[occupation_counts >= 1000].index),\n",
    "    age_df['Occupation'],\n",
    "    'Other'\n",
    ")\n",
    "\n",
    "# Encode gender (handle unknowns)\n",
    "age_df['Gender'] = np.where(\n",
    "    age_df['Gender'] == 'Male', 1,\n",
    "    np.where(age_df['Gender'] == 'Female', 0, 0.5)\n",
    ")\n",
    "\n",
    "# Drop unnecessary column\n",
    "age_df = age_df.drop(columns=['Manner of death'])\n",
    "\n",
    "# ---------------------------- Validation -------------------------------------\n",
    "print(\"\\nFinal Missing Values:\")\n",
    "print(\"Life Expectancy:\\n\", life_exp_df.isnull().sum())\n",
    "print(\"\\nHeart Failure:\\n\", heart_df.isnull().sum())\n",
    "print(\"\\nAge Dataset:\\n\", age_df.isnull().sum())\n",
    "\n",
    "print(\"\\nSample Categories:\")\n",
    "print(\"Occupations:\", age_df['Occupation'].unique()[:10])\n",
    "print(\"Countries:\", age_df['Country'].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Dataset Occupations: ['Politician' 'Artist' 'Other' 'Astronomer' 'Athlete' 'Researcher'\n",
      " 'Military personnel' 'Philosopher' 'Businessperson' 'Explorer'\n",
      " 'Architect' 'Teacher' 'Aristocrat' 'Entrepreneur' 'Journalist' 'Engineer'\n",
      " 'Author' 'Unknown' 'Religious figure' 'Official']\n",
      "Life Expectancy Countries: ['Afghanistan' 'Albania' 'Algeria' 'Angola' 'Antigua and Barbuda'\n",
      " 'Argentina' 'Armenia' 'Australia' 'Austria' 'Azerbaijan' 'Bahamas'\n",
      " 'Bahrain' 'Bangladesh' 'Barbados' 'Belarus' 'Belgium' 'Belize' 'Benin'\n",
      " 'Bhutan' 'Bolivia (Plurinational State of)']\n",
      "Life Expectancy Missing After Cleaning: Country                   0\n",
      "Year                      0\n",
      "Status                    0\n",
      "Life expectancy           0\n",
      "Adult Mortality           0\n",
      "infant deaths             0\n",
      "Alcohol                   0\n",
      "percentage expenditure    0\n",
      "Measles                   0\n",
      " BMI                      0\n",
      "under-five deaths         0\n",
      "Polio                     0\n",
      "Total expenditure         0\n",
      "Diphtheria                0\n",
      " HIV/AIDS                 0\n",
      "GDP                       0\n",
      "Schooling                 0\n",
      "dtype: int64\n",
      "Age Dataset Missing After Cleaning: Id                   0\n",
      "Name                 0\n",
      "Short description    0\n",
      "Gender               0\n",
      "Country              0\n",
      "Occupation           0\n",
      "Birth year           0\n",
      "Death year           0\n",
      "Age of death         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Age Dataset Occupations:\", age_df['Occupation'].unique()[:20])\n",
    "print(\"Life Expectancy Countries:\", life_exp_df['Country'].unique()[:20])\n",
    "\n",
    "print(\"Life Expectancy Missing After Cleaning:\", life_exp_df.isnull().sum())\n",
    "print(\"Age Dataset Missing After Cleaning:\", age_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings for batch processing\n",
    "BATCH_SIZE = 50000\n",
    "N_ITERATIONS = 5\n",
    "BASE_RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_engineer_features(df, life_exp_df):\n",
    "    \"\"\"\n",
    "    Given a raw batch (df) from the Age dataset and the life expectancy dataframe,\n",
    "    perform data cleaning and enhanced synthetic feature engineering.\n",
    "    \"\"\"\n",
    "    # --- Preprocessing & Cleaning ---\n",
    "    df = df.dropna(subset=['Death year', 'Age of death'])\n",
    "    for col in ['Gender', 'Country', 'Occupation', 'Short description']:\n",
    "        df[col] = df[col].fillna('Unknown')\n",
    "    df['Country'] = df['Country'].str.split(';').str[0]\n",
    "    occupation_counts = df['Occupation'].value_counts()\n",
    "    df['Occupation'] = np.where(\n",
    "        df['Occupation'].isin(occupation_counts[occupation_counts >= 1000].index),\n",
    "        df['Occupation'],\n",
    "        'Other'\n",
    "    )\n",
    "    # Encode Gender: Male=1, Female=0, Others=0.5\n",
    "    df['Gender'] = np.where(df['Gender'] == 'Male', 1,\n",
    "                            np.where(df['Gender'] == 'Female', 0, 0.5))\n",
    "    if 'Manner of death' in df.columns:\n",
    "        df = df.drop(columns=['Manner of death'])\n",
    "    \n",
    "    # --- Enhanced Synthetic Feature Engineering ---\n",
    "    ## 1. Stress Score (normalize to [0, 1])\n",
    "    stress_map = {\n",
    "        'Politician': 9, 'Military personnel': 8, 'Journalist': 7,\n",
    "        'Businessperson': 6, 'Artist': 5, 'Teacher': 4, \n",
    "        'Researcher': 3, 'Other': 5, 'Unknown': 5\n",
    "    }\n",
    "    df['stress_score_raw'] = df['Occupation'].map(stress_map).fillna(5).astype('float32')\n",
    "    # Normalize: divide by the maximum (assume 9 is maximum)\n",
    "    df['stress_score'] = df['stress_score_raw'] / 9.0\n",
    "\n",
    "    ## 2. BMI from Country\n",
    "    # Use median BMI per country for robustness\n",
    "    country_bmi = life_exp_df.groupby('Country')[' BMI '].median().to_dict()\n",
    "    df['avg_bmi_raw'] = df['Country'].map(country_bmi).fillna(25).astype('float32')\n",
    "    # Standardize BMI: subtract mean and divide by standard deviation (using the batch statistics)\n",
    "    df['avg_bmi'] = (df['avg_bmi_raw'] - df['avg_bmi_raw'].mean()) / df['avg_bmi_raw'].std()\n",
    "\n",
    "    ## 3. Heart Disease Risk (Composite Risk Score)\n",
    "    # Instead of a fixed binary proxy, we combine gender, stress, and BMI.\n",
    "    # For instance, we weight Gender at 0.4, stress at 0.3, and BMI at 0.3.\n",
    "    df['heart_disease_risk'] = (0.4 * df['Gender'] +\n",
    "                                0.3 * df['stress_score'] +\n",
    "                                0.3 * df['avg_bmi']).astype('float32')\n",
    "\n",
    "    ## 4. Smoking Prevalence\n",
    "    # Use a logistic function to create a smoother curve\n",
    "    # Logistic: 1 / (1 + exp((birth_year - 1950) / 10))\n",
    "    birth_years = df['Birth year'].to_numpy()\n",
    "    df['smoking_prev'] = (1 / (1 + np.exp((birth_years - 1950) / 10))).astype('float32')\n",
    "    # Optionally, scale it to a desired range (e.g., [0.1, 0.6])\n",
    "    df['smoking_prev'] = np.clip(df['smoking_prev'], 0.1, 0.6)\n",
    "\n",
    "    ## 5. Country-Level Features\n",
    "    # Merge with life expectancy data: use median values for consistency\n",
    "    life_exp_filtered = (life_exp_df[['Country', 'Alcohol', 'GDP', 'Schooling']]\n",
    "                         .sort_values('Country')\n",
    "                         .groupby('Country').median()\n",
    "                         .add_prefix('country_'))\n",
    "    df['Country'] = df['Country'].astype('category')\n",
    "    df = df.join(life_exp_filtered, on='Country', how='left')\n",
    "    # Fill missing values and then standardize each country-level feature\n",
    "    for col in ['country_Alcohol', 'country_GDP', 'country_Schooling']:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "        df[col + '_scaled'] = (df[col] - df[col].mean()) / df[col].std()\n",
    "\n",
    "    ## 6. Text Features\n",
    "    # Use TF-IDF on the combination of Occupation and Short description\n",
    "    text_data = df['Occupation'] + \" \" + df['Short description'].fillna('')\n",
    "    # Include unigrams and bigrams and remove English stop words\n",
    "    tfidf = TfidfVectorizer(max_features=100, ngram_range=(1,2), stop_words='english')\n",
    "    text_features = tfidf.fit_transform(text_data)\n",
    "    text_df = pd.DataFrame(\n",
    "        text_features.toarray(),\n",
    "        columns=[f\"tfidf_{i}\" for i in range(text_features.shape[1])],\n",
    "        index=df.index\n",
    "    )\n",
    "    # Optionally, you could reduce dimensionality with Truncated SVD here\n",
    "    df = pd.concat([df, text_df], axis=1)\n",
    "\n",
    "    ## 7. Interaction Features\n",
    "    # Create an interaction term between stress and smoking prevalence\n",
    "    df['stress_x_smoking'] = (df['stress_score'] * df['smoking_prev']).astype('float32')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(df):\n",
    "    \"\"\"\n",
    "    Given a processed DataFrame with synthetic features, split it into training/validation sets,\n",
    "    train an XGBoost survival model (AFT) and a Cox PH model, and return evaluation metrics,\n",
    "    including feature importance rankings.\n",
    "    \"\"\"\n",
    "    # Define the features to use: synthetic features + any TF-IDF features.\n",
    "    feature_list = ['stress_score', 'avg_bmi', 'heart_disease_risk', 'smoking_prev', 'country_Alcohol', 'country_GDP']\n",
    "    tfidf_cols = [col for col in df.columns if col.startswith('tfidf_')]\n",
    "    features = feature_list + tfidf_cols\n",
    "\n",
    "    X = df[features]\n",
    "    y = df['Age of death']\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=BASE_RANDOM_STATE)\n",
    "\n",
    "    # ----- XGBoost AFT Model -----\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=features)\n",
    "    dval   = xgb.DMatrix(X_val, label=y_val, feature_names=features)\n",
    "    # For the AFT objective, set label lower and upper bounds (since data is uncensored, both are y)\n",
    "    dtrain.set_float_info(\"label_lower_bound\", y_train)\n",
    "    dtrain.set_float_info(\"label_upper_bound\", y_train)\n",
    "    dval.set_float_info(\"label_lower_bound\", y_val)\n",
    "    dval.set_float_info(\"label_upper_bound\", y_val)\n",
    "\n",
    "    params_aft = {\n",
    "        'objective': 'survival:aft',\n",
    "        'eval_metric': 'aft-nloglik',\n",
    "        'aft_loss_distribution': 'normal',\n",
    "        'aft_loss_distribution_scale': 0.1,\n",
    "        'tree_method': 'approx',\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 4,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'verbosity': 1\n",
    "    }\n",
    "    print(\"\\nXGBoost AFT Training:\")\n",
    "    try:\n",
    "        model_xgb = xgb.train(\n",
    "            params_aft,\n",
    "            dtrain,\n",
    "            num_boost_round=100,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            verbose_eval=10\n",
    "        )\n",
    "        preds_xgb = model_xgb.predict(dval)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds_xgb))\n",
    "        mae = mean_absolute_error(y_val, preds_xgb)\n",
    "        \n",
    "        # Extract XGBoost feature importance using gain\n",
    "        xgb_importance = model_xgb.get_score(importance_type='gain')\n",
    "        # Convert the dictionary into a DataFrame\n",
    "        xgb_importance_df = pd.DataFrame({\n",
    "            'feature': list(xgb_importance.keys()),\n",
    "            'xgb_gain': list(xgb_importance.values())\n",
    "        }).sort_values('xgb_gain', ascending=False)\n",
    "    except Exception as e:\n",
    "        print(\"XGBoost training failed:\", e)\n",
    "        rmse = None\n",
    "        mae = None\n",
    "        xgb_importance_df = None\n",
    "\n",
    "    # ----- Cox Proportional Hazards Model -----\n",
    "    y_surv_train = Surv.from_arrays(event=np.ones(len(y_train), dtype=bool), time=y_train.to_numpy())\n",
    "    cox = CoxPHSurvivalAnalysis(alpha=0.5)\n",
    "    cox.fit(X_train, y_surv_train)\n",
    "    cindex = concordance_index_censored(\n",
    "        np.ones(len(y_val), dtype=bool),\n",
    "        y_val.to_numpy(),\n",
    "        cox.predict(X_val)\n",
    "    )[0]\n",
    "\n",
    "    # Extract feature coefficients from the Cox model\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'coef': cox.coef_,\n",
    "        'abs_coef': np.abs(cox.coef_)\n",
    "    }).sort_values('abs_coef', ascending=False)\n",
    "\n",
    "    metrics = {\n",
    "        'XGBoost_RMSE': rmse,\n",
    "        'XGBoost_MAE': mae,\n",
    "        'XGBoost_Feature_Importance': xgb_importance_df,\n",
    "        'Cox_Concordance': cindex,\n",
    "        'Cox_Feature_Coefficients': coef_df.head(10)\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_deepsurv(df):\n",
    "    \"\"\"\n",
    "    Given a processed DataFrame with synthetic features, train a DeepSurv (deep Cox PH)\n",
    "    model using pycox and evaluate its performance using the concordance index.\n",
    "    Returns a dictionary with the concordance index and the trained model.\n",
    "    \"\"\"\n",
    "    feature_list = ['stress_score', 'avg_bmi', 'heart_disease_risk', 'smoking_prev', 'country_Alcohol', 'country_GDP']\n",
    "    tfidf_cols = [col for col in df.columns if col.startswith('tfidf_')]\n",
    "    features = feature_list + tfidf_cols\n",
    "\n",
    "    X = df[features].values.astype('float32')\n",
    "    durations = df['Age of death'].values.astype('float32')\n",
    "    events = np.ones(len(durations), dtype=bool)  # assume all events observed\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, durations_train, durations_val, events_train, events_val = train_test_split(\n",
    "        X, durations, events, test_size=0.2, random_state=BASE_RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    in_features = X_train.shape[1]\n",
    "    num_nodes = [64, 64]\n",
    "    out_features = 1\n",
    "    batch_norm = True\n",
    "    dropout = 0.1\n",
    "    net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout)\n",
    "    \n",
    "    model = CoxPH(net, tt.optim.Adam)\n",
    "    y_train_tuple = (durations_train, events_train)\n",
    "    y_val_tuple = (durations_val, events_val)\n",
    "    \n",
    "    model.fit(X_train, y_train_tuple, batch_size=256, epochs=100, verbose=True, val_data=(X_val, y_val_tuple))\n",
    "    \n",
    "    # IMPORTANT: Compute baseline hazards before prediction.\n",
    "    model.compute_baseline_hazards()\n",
    "    \n",
    "    # Get the survival function DataFrame\n",
    "    surv = model.predict_surv_df(X_val)\n",
    "    # Force the index to be numeric.\n",
    "    surv.index = pd.to_numeric(surv.index, errors='coerce')\n",
    "    # If time 0 is not present, insert a row for time 0 with survival probability 1 for every sample.\n",
    "    if 0 not in surv.index:\n",
    "        new_row = pd.DataFrame(np.ones((1, surv.shape[1])), index=[0], columns=surv.columns)\n",
    "        surv = pd.concat([new_row, surv])\n",
    "        surv = surv.sort_index()\n",
    "    \n",
    "    from pycox.evaluation import EvalSurv\n",
    "    ev = EvalSurv(surv, durations_val, events_val, censor_surv='km')\n",
    "    c_index = ev.concordance_td('antolini')\n",
    "    \n",
    "    return {\"DeepSurv_Concordance\": c_index, \"DeepSurv_Model\": model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_tabnet(df):\n",
    "    \"\"\"\n",
    "    Given a processed DataFrame with synthetic features, split it into training and validation sets,\n",
    "    then train a TabNetRegressor and return its RMSE.\n",
    "    \"\"\"\n",
    "    # Reset the DataFrame index to ensure a RangeIndex starting at 0\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Define features: synthetic features + any TF-IDF features.\n",
    "    feature_list = ['stress_score', 'avg_bmi', 'heart_disease_risk', \n",
    "                    'smoking_prev', 'country_Alcohol', 'country_GDP']\n",
    "    tfidf_cols = [col for col in df.columns if col.startswith('tfidf_')]\n",
    "    features = feature_list + tfidf_cols\n",
    "\n",
    "    # Extract X and y. Here, we force the feature DataFrame's columns to be numeric indices.\n",
    "    X = df[features].copy()\n",
    "    X.columns = list(range(X.shape[1]))  # reset column names to 0, 1, 2, ..., ensuring no KeyError later\n",
    "    y = df['Age of death']\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=BASE_RANDOM_STATE)\n",
    "    \n",
    "    # Explicitly convert the DataFrames to numpy arrays with float32 dtype\n",
    "    X_train_np = X_train.to_numpy().astype(np.float32)\n",
    "    X_test_np = X_test.to_numpy().astype(np.float32)\n",
    "    y_train_np = y_train.to_numpy().astype(np.float32).reshape(-1, 1)\n",
    "    y_test_np = y_test.to_numpy().astype(np.float32).reshape(-1, 1)\n",
    "    \n",
    "    # TabNet parameters (assuming all features are numeric)\n",
    "    tabnet_params = {\n",
    "        \"cat_idxs\": [],\n",
    "        \"cat_dims\": [],\n",
    "        \"cat_emb_dim\": 1,\n",
    "        \"n_d\": 64,\n",
    "        \"n_a\": 64,\n",
    "        \"n_steps\": 5,\n",
    "        \"gamma\": 1.3,\n",
    "        \"optimizer_fn\": torch.optim.Adam,\n",
    "        \"optimizer_params\": dict(lr=1e-3, weight_decay=1e-5),\n",
    "        \"scheduler_params\": {\"step_size\": 50, \"gamma\": 0.9},\n",
    "        \"scheduler_fn\": torch.optim.lr_scheduler.StepLR,\n",
    "        \"mask_type\": \"entmax\",\n",
    "        \"verbose\": 1,\n",
    "    }\n",
    "    \n",
    "    from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "    tabnet_model = TabNetRegressor(**tabnet_params)\n",
    "    tabnet_model.fit(\n",
    "        X_train=X_train_np,\n",
    "        y_train=y_train_np,\n",
    "        eval_set=[(X_train_np, y_train_np), (X_test_np, y_test_np)],\n",
    "        eval_name=[\"train\", \"valid\"],\n",
    "        eval_metric=[\"rmse\"],\n",
    "        max_epochs=100,\n",
    "        patience=10,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    # Predict and compute RMSE\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    y_pred = tabnet_model.predict(X_test_np).flatten()\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_np, y_pred))\n",
    "    \n",
    "    return {\"TabNet_RMSE\": rmse, \"TabNet_Model\": tabnet_model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model EBM/GAM\n",
    "def train_and_evaluate_ebm(df):\n",
    "    feature_list = ['stress_score', 'avg_bmi', 'heart_disease_risk', 'smoking_prev', 'country_Alcohol', 'country_GDP']\n",
    "    tfidf_cols = [col for col in df.columns if col.startswith('tfidf_')]\n",
    "    features = feature_list + tfidf_cols\n",
    "\n",
    "    X = df[features]\n",
    "    y = df['Age of death']\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=BASE_RANDOM_STATE)\n",
    "    \n",
    "    ebm = ExplainableBoostingRegressor(random_state=BASE_RANDOM_STATE)\n",
    "    ebm.fit(X_train, y_train)\n",
    "    y_pred = ebm.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    global_explanation = ebm.explain_global()\n",
    "    \n",
    "    return {\"EBM_RMSE\": rmse, \"EBM_MAE\": mae, \"EBM_Global_Importance\": global_explanation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model LightGBM + SHAP\n",
    "def train_and_evaluate_lgbm_shap(df):\n",
    "    feature_list = ['stress_score', 'avg_bmi', 'heart_disease_risk', 'smoking_prev', 'country_Alcohol', 'country_GDP']\n",
    "    tfidf_cols = [col for col in df.columns if col.startswith('tfidf_')]\n",
    "    features = feature_list + tfidf_cols\n",
    "\n",
    "    X = df[features]\n",
    "    y = df['Age of death']\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=BASE_RANDOM_STATE)\n",
    "    \n",
    "    lgb_model = lgb.LGBMRegressor(random_state=BASE_RANDOM_STATE)\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    y_pred = lgb_model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    \n",
    "    explainer = shap.TreeExplainer(lgb_model)\n",
    "    shap_values = explainer.shap_values(X_val)\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "    shap_importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'shap_importance': shap_importance\n",
    "    }).sort_values('shap_importance', ascending=False)\n",
    "    \n",
    "    return {\"LGBM_RMSE\": rmse, \"LGBM_MAE\": mae, \"LGBM_SHAP_Importance\": shap_importance_df}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNODE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims, num_layers):\n",
    "        super(SimpleNODE, self).__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        # Create a stack of layers\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(current_dim, hidden_dims[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            current_dim = hidden_dims[i]\n",
    "        # Final output layer\n",
    "        layers.append(nn.Linear(current_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_node(df):\n",
    "    \"\"\"\n",
    "    Given a processed DataFrame with synthetic features, split the data,\n",
    "    train the SimpleNODE model, and evaluate its performance.\n",
    "    Returns a dictionary with NODE RMSE, MAE, and the trained model.\n",
    "    \"\"\"\n",
    "    # Define features: synthetic features + any TF-IDF features.\n",
    "    feature_list = ['stress_score', 'avg_bmi', 'heart_disease_risk', \n",
    "                    'smoking_prev', 'country_Alcohol', 'country_GDP']\n",
    "    tfidf_cols = [col for col in df.columns if col.startswith('tfidf_')]\n",
    "    features = feature_list + tfidf_cols\n",
    "\n",
    "    X = df[features].values.astype('float32')\n",
    "    y = df['Age of death'].values.astype('float32')\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=BASE_RANDOM_STATE)\n",
    "    \n",
    "    # Initialize our SimpleNODE model\n",
    "    # For example, use hidden dimensions of 64 for each layer, and 3 layers.\n",
    "    model = SimpleNODE(input_dim=X_train.shape[1], output_dim=1, hidden_dims=[64, 64, 64], num_layers=3)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    epochs = 100\n",
    "    # Convert training data to torch tensors\n",
    "    X_train_tensor = torch.from_numpy(X_train)\n",
    "    y_train_tensor = torch.from_numpy(y_train).unsqueeze(1)  # shape (N, 1)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_train_tensor)\n",
    "        loss = loss_fn(preds, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    X_test_tensor = torch.from_numpy(X_test)\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_test_tensor).squeeze().numpy()\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    return {\"NODE_RMSE\": rmse, \"NODE_MAE\": mae, \"NODE_Model\": model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_train_batch(random_seed):\n",
    "    \"\"\"\n",
    "    Sample a batch from the full age_df, apply preprocessing/feature engineering,\n",
    "    then train and evaluate the models.\n",
    "    \"\"\"\n",
    "    batch_df = age_df.sample(n=BATCH_SIZE, random_state=random_seed).copy()\n",
    "    print(f\"\\nProcessing batch with random seed {random_seed} (shape: {batch_df.shape})\")\n",
    "    \n",
    "    processed_batch = preprocess_and_engineer_features(batch_df, life_exp_df)\n",
    "    metrics_surv = train_and_evaluate_models(processed_batch)\n",
    "    metrics_deepsurv = train_and_evaluate_deepsurv(processed_batch)\n",
    "    metrics_tabnet = train_and_evaluate_tabnet(processed_batch)\n",
    "\n",
    "    combined_metrics = {**metrics_surv, **metrics_deepsurv, **metrics_tabnet} \n",
    "    # Free memory\n",
    "    del batch_df, processed_batch\n",
    "    gc.collect()\n",
    "    \n",
    "    return combined_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_batch(random_seed):\n",
    "    \"\"\"\n",
    "    Samples a mini-batch from the full age_df and applies preprocessing and feature engineering.\n",
    "    \"\"\"\n",
    "    batch_df = age_df.sample(n=BATCH_SIZE, random_state=random_seed).copy()\n",
    "    processed_batch = preprocess_and_engineer_features(batch_df, life_exp_df)\n",
    "    return processed_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost AFT Training:\n",
      "[0]\ttrain-aft-nloglik:27.62989\tval-aft-nloglik:27.62989\n",
      "[10]\ttrain-aft-nloglik:27.62862\tval-aft-nloglik:27.62862\n",
      "[20]\ttrain-aft-nloglik:27.62755\tval-aft-nloglik:27.62755\n",
      "[30]\ttrain-aft-nloglik:27.62669\tval-aft-nloglik:27.62669\n",
      "[40]\ttrain-aft-nloglik:27.62602\tval-aft-nloglik:27.62602\n",
      "[50]\ttrain-aft-nloglik:27.62555\tval-aft-nloglik:27.62555\n",
      "[60]\ttrain-aft-nloglik:27.62529\tval-aft-nloglik:27.62529\n",
      "[70]\ttrain-aft-nloglik:27.62481\tval-aft-nloglik:27.62468\n",
      "[80]\ttrain-aft-nloglik:27.62447\tval-aft-nloglik:27.62418\n",
      "[90]\ttrain-aft-nloglik:27.62437\tval-aft-nloglik:27.62399\n",
      "[99]\ttrain-aft-nloglik:27.62451\tval-aft-nloglik:27.62407\n",
      "XGBoost/Cox Results for batch 1:\n",
      "  XGBoost RMSE: 69.7956799196606\n",
      "  XGBoost MAE: 67.78840663439036\n",
      "  XGBoost Feature Importance:\n",
      "              feature   xgb_gain\n",
      "0             avg_bmi  25.901558\n",
      "2        smoking_prev  12.995339\n",
      "1  heart_disease_risk  12.753188\n",
      "3     country_Alcohol   3.081598\n",
      "4            tfidf_97   2.358752\n",
      "  Cox Concordance Index: 0.5822343676263121\n",
      "  Top Cox Features:\n",
      "         feature      coef  abs_coef\n",
      "3   smoking_prev -6.585696  6.585696\n",
      "91      tfidf_85  0.818487  0.818487\n",
      "59      tfidf_53 -0.576207  0.576207\n",
      "73      tfidf_67  0.526036  0.526036\n",
      "92      tfidf_86 -0.484355  0.484355\n",
      "68      tfidf_62 -0.445978  0.445978\n",
      "26      tfidf_20 -0.417978  0.417978\n",
      "83      tfidf_77  0.387915  0.387915\n",
      "27      tfidf_21  0.311519  0.311519\n",
      "49      tfidf_43  0.260375  0.260375\n",
      "--------------------------------------------------\n",
      "\n",
      "XGBoost AFT Training:\n",
      "[0]\ttrain-aft-nloglik:27.63045\tval-aft-nloglik:27.63102\n",
      "[10]\ttrain-aft-nloglik:27.62982\tval-aft-nloglik:27.63102\n",
      "[20]\ttrain-aft-nloglik:27.62929\tval-aft-nloglik:27.63102\n",
      "[30]\ttrain-aft-nloglik:27.62885\tval-aft-nloglik:27.63102\n",
      "[40]\ttrain-aft-nloglik:27.62852\tval-aft-nloglik:27.63102\n",
      "[50]\ttrain-aft-nloglik:27.62829\tval-aft-nloglik:27.63102\n",
      "[60]\ttrain-aft-nloglik:27.62815\tval-aft-nloglik:27.63102\n",
      "[70]\ttrain-aft-nloglik:27.62771\tval-aft-nloglik:27.63048\n",
      "[80]\ttrain-aft-nloglik:27.62731\tval-aft-nloglik:27.62986\n",
      "[90]\ttrain-aft-nloglik:27.62708\tval-aft-nloglik:27.62933\n",
      "[99]\ttrain-aft-nloglik:27.62702\tval-aft-nloglik:27.62894\n",
      "XGBoost/Cox Results for batch 2:\n",
      "  XGBoost RMSE: 69.81567498042608\n",
      "  XGBoost MAE: 67.8286609427929\n",
      "  XGBoost Feature Importance:\n",
      "    feature  xgb_gain\n",
      "0  tfidf_97  0.642708\n",
      "  Cox Concordance Index: 0.57842231920339\n",
      "  Top Cox Features:\n",
      "         feature      coef  abs_coef\n",
      "3   smoking_prev -6.588269  6.588269\n",
      "19      tfidf_13 -0.785588  0.785588\n",
      "92      tfidf_86  0.568325  0.568325\n",
      "18      tfidf_12  0.507888  0.507888\n",
      "51      tfidf_45  0.496834  0.496834\n",
      "36      tfidf_30  0.429135  0.429135\n",
      "27      tfidf_21  0.376894  0.376894\n",
      "73      tfidf_67  0.323185  0.323185\n",
      "26      tfidf_20 -0.302621  0.302621\n",
      "62      tfidf_56 -0.295842  0.295842\n",
      "--------------------------------------------------\n",
      "\n",
      "XGBoost AFT Training:\n",
      "[0]\ttrain-aft-nloglik:27.63088\tval-aft-nloglik:27.63045\n",
      "[10]\ttrain-aft-nloglik:27.63072\tval-aft-nloglik:27.62982\n",
      "[20]\ttrain-aft-nloglik:27.63059\tval-aft-nloglik:27.62929\n",
      "[30]\ttrain-aft-nloglik:27.63048\tval-aft-nloglik:27.62885\n",
      "[40]\ttrain-aft-nloglik:27.63040\tval-aft-nloglik:27.62852\n",
      "[50]\ttrain-aft-nloglik:27.63034\tval-aft-nloglik:27.62829\n",
      "[60]\ttrain-aft-nloglik:27.63030\tval-aft-nloglik:27.62815\n",
      "[70]\ttrain-aft-nloglik:27.62962\tval-aft-nloglik:27.62812\n",
      "[80]\ttrain-aft-nloglik:27.62885\tval-aft-nloglik:27.62819\n",
      "[90]\ttrain-aft-nloglik:27.62823\tval-aft-nloglik:27.62835\n",
      "[99]\ttrain-aft-nloglik:27.62781\tval-aft-nloglik:27.62859\n",
      "XGBoost/Cox Results for batch 3:\n",
      "  XGBoost RMSE: 70.02715676006021\n",
      "  XGBoost MAE: 68.02302773590088\n",
      "  XGBoost Feature Importance:\n",
      "Empty DataFrame\n",
      "Columns: [feature, xgb_gain]\n",
      "Index: []\n",
      "  Cox Concordance Index: 0.5845936384987165\n",
      "  Top Cox Features:\n",
      "          feature      coef  abs_coef\n",
      "3    smoking_prev -6.639855  6.639855\n",
      "88       tfidf_82  0.957210  0.957210\n",
      "84       tfidf_78 -0.774079  0.774079\n",
      "95       tfidf_89  0.699137  0.699137\n",
      "89       tfidf_83 -0.481695  0.481695\n",
      "56       tfidf_50 -0.442290  0.442290\n",
      "46       tfidf_40  0.438571  0.438571\n",
      "28       tfidf_22  0.421498  0.421498\n",
      "83       tfidf_77  0.415925  0.415925\n",
      "101      tfidf_95 -0.399579  0.399579\n",
      "--------------------------------------------------\n",
      "\n",
      "XGBoost AFT Training:\n",
      "[0]\ttrain-aft-nloglik:27.63017\tval-aft-nloglik:27.63045\n",
      "[10]\ttrain-aft-nloglik:27.62922\tval-aft-nloglik:27.62982\n",
      "[20]\ttrain-aft-nloglik:27.62842\tval-aft-nloglik:27.62929\n",
      "[30]\ttrain-aft-nloglik:27.62777\tval-aft-nloglik:27.62885\n",
      "[40]\ttrain-aft-nloglik:27.62727\tval-aft-nloglik:27.62852\n",
      "[50]\ttrain-aft-nloglik:27.62692\tval-aft-nloglik:27.62829\n",
      "[60]\ttrain-aft-nloglik:27.62672\tval-aft-nloglik:27.62815\n",
      "[70]\ttrain-aft-nloglik:27.62654\tval-aft-nloglik:27.62703\n",
      "[80]\ttrain-aft-nloglik:27.62648\tval-aft-nloglik:27.62585\n",
      "[90]\ttrain-aft-nloglik:27.62660\tval-aft-nloglik:27.62496\n",
      "[99]\ttrain-aft-nloglik:27.62685\tval-aft-nloglik:27.62442\n",
      "XGBoost/Cox Results for batch 4:\n",
      "  XGBoost RMSE: 69.62923158766428\n",
      "  XGBoost MAE: 67.56264846544266\n",
      "  XGBoost Feature Importance:\n",
      "              feature  xgb_gain\n",
      "7            tfidf_98  8.325629\n",
      "6            tfidf_97  5.310947\n",
      "1  heart_disease_risk  2.470822\n",
      "0             avg_bmi  2.107512\n",
      "3         country_GDP  1.278481\n",
      "2        smoking_prev  0.914202\n",
      "5            tfidf_14  0.511597\n",
      "4             tfidf_6  0.196228\n",
      "  Cox Concordance Index: 0.5837336134173335\n",
      "  Top Cox Features:\n",
      "         feature      coef  abs_coef\n",
      "3   smoking_prev -6.534631  6.534631\n",
      "36      tfidf_30  0.501413  0.501413\n",
      "90      tfidf_84  0.494760  0.494760\n",
      "91      tfidf_85 -0.475400  0.475400\n",
      "28      tfidf_22  0.425338  0.425338\n",
      "19      tfidf_13 -0.423959  0.423959\n",
      "71      tfidf_65 -0.389145  0.389145\n",
      "50      tfidf_44  0.388836  0.388836\n",
      "61      tfidf_55 -0.304196  0.304196\n",
      "23      tfidf_17 -0.300619  0.300619\n",
      "--------------------------------------------------\n",
      "\n",
      "XGBoost AFT Training:\n",
      "[0]\ttrain-aft-nloglik:27.63060\tval-aft-nloglik:27.62875\n",
      "[10]\ttrain-aft-nloglik:27.63012\tval-aft-nloglik:27.62622\n",
      "[20]\ttrain-aft-nloglik:27.62972\tval-aft-nloglik:27.62409\n",
      "[30]\ttrain-aft-nloglik:27.62940\tval-aft-nloglik:27.62236\n",
      "[40]\ttrain-aft-nloglik:27.62915\tval-aft-nloglik:27.62103\n",
      "[50]\ttrain-aft-nloglik:27.62897\tval-aft-nloglik:27.62009\n",
      "[60]\ttrain-aft-nloglik:27.62887\tval-aft-nloglik:27.61956\n",
      "[70]\ttrain-aft-nloglik:27.62871\tval-aft-nloglik:27.61888\n",
      "[80]\ttrain-aft-nloglik:27.62860\tval-aft-nloglik:27.61852\n",
      "[90]\ttrain-aft-nloglik:27.62860\tval-aft-nloglik:27.61865\n",
      "[99]\ttrain-aft-nloglik:27.62867\tval-aft-nloglik:27.61920\n",
      "XGBoost/Cox Results for batch 5:\n",
      "  XGBoost RMSE: 69.91629084487425\n",
      "  XGBoost MAE: 67.92845592536926\n",
      "  XGBoost Feature Importance:\n",
      "              feature   xgb_gain\n",
      "0             avg_bmi  13.065449\n",
      "1  heart_disease_risk  11.427749\n",
      "4         country_GDP   4.594955\n",
      "3     country_Alcohol   2.171860\n",
      "2        smoking_prev   2.011719\n",
      "5            tfidf_97   1.716643\n",
      "  Cox Concordance Index: 0.5874482315249626\n",
      "  Top Cox Features:\n",
      "         feature      coef  abs_coef\n",
      "3   smoking_prev -6.529404  6.529404\n",
      "91      tfidf_85  0.640562  0.640562\n",
      "86      tfidf_80 -0.586261  0.586261\n",
      "28      tfidf_22  0.472899  0.472899\n",
      "37      tfidf_31 -0.411194  0.411194\n",
      "26      tfidf_20 -0.373843  0.373843\n",
      "47      tfidf_41  0.363290  0.363290\n",
      "33      tfidf_27 -0.353941  0.353941\n",
      "80      tfidf_74 -0.312040  0.312040\n",
      "85      tfidf_79  0.307959  0.307959\n",
      "--------------------------------------------------\n",
      "\n",
      "Final aggregated XGBoost/Cox results:\n",
      "[{'XGBoost_RMSE': np.float64(69.7956799196606), 'XGBoost_MAE': np.float64(67.78840663439036), 'XGBoost_Feature_Importance':               feature   xgb_gain\n",
      "0             avg_bmi  25.901558\n",
      "2        smoking_prev  12.995339\n",
      "1  heart_disease_risk  12.753188\n",
      "3     country_Alcohol   3.081598\n",
      "4            tfidf_97   2.358752, 'Cox_Concordance': np.float64(0.5822343676263121), 'Cox_Feature_Coefficients':          feature      coef  abs_coef\n",
      "3   smoking_prev -6.585696  6.585696\n",
      "91      tfidf_85  0.818487  0.818487\n",
      "59      tfidf_53 -0.576207  0.576207\n",
      "73      tfidf_67  0.526036  0.526036\n",
      "92      tfidf_86 -0.484355  0.484355\n",
      "68      tfidf_62 -0.445978  0.445978\n",
      "26      tfidf_20 -0.417978  0.417978\n",
      "83      tfidf_77  0.387915  0.387915\n",
      "27      tfidf_21  0.311519  0.311519\n",
      "49      tfidf_43  0.260375  0.260375}, {'XGBoost_RMSE': np.float64(69.81567498042608), 'XGBoost_MAE': np.float64(67.8286609427929), 'XGBoost_Feature_Importance':     feature  xgb_gain\n",
      "0  tfidf_97  0.642708, 'Cox_Concordance': np.float64(0.57842231920339), 'Cox_Feature_Coefficients':          feature      coef  abs_coef\n",
      "3   smoking_prev -6.588269  6.588269\n",
      "19      tfidf_13 -0.785588  0.785588\n",
      "92      tfidf_86  0.568325  0.568325\n",
      "18      tfidf_12  0.507888  0.507888\n",
      "51      tfidf_45  0.496834  0.496834\n",
      "36      tfidf_30  0.429135  0.429135\n",
      "27      tfidf_21  0.376894  0.376894\n",
      "73      tfidf_67  0.323185  0.323185\n",
      "26      tfidf_20 -0.302621  0.302621\n",
      "62      tfidf_56 -0.295842  0.295842}, {'XGBoost_RMSE': np.float64(70.02715676006021), 'XGBoost_MAE': np.float64(68.02302773590088), 'XGBoost_Feature_Importance': Empty DataFrame\n",
      "Columns: [feature, xgb_gain]\n",
      "Index: [], 'Cox_Concordance': np.float64(0.5845936384987165), 'Cox_Feature_Coefficients':           feature      coef  abs_coef\n",
      "3    smoking_prev -6.639855  6.639855\n",
      "88       tfidf_82  0.957210  0.957210\n",
      "84       tfidf_78 -0.774079  0.774079\n",
      "95       tfidf_89  0.699137  0.699137\n",
      "89       tfidf_83 -0.481695  0.481695\n",
      "56       tfidf_50 -0.442290  0.442290\n",
      "46       tfidf_40  0.438571  0.438571\n",
      "28       tfidf_22  0.421498  0.421498\n",
      "83       tfidf_77  0.415925  0.415925\n",
      "101      tfidf_95 -0.399579  0.399579}, {'XGBoost_RMSE': np.float64(69.62923158766428), 'XGBoost_MAE': np.float64(67.56264846544266), 'XGBoost_Feature_Importance':               feature  xgb_gain\n",
      "7            tfidf_98  8.325629\n",
      "6            tfidf_97  5.310947\n",
      "1  heart_disease_risk  2.470822\n",
      "0             avg_bmi  2.107512\n",
      "3         country_GDP  1.278481\n",
      "2        smoking_prev  0.914202\n",
      "5            tfidf_14  0.511597\n",
      "4             tfidf_6  0.196228, 'Cox_Concordance': np.float64(0.5837336134173335), 'Cox_Feature_Coefficients':          feature      coef  abs_coef\n",
      "3   smoking_prev -6.534631  6.534631\n",
      "36      tfidf_30  0.501413  0.501413\n",
      "90      tfidf_84  0.494760  0.494760\n",
      "91      tfidf_85 -0.475400  0.475400\n",
      "28      tfidf_22  0.425338  0.425338\n",
      "19      tfidf_13 -0.423959  0.423959\n",
      "71      tfidf_65 -0.389145  0.389145\n",
      "50      tfidf_44  0.388836  0.388836\n",
      "61      tfidf_55 -0.304196  0.304196\n",
      "23      tfidf_17 -0.300619  0.300619}, {'XGBoost_RMSE': np.float64(69.91629084487425), 'XGBoost_MAE': np.float64(67.92845592536926), 'XGBoost_Feature_Importance':               feature   xgb_gain\n",
      "0             avg_bmi  13.065449\n",
      "1  heart_disease_risk  11.427749\n",
      "4         country_GDP   4.594955\n",
      "3     country_Alcohol   2.171860\n",
      "2        smoking_prev   2.011719\n",
      "5            tfidf_97   1.716643, 'Cox_Concordance': np.float64(0.5874482315249626), 'Cox_Feature_Coefficients':          feature      coef  abs_coef\n",
      "3   smoking_prev -6.529404  6.529404\n",
      "91      tfidf_85  0.640562  0.640562\n",
      "86      tfidf_80 -0.586261  0.586261\n",
      "28      tfidf_22  0.472899  0.472899\n",
      "37      tfidf_31 -0.411194  0.411194\n",
      "26      tfidf_20 -0.373843  0.373843\n",
      "47      tfidf_41  0.363290  0.363290\n",
      "33      tfidf_27 -0.353941  0.353941\n",
      "80      tfidf_74 -0.312040  0.312040\n",
      "85      tfidf_79  0.307959  0.307959}]\n"
     ]
    }
   ],
   "source": [
    "xgb_cox_results = []\n",
    "for i in range(N_ITERATIONS):\n",
    "    seed = BASE_RANDOM_STATE + i\n",
    "    processed_batch = get_processed_batch(seed)\n",
    "    metrics_surv = train_and_evaluate_models(processed_batch)\n",
    "    xgb_cox_results.append(metrics_surv)\n",
    "    print(f\"XGBoost/Cox Results for batch {i+1}:\")\n",
    "    print(f\"  XGBoost RMSE: {metrics_surv['XGBoost_RMSE']}\")\n",
    "    print(f\"  XGBoost MAE: {metrics_surv['XGBoost_MAE']}\")\n",
    "    print(\"  XGBoost Feature Importance:\")\n",
    "    print(metrics_surv['XGBoost_Feature_Importance'])\n",
    "    print(f\"  Cox Concordance Index: {metrics_surv['Cox_Concordance']}\")\n",
    "    print(\"  Top Cox Features:\")\n",
    "    print(metrics_surv['Cox_Feature_Coefficients'])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nFinal aggregated XGBoost/Cox results:\")\n",
    "print(xgb_cox_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 4.5779,\tval_loss: 7.2515\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 4.5566,\tval_loss: 7.2477\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 4.5528,\tval_loss: 7.2469\n",
      "3:\t[0s / 1s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2475\n",
      "4:\t[0s / 2s],\t\ttrain_loss: 4.5496,\tval_loss: 7.2467\n",
      "5:\t[0s / 2s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2470\n",
      "6:\t[0s / 2s],\t\ttrain_loss: 4.5495,\tval_loss: 7.2471\n",
      "7:\t[0s / 3s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2471\n",
      "8:\t[0s / 3s],\t\ttrain_loss: 4.5495,\tval_loss: 7.2487\n",
      "9:\t[0s / 4s],\t\ttrain_loss: 4.5493,\tval_loss: 7.2481\n",
      "10:\t[0s / 4s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2485\n",
      "11:\t[0s / 5s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2468\n",
      "12:\t[0s / 5s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2471\n",
      "13:\t[2s / 8s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2465\n",
      "14:\t[0s / 8s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2464\n",
      "15:\t[0s / 8s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2471\n",
      "16:\t[0s / 9s],\t\ttrain_loss: 4.5482,\tval_loss: 7.2460\n",
      "17:\t[0s / 9s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2460\n",
      "18:\t[0s / 10s],\t\ttrain_loss: 4.5480,\tval_loss: 7.2459\n",
      "19:\t[0s / 10s],\t\ttrain_loss: 4.5483,\tval_loss: 7.2468\n",
      "20:\t[0s / 11s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2460\n",
      "21:\t[0s / 11s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2486\n",
      "22:\t[0s / 11s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2462\n",
      "23:\t[0s / 12s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2462\n",
      "24:\t[0s / 12s],\t\ttrain_loss: 4.5483,\tval_loss: 7.2460\n",
      "25:\t[0s / 13s],\t\ttrain_loss: 4.5483,\tval_loss: 7.2481\n",
      "26:\t[0s / 13s],\t\ttrain_loss: 4.5508,\tval_loss: 7.2501\n",
      "27:\t[0s / 14s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "28:\t[0s / 14s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "29:\t[0s / 14s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "30:\t[0s / 15s],\t\ttrain_loss: 4.5515,\tval_loss: 7.2501\n",
      "31:\t[0s / 15s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "32:\t[0s / 16s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "33:\t[0s / 16s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "34:\t[0s / 17s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "35:\t[0s / 17s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "36:\t[0s / 17s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "37:\t[0s / 18s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "38:\t[0s / 18s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "39:\t[0s / 19s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "40:\t[0s / 19s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "41:\t[0s / 19s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "42:\t[0s / 20s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "43:\t[0s / 20s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "44:\t[0s / 21s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "45:\t[0s / 21s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "46:\t[0s / 21s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "47:\t[0s / 22s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "48:\t[0s / 22s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "49:\t[0s / 23s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "50:\t[0s / 23s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "51:\t[0s / 24s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "52:\t[0s / 24s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "53:\t[0s / 25s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "54:\t[0s / 25s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "55:\t[0s / 25s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "56:\t[0s / 26s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "57:\t[0s / 26s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "58:\t[0s / 27s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "59:\t[0s / 27s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "60:\t[0s / 28s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "61:\t[0s / 28s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "62:\t[0s / 28s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "63:\t[0s / 29s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "64:\t[0s / 29s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "65:\t[0s / 30s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "66:\t[0s / 30s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "67:\t[0s / 30s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "68:\t[0s / 31s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "69:\t[0s / 31s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "70:\t[0s / 32s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "71:\t[0s / 32s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "72:\t[0s / 32s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "73:\t[0s / 33s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "74:\t[0s / 33s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "75:\t[0s / 34s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "76:\t[0s / 34s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "77:\t[0s / 35s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "78:\t[0s / 35s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "79:\t[0s / 36s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "80:\t[0s / 36s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "81:\t[0s / 36s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "82:\t[0s / 37s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "83:\t[0s / 37s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "84:\t[0s / 38s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "85:\t[0s / 39s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "86:\t[0s / 39s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "87:\t[0s / 39s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "88:\t[0s / 40s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "89:\t[0s / 40s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "90:\t[0s / 41s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "91:\t[0s / 41s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "92:\t[0s / 41s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "93:\t[0s / 42s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "94:\t[0s / 42s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "95:\t[0s / 43s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "96:\t[0s / 43s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "97:\t[0s / 44s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "98:\t[0s / 44s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "99:\t[0s / 44s],\t\ttrain_loss: 4.5509,\tval_loss: 7.2501\n",
      "DeepSurv Results for batch 1:\n",
      "  DeepSurv Concordance Index: 0.0\n",
      "--------------------------------------------------\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 4.6049,\tval_loss: 7.2486\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 4.5662,\tval_loss: 7.2441\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 4.5591,\tval_loss: 7.2488\n",
      "3:\t[0s / 1s],\t\ttrain_loss: 4.5565,\tval_loss: 7.2451\n",
      "4:\t[0s / 2s],\t\ttrain_loss: 4.5538,\tval_loss: 7.2439\n",
      "5:\t[0s / 2s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2452\n",
      "6:\t[0s / 3s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2459\n",
      "7:\t[0s / 3s],\t\ttrain_loss: 4.5507,\tval_loss: 7.2496\n",
      "8:\t[0s / 4s],\t\ttrain_loss: 4.5499,\tval_loss: 7.2444\n",
      "9:\t[0s / 4s],\t\ttrain_loss: 4.5493,\tval_loss: 7.2457\n",
      "10:\t[1s / 5s],\t\ttrain_loss: 4.5495,\tval_loss: 7.2446\n",
      "11:\t[0s / 6s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2448\n",
      "12:\t[0s / 6s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2451\n",
      "13:\t[0s / 7s],\t\ttrain_loss: 4.5495,\tval_loss: 7.2459\n",
      "14:\t[0s / 7s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2439\n",
      "15:\t[0s / 8s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2439\n",
      "16:\t[0s / 8s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2484\n",
      "17:\t[0s / 9s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2460\n",
      "18:\t[0s / 9s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2456\n",
      "19:\t[0s / 10s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2449\n",
      "20:\t[0s / 10s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2436\n",
      "21:\t[0s / 10s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2448\n",
      "22:\t[0s / 11s],\t\ttrain_loss: 4.5493,\tval_loss: 7.2456\n",
      "23:\t[0s / 11s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2439\n",
      "24:\t[0s / 12s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2438\n",
      "25:\t[0s / 12s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2447\n",
      "26:\t[0s / 12s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2436\n",
      "27:\t[0s / 13s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2453\n",
      "28:\t[0s / 13s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2438\n",
      "29:\t[0s / 14s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2445\n",
      "30:\t[0s / 14s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2451\n",
      "31:\t[0s / 15s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2438\n",
      "32:\t[0s / 15s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2444\n",
      "33:\t[0s / 15s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2443\n",
      "34:\t[0s / 16s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2435\n",
      "35:\t[0s / 16s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2436\n",
      "36:\t[0s / 17s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2440\n",
      "37:\t[0s / 17s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2445\n",
      "38:\t[0s / 17s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2442\n",
      "39:\t[0s / 18s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2441\n",
      "40:\t[0s / 18s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2434\n",
      "41:\t[0s / 19s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2436\n",
      "42:\t[0s / 20s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2437\n",
      "43:\t[0s / 20s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2439\n",
      "44:\t[0s / 20s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2469\n",
      "45:\t[0s / 21s],\t\ttrain_loss: 4.5493,\tval_loss: 7.2442\n",
      "46:\t[0s / 21s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2440\n",
      "47:\t[0s / 22s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2459\n",
      "48:\t[0s / 22s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2446\n",
      "49:\t[0s / 22s],\t\ttrain_loss: 4.5493,\tval_loss: 7.2441\n",
      "50:\t[0s / 23s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2434\n",
      "51:\t[0s / 23s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2449\n",
      "52:\t[0s / 24s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2449\n",
      "53:\t[0s / 24s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2431\n",
      "54:\t[0s / 24s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2434\n",
      "55:\t[0s / 25s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2442\n",
      "56:\t[0s / 25s],\t\ttrain_loss: 4.5481,\tval_loss: 7.2435\n",
      "57:\t[0s / 26s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2453\n",
      "58:\t[0s / 26s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2467\n",
      "59:\t[0s / 26s],\t\ttrain_loss: 4.5483,\tval_loss: 7.2447\n",
      "60:\t[0s / 27s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2450\n",
      "61:\t[0s / 27s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2501\n",
      "62:\t[0s / 28s],\t\ttrain_loss: 4.5517,\tval_loss: 7.2501\n",
      "63:\t[0s / 28s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "64:\t[0s / 29s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "65:\t[0s / 29s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "66:\t[0s / 29s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "67:\t[0s / 30s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "68:\t[0s / 30s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "69:\t[0s / 31s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "70:\t[0s / 31s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "71:\t[0s / 32s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "72:\t[0s / 32s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "73:\t[0s / 33s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "74:\t[0s / 33s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "75:\t[0s / 34s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "76:\t[0s / 34s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "77:\t[0s / 34s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "78:\t[0s / 35s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "79:\t[0s / 35s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "80:\t[0s / 36s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "81:\t[0s / 36s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "82:\t[0s / 36s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "83:\t[0s / 37s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "84:\t[0s / 37s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "85:\t[0s / 38s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "86:\t[0s / 38s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "87:\t[0s / 39s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "88:\t[0s / 39s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "89:\t[0s / 40s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "90:\t[0s / 40s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "91:\t[0s / 41s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "92:\t[0s / 41s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "93:\t[0s / 41s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "94:\t[0s / 42s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "95:\t[0s / 42s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "96:\t[0s / 43s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "97:\t[0s / 43s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "98:\t[0s / 44s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "99:\t[0s / 44s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "DeepSurv Results for batch 2:\n",
      "  DeepSurv Concordance Index: 0.0\n",
      "--------------------------------------------------\n",
      "0:\t[1s / 1s],\t\ttrain_loss: 4.5700,\tval_loss: 7.2467\n",
      "1:\t[1s / 2s],\t\ttrain_loss: 4.5573,\tval_loss: 7.2472\n",
      "2:\t[0s / 3s],\t\ttrain_loss: 4.5539,\tval_loss: 7.2529\n",
      "3:\t[0s / 3s],\t\ttrain_loss: 4.5521,\tval_loss: 7.2495\n",
      "4:\t[0s / 3s],\t\ttrain_loss: 4.5517,\tval_loss: 7.2482\n",
      "5:\t[0s / 4s],\t\ttrain_loss: 4.5506,\tval_loss: 7.2471\n",
      "6:\t[0s / 4s],\t\ttrain_loss: 4.5505,\tval_loss: 7.2469\n",
      "7:\t[0s / 5s],\t\ttrain_loss: 4.5501,\tval_loss: 7.2474\n",
      "8:\t[0s / 5s],\t\ttrain_loss: 4.5499,\tval_loss: 7.2473\n",
      "9:\t[0s / 6s],\t\ttrain_loss: 4.5496,\tval_loss: 7.2472\n",
      "10:\t[0s / 6s],\t\ttrain_loss: 4.5497,\tval_loss: 7.2479\n",
      "11:\t[0s / 7s],\t\ttrain_loss: 4.5498,\tval_loss: 7.2472\n",
      "12:\t[0s / 7s],\t\ttrain_loss: 4.5493,\tval_loss: 7.2473\n",
      "13:\t[0s / 7s],\t\ttrain_loss: 4.5494,\tval_loss: 7.2476\n",
      "14:\t[0s / 8s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2467\n",
      "15:\t[0s / 8s],\t\ttrain_loss: 4.5493,\tval_loss: 7.2469\n",
      "16:\t[0s / 9s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2469\n",
      "17:\t[0s / 9s],\t\ttrain_loss: 4.5494,\tval_loss: 7.2471\n",
      "18:\t[0s / 9s],\t\ttrain_loss: 4.5493,\tval_loss: 7.2481\n",
      "19:\t[0s / 10s],\t\ttrain_loss: 4.5494,\tval_loss: 7.2465\n",
      "20:\t[0s / 10s],\t\ttrain_loss: 4.5493,\tval_loss: 7.2476\n",
      "21:\t[0s / 11s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2463\n",
      "22:\t[0s / 11s],\t\ttrain_loss: 4.5494,\tval_loss: 7.2484\n",
      "23:\t[0s / 11s],\t\ttrain_loss: 4.5495,\tval_loss: 7.2481\n",
      "24:\t[0s / 12s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2465\n",
      "25:\t[0s / 12s],\t\ttrain_loss: 4.5493,\tval_loss: 7.2478\n",
      "26:\t[0s / 13s],\t\ttrain_loss: 4.5495,\tval_loss: 7.2465\n",
      "27:\t[0s / 13s],\t\ttrain_loss: 4.5501,\tval_loss: 7.2473\n",
      "28:\t[0s / 14s],\t\ttrain_loss: 4.5502,\tval_loss: 7.2501\n",
      "29:\t[0s / 14s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "30:\t[0s / 14s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "31:\t[0s / 15s],\t\ttrain_loss: 4.5515,\tval_loss: 7.2501\n",
      "32:\t[0s / 15s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "33:\t[0s / 16s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "34:\t[0s / 16s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "35:\t[0s / 16s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "36:\t[0s / 17s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "37:\t[0s / 17s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "38:\t[0s / 18s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "39:\t[0s / 18s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "40:\t[0s / 19s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "41:\t[0s / 19s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "42:\t[0s / 20s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "43:\t[0s / 20s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "44:\t[0s / 20s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "45:\t[0s / 21s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "46:\t[0s / 21s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "47:\t[0s / 22s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "48:\t[0s / 22s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "49:\t[0s / 22s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "50:\t[0s / 23s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "51:\t[0s / 23s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "52:\t[0s / 24s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "53:\t[0s / 24s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "54:\t[0s / 25s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "55:\t[0s / 25s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "56:\t[0s / 26s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "57:\t[0s / 26s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "58:\t[0s / 26s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "59:\t[0s / 27s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "60:\t[0s / 27s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "61:\t[0s / 28s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "62:\t[0s / 28s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "63:\t[0s / 29s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "64:\t[0s / 29s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "65:\t[0s / 29s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "66:\t[0s / 30s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "67:\t[0s / 30s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "68:\t[0s / 31s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "69:\t[0s / 31s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "70:\t[0s / 31s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "71:\t[0s / 32s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "72:\t[0s / 32s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "73:\t[0s / 33s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "74:\t[0s / 33s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "75:\t[0s / 33s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "76:\t[0s / 34s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "77:\t[0s / 34s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "78:\t[0s / 35s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "79:\t[0s / 35s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "80:\t[0s / 36s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "81:\t[0s / 36s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "82:\t[0s / 36s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "83:\t[0s / 37s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "84:\t[0s / 37s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "85:\t[0s / 38s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "86:\t[0s / 38s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "87:\t[0s / 38s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "88:\t[0s / 39s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "89:\t[0s / 39s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "90:\t[0s / 40s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "91:\t[0s / 40s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "92:\t[0s / 41s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "93:\t[0s / 41s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "94:\t[0s / 41s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "95:\t[0s / 42s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "96:\t[0s / 42s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "97:\t[0s / 43s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "98:\t[0s / 43s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "99:\t[0s / 43s],\t\ttrain_loss: 4.5510,\tval_loss: 7.2501\n",
      "DeepSurv Results for batch 3:\n",
      "  DeepSurv Concordance Index: 0.0\n",
      "--------------------------------------------------\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 4.6174,\tval_loss: 7.2485\n",
      "1:\t[0s / 1s],\t\ttrain_loss: 4.5632,\tval_loss: 7.2476\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 4.5601,\tval_loss: 7.2498\n",
      "3:\t[0s / 2s],\t\ttrain_loss: 4.5560,\tval_loss: 7.2477\n",
      "4:\t[0s / 2s],\t\ttrain_loss: 4.5528,\tval_loss: 7.2479\n",
      "5:\t[0s / 2s],\t\ttrain_loss: 4.5535,\tval_loss: 7.2488\n",
      "6:\t[0s / 3s],\t\ttrain_loss: 4.5532,\tval_loss: 7.2485\n",
      "7:\t[0s / 3s],\t\ttrain_loss: 4.5518,\tval_loss: 7.2486\n",
      "8:\t[0s / 4s],\t\ttrain_loss: 4.5509,\tval_loss: 7.2493\n",
      "9:\t[0s / 4s],\t\ttrain_loss: 4.5501,\tval_loss: 7.2480\n",
      "10:\t[0s / 4s],\t\ttrain_loss: 4.5494,\tval_loss: 7.2478\n",
      "11:\t[0s / 5s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2490\n",
      "12:\t[0s / 5s],\t\ttrain_loss: 4.5494,\tval_loss: 7.2483\n",
      "13:\t[0s / 6s],\t\ttrain_loss: 4.5496,\tval_loss: 7.2490\n",
      "14:\t[0s / 6s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2482\n",
      "15:\t[0s / 7s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2480\n",
      "16:\t[0s / 7s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2490\n",
      "17:\t[0s / 7s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2485\n",
      "18:\t[0s / 8s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2481\n",
      "19:\t[0s / 8s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2489\n",
      "20:\t[0s / 9s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2484\n",
      "21:\t[0s / 9s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2483\n",
      "22:\t[0s / 10s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2479\n",
      "23:\t[0s / 10s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2497\n",
      "24:\t[0s / 10s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2487\n",
      "25:\t[0s / 11s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2482\n",
      "26:\t[0s / 12s],\t\ttrain_loss: 4.5484,\tval_loss: 7.2481\n",
      "27:\t[0s / 12s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2483\n",
      "28:\t[0s / 12s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2482\n",
      "29:\t[0s / 13s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2478\n",
      "30:\t[0s / 13s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2482\n",
      "31:\t[0s / 14s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2495\n",
      "32:\t[0s / 14s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2475\n",
      "33:\t[0s / 14s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2476\n",
      "34:\t[0s / 15s],\t\ttrain_loss: 4.5482,\tval_loss: 7.2477\n",
      "35:\t[0s / 15s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2481\n",
      "36:\t[0s / 16s],\t\ttrain_loss: 4.5483,\tval_loss: 7.2485\n",
      "37:\t[0s / 16s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2476\n",
      "38:\t[0s / 17s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2484\n",
      "39:\t[0s / 17s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2488\n",
      "40:\t[0s / 18s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2482\n",
      "41:\t[0s / 18s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2478\n",
      "42:\t[0s / 18s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2480\n",
      "43:\t[0s / 19s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2488\n",
      "44:\t[0s / 19s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2482\n",
      "45:\t[0s / 20s],\t\ttrain_loss: 4.5483,\tval_loss: 7.2478\n",
      "46:\t[0s / 20s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2478\n",
      "47:\t[0s / 21s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2480\n",
      "48:\t[0s / 21s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2477\n",
      "49:\t[0s / 22s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2486\n",
      "50:\t[0s / 22s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2476\n",
      "51:\t[0s / 23s],\t\ttrain_loss: 4.5484,\tval_loss: 7.2476\n",
      "52:\t[0s / 23s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2478\n",
      "53:\t[0s / 23s],\t\ttrain_loss: 4.5493,\tval_loss: 7.2484\n",
      "54:\t[0s / 24s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2478\n",
      "55:\t[0s / 24s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2477\n",
      "56:\t[0s / 25s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2484\n",
      "57:\t[0s / 25s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2481\n",
      "58:\t[0s / 26s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2488\n",
      "59:\t[0s / 26s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2473\n",
      "60:\t[0s / 27s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2479\n",
      "61:\t[0s / 27s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2485\n",
      "62:\t[0s / 27s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2485\n",
      "63:\t[0s / 28s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2497\n",
      "64:\t[0s / 28s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2480\n",
      "65:\t[0s / 29s],\t\ttrain_loss: 4.5483,\tval_loss: 7.2478\n",
      "66:\t[0s / 29s],\t\ttrain_loss: 4.5483,\tval_loss: 7.2476\n",
      "67:\t[0s / 29s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2478\n",
      "68:\t[0s / 30s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2478\n",
      "69:\t[0s / 30s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2482\n",
      "70:\t[0s / 31s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2487\n",
      "71:\t[0s / 31s],\t\ttrain_loss: 4.5484,\tval_loss: 7.2481\n",
      "72:\t[0s / 32s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2475\n",
      "73:\t[0s / 32s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2482\n",
      "74:\t[0s / 33s],\t\ttrain_loss: 4.5479,\tval_loss: 7.2495\n",
      "75:\t[0s / 33s],\t\ttrain_loss: 4.5515,\tval_loss: 7.2501\n",
      "76:\t[0s / 33s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "77:\t[0s / 34s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "78:\t[0s / 34s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "79:\t[0s / 35s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "80:\t[0s / 35s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "81:\t[0s / 35s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "82:\t[0s / 36s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "83:\t[0s / 36s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "84:\t[0s / 37s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "85:\t[0s / 37s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "86:\t[0s / 37s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "87:\t[0s / 38s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "88:\t[0s / 38s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "89:\t[0s / 39s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "90:\t[0s / 39s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "91:\t[0s / 40s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "92:\t[0s / 40s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "93:\t[0s / 41s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "94:\t[0s / 41s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "95:\t[0s / 41s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "96:\t[0s / 42s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "97:\t[0s / 42s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "98:\t[0s / 43s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "99:\t[0s / 43s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "DeepSurv Results for batch 4:\n",
      "  DeepSurv Concordance Index: 0.0\n",
      "--------------------------------------------------\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 4.5710,\tval_loss: 7.2482\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 4.5581,\tval_loss: 7.2561\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 4.5530,\tval_loss: 7.2468\n",
      "3:\t[0s / 1s],\t\ttrain_loss: 4.5519,\tval_loss: 7.2471\n",
      "4:\t[0s / 2s],\t\ttrain_loss: 4.5506,\tval_loss: 7.2466\n",
      "5:\t[0s / 2s],\t\ttrain_loss: 4.5505,\tval_loss: 7.2459\n",
      "6:\t[0s / 3s],\t\ttrain_loss: 4.5493,\tval_loss: 7.2474\n",
      "7:\t[0s / 3s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2478\n",
      "8:\t[0s / 3s],\t\ttrain_loss: 4.5498,\tval_loss: 7.2470\n",
      "9:\t[0s / 4s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2459\n",
      "10:\t[0s / 4s],\t\ttrain_loss: 4.5493,\tval_loss: 7.2478\n",
      "11:\t[0s / 5s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2467\n",
      "12:\t[0s / 5s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2463\n",
      "13:\t[0s / 5s],\t\ttrain_loss: 4.5496,\tval_loss: 7.2474\n",
      "14:\t[0s / 6s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2464\n",
      "15:\t[0s / 6s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2455\n",
      "16:\t[0s / 7s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2453\n",
      "17:\t[0s / 7s],\t\ttrain_loss: 4.5494,\tval_loss: 7.2452\n",
      "18:\t[0s / 8s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2459\n",
      "19:\t[0s / 8s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2468\n",
      "20:\t[0s / 8s],\t\ttrain_loss: 4.5486,\tval_loss: 7.2456\n",
      "21:\t[0s / 9s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2460\n",
      "22:\t[0s / 9s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2465\n",
      "23:\t[0s / 10s],\t\ttrain_loss: 4.5498,\tval_loss: 7.2459\n",
      "24:\t[0s / 10s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2454\n",
      "25:\t[0s / 11s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2448\n",
      "26:\t[0s / 11s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2454\n",
      "27:\t[0s / 11s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2456\n",
      "28:\t[0s / 12s],\t\ttrain_loss: 4.5485,\tval_loss: 7.2450\n",
      "29:\t[0s / 12s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2448\n",
      "30:\t[0s / 13s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2446\n",
      "31:\t[0s / 13s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2454\n",
      "32:\t[0s / 13s],\t\ttrain_loss: 4.5491,\tval_loss: 7.2448\n",
      "33:\t[0s / 14s],\t\ttrain_loss: 4.5487,\tval_loss: 7.2456\n",
      "34:\t[0s / 14s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2479\n",
      "35:\t[0s / 15s],\t\ttrain_loss: 4.5492,\tval_loss: 7.2447\n",
      "36:\t[0s / 15s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2482\n",
      "37:\t[0s / 16s],\t\ttrain_loss: 4.5490,\tval_loss: 7.2451\n",
      "38:\t[0s / 16s],\t\ttrain_loss: 4.5489,\tval_loss: 7.2452\n",
      "39:\t[0s / 16s],\t\ttrain_loss: 4.5488,\tval_loss: 7.2449\n",
      "40:\t[0s / 17s],\t\ttrain_loss: 4.5481,\tval_loss: 7.2472\n",
      "41:\t[0s / 17s],\t\ttrain_loss: 4.5508,\tval_loss: 7.2501\n",
      "42:\t[0s / 18s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "43:\t[0s / 18s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "44:\t[0s / 19s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "45:\t[0s / 19s],\t\ttrain_loss: 4.5516,\tval_loss: 7.2501\n",
      "46:\t[0s / 19s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "47:\t[0s / 20s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "48:\t[0s / 20s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "49:\t[0s / 21s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "50:\t[0s / 21s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "51:\t[0s / 22s],\t\ttrain_loss: 4.5516,\tval_loss: 7.2501\n",
      "52:\t[0s / 22s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "53:\t[0s / 22s],\t\ttrain_loss: 4.5514,\tval_loss: 7.2501\n",
      "54:\t[0s / 23s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "55:\t[0s / 23s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "56:\t[0s / 24s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "57:\t[0s / 24s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "58:\t[0s / 24s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "59:\t[0s / 25s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "60:\t[0s / 25s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "61:\t[0s / 26s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "62:\t[0s / 26s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "63:\t[0s / 26s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "64:\t[0s / 27s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "65:\t[0s / 27s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "66:\t[0s / 28s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "67:\t[0s / 28s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "68:\t[0s / 29s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "69:\t[0s / 29s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "70:\t[0s / 30s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "71:\t[0s / 30s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "72:\t[0s / 30s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "73:\t[0s / 31s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "74:\t[0s / 31s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "75:\t[0s / 32s],\t\ttrain_loss: 4.5513,\tval_loss: 7.2501\n",
      "76:\t[0s / 32s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "77:\t[0s / 33s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "78:\t[0s / 33s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "79:\t[0s / 33s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "80:\t[0s / 34s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "81:\t[0s / 34s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "82:\t[0s / 35s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "83:\t[0s / 35s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "84:\t[0s / 35s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "85:\t[0s / 36s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "86:\t[0s / 36s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "87:\t[0s / 37s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "88:\t[0s / 37s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "89:\t[0s / 38s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "90:\t[0s / 38s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "91:\t[0s / 39s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "92:\t[0s / 39s],\t\ttrain_loss: 4.5512,\tval_loss: 7.2501\n",
      "93:\t[0s / 39s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "94:\t[0s / 40s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "95:\t[0s / 40s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "96:\t[0s / 41s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "97:\t[0s / 41s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "98:\t[0s / 42s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "99:\t[0s / 42s],\t\ttrain_loss: 4.5511,\tval_loss: 7.2501\n",
      "DeepSurv Results for batch 5:\n",
      "  DeepSurv Concordance Index: 0.0\n",
      "--------------------------------------------------\n",
      "\n",
      "Final aggregated DeepSurv results:\n",
      "[{'DeepSurv_Concordance': 0.0, 'DeepSurv_Model': <pycox.models.cox.CoxPH object at 0x7498f845e420>}, {'DeepSurv_Concordance': 0.0, 'DeepSurv_Model': <pycox.models.cox.CoxPH object at 0x7498fab74bc0>}, {'DeepSurv_Concordance': 0.0, 'DeepSurv_Model': <pycox.models.cox.CoxPH object at 0x7498f88a89b0>}, {'DeepSurv_Concordance': 0.0, 'DeepSurv_Model': <pycox.models.cox.CoxPH object at 0x7498f353ac00>}, {'DeepSurv_Concordance': 0.0, 'DeepSurv_Model': <pycox.models.cox.CoxPH object at 0x7498f1ae2ba0>}]\n"
     ]
    }
   ],
   "source": [
    "deepsurv_results = []\n",
    "for i in range(N_ITERATIONS):\n",
    "    seed = BASE_RANDOM_STATE + i\n",
    "    processed_batch = get_processed_batch(seed)\n",
    "    metrics_deepsurv = train_and_evaluate_deepsurv(processed_batch)\n",
    "    deepsurv_results.append(metrics_deepsurv)\n",
    "    print(f\"DeepSurv Results for batch {i+1}:\")\n",
    "    print(f\"  DeepSurv Concordance Index: {metrics_deepsurv['DeepSurv_Concordance']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nFinal aggregated DeepSurv results:\")\n",
    "print(deepsurv_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4702.49855| train_rmse: 69.2440414428711| valid_rmse: 69.09040069580078|  0:00:10s\n",
      "epoch 1  | loss: 4201.54684| train_rmse: 66.63997650146484| valid_rmse: 66.48587799072266|  0:00:21s\n",
      "epoch 2  | loss: 3612.44385| train_rmse: 63.25355911254883| valid_rmse: 63.09770965576172|  0:00:32s\n",
      "epoch 3  | loss: 2873.85678| train_rmse: 56.47513961791992| valid_rmse: 56.28746032714844|  0:00:43s\n",
      "epoch 4  | loss: 2009.1286| train_rmse: 46.829280853271484| valid_rmse: 46.65406036376953|  0:00:54s\n",
      "epoch 5  | loss: 973.11141| train_rmse: 29.548770904541016| valid_rmse: 29.3498592376709|  0:01:05s\n",
      "epoch 6  | loss: 337.81808| train_rmse: 18.246740341186523| valid_rmse: 18.146390914916992|  0:01:16s\n",
      "epoch 7  | loss: 274.26696| train_rmse: 16.802780151367188| valid_rmse: 16.745180130004883|  0:01:27s\n",
      "epoch 8  | loss: 263.89251| train_rmse: 16.262859344482422| valid_rmse: 16.247400283813477|  0:01:38s\n",
      "epoch 9  | loss: 258.83576| train_rmse: 16.236499786376953| valid_rmse: 16.21162986755371|  0:01:49s\n",
      "epoch 10 | loss: 256.70347| train_rmse: 16.11467933654785| valid_rmse: 16.078569412231445|  0:01:59s\n",
      "epoch 11 | loss: 255.23825| train_rmse: 15.962459564208984| valid_rmse: 15.958869934082031|  0:02:10s\n",
      "epoch 12 | loss: 254.13757| train_rmse: 15.970820426940918| valid_rmse: 15.935859680175781|  0:02:21s\n",
      "epoch 13 | loss: 252.34863| train_rmse: 15.8781099319458| valid_rmse: 15.867899894714355|  0:02:33s\n",
      "epoch 14 | loss: 252.18269| train_rmse: 15.83920955657959| valid_rmse: 15.843939781188965|  0:02:43s\n",
      "epoch 15 | loss: 251.42537| train_rmse: 15.813170433044434| valid_rmse: 15.855210304260254|  0:02:55s\n",
      "epoch 16 | loss: 251.09685| train_rmse: 15.777090072631836| valid_rmse: 15.825429916381836|  0:03:06s\n",
      "epoch 17 | loss: 251.07655| train_rmse: 15.778289794921875| valid_rmse: 15.834919929504395|  0:03:17s\n",
      "epoch 18 | loss: 250.68856| train_rmse: 15.76626968383789| valid_rmse: 15.793669700622559|  0:03:28s\n",
      "epoch 19 | loss: 250.02168| train_rmse: 15.754799842834473| valid_rmse: 15.80385971069336|  0:03:38s\n",
      "epoch 20 | loss: 249.68463| train_rmse: 15.742349624633789| valid_rmse: 15.80737018585205|  0:03:49s\n",
      "epoch 21 | loss: 248.83787| train_rmse: 15.720979690551758| valid_rmse: 15.803589820861816|  0:04:00s\n",
      "epoch 22 | loss: 249.01767| train_rmse: 15.712300300598145| valid_rmse: 15.787269592285156|  0:04:11s\n",
      "epoch 23 | loss: 248.80393| train_rmse: 15.720069885253906| valid_rmse: 15.783709526062012|  0:04:22s\n",
      "epoch 24 | loss: 248.2012| train_rmse: 15.719380378723145| valid_rmse: 15.856280326843262|  0:04:33s\n",
      "epoch 25 | loss: 247.83537| train_rmse: 15.671799659729004| valid_rmse: 15.755510330200195|  0:04:44s\n",
      "epoch 26 | loss: 247.746 | train_rmse: 15.689599990844727| valid_rmse: 15.734880447387695|  0:04:55s\n",
      "epoch 27 | loss: 246.99912| train_rmse: 15.676300048828125| valid_rmse: 15.74917984008789|  0:05:06s\n",
      "epoch 28 | loss: 247.32317| train_rmse: 15.665730476379395| valid_rmse: 15.733420372009277|  0:05:17s\n",
      "epoch 29 | loss: 246.99895| train_rmse: 15.674059867858887| valid_rmse: 15.75376033782959|  0:05:28s\n",
      "epoch 30 | loss: 247.22165| train_rmse: 15.662469863891602| valid_rmse: 15.748499870300293|  0:05:39s\n",
      "epoch 31 | loss: 246.9507| train_rmse: 15.686670303344727| valid_rmse: 15.764479637145996|  0:05:50s\n",
      "epoch 32 | loss: 246.96969| train_rmse: 15.647660255432129| valid_rmse: 15.742179870605469|  0:06:01s\n",
      "epoch 33 | loss: 246.55047| train_rmse: 15.658370018005371| valid_rmse: 15.738980293273926|  0:06:12s\n",
      "epoch 34 | loss: 246.91774| train_rmse: 15.635279655456543| valid_rmse: 15.762869834899902|  0:06:23s\n",
      "epoch 35 | loss: 246.42059| train_rmse: 15.615059852600098| valid_rmse: 15.728119850158691|  0:06:34s\n",
      "epoch 36 | loss: 245.83976| train_rmse: 15.618109703063965| valid_rmse: 15.744400024414062|  0:06:45s\n",
      "epoch 37 | loss: 245.66058| train_rmse: 15.609299659729004| valid_rmse: 15.756179809570312|  0:06:55s\n",
      "epoch 38 | loss: 245.61094| train_rmse: 15.605759620666504| valid_rmse: 15.740739822387695|  0:07:06s\n",
      "epoch 39 | loss: 245.49872| train_rmse: 15.582050323486328| valid_rmse: 15.725899696350098|  0:07:17s\n",
      "epoch 40 | loss: 245.48006| train_rmse: 15.580690383911133| valid_rmse: 15.729720115661621|  0:07:28s\n",
      "epoch 41 | loss: 244.9096| train_rmse: 15.575130462646484| valid_rmse: 15.728280067443848|  0:07:39s\n",
      "epoch 42 | loss: 244.03459| train_rmse: 15.568110466003418| valid_rmse: 15.74312973022461|  0:07:50s\n",
      "epoch 43 | loss: 244.70935| train_rmse: 15.56781005859375| valid_rmse: 15.74176025390625|  0:08:01s\n",
      "epoch 44 | loss: 244.59638| train_rmse: 15.55420970916748| valid_rmse: 15.731789588928223|  0:08:12s\n",
      "epoch 45 | loss: 243.85671| train_rmse: 15.56383991241455| valid_rmse: 15.797590255737305|  0:08:22s\n",
      "epoch 46 | loss: 243.75363| train_rmse: 15.61771011352539| valid_rmse: 15.760319709777832|  0:08:33s\n",
      "epoch 47 | loss: 243.97502| train_rmse: 15.56309986114502| valid_rmse: 15.777310371398926|  0:08:44s\n",
      "epoch 48 | loss: 243.53931| train_rmse: 15.536879539489746| valid_rmse: 15.737059593200684|  0:08:55s\n",
      "epoch 49 | loss: 243.71748| train_rmse: 15.528949737548828| valid_rmse: 15.74547004699707|  0:09:06s\n",
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 39 and best_valid_rmse = 15.725899696350098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet Results for batch 1:\n",
      "  TabNet RMSE: 15.725902557373047\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4703.67998| train_rmse: 69.60569763183594| valid_rmse: 69.42298126220703|  0:00:10s\n",
      "epoch 1  | loss: 4186.87566| train_rmse: 67.3723373413086| valid_rmse: 67.18720245361328|  0:00:21s\n",
      "epoch 2  | loss: 3639.8257| train_rmse: 64.09623718261719| valid_rmse: 63.9104118347168|  0:00:32s\n",
      "epoch 3  | loss: 2936.44579| train_rmse: 57.59941864013672| valid_rmse: 57.41584014892578|  0:00:43s\n",
      "epoch 4  | loss: 1937.26119| train_rmse: 46.934688568115234| valid_rmse: 46.761661529541016|  0:00:54s\n",
      "epoch 5  | loss: 932.03984| train_rmse: 31.34058952331543| valid_rmse: 31.10984992980957|  0:01:05s\n",
      "epoch 6  | loss: 372.20659| train_rmse: 19.676210403442383| valid_rmse: 19.41880989074707|  0:01:16s\n",
      "epoch 7  | loss: 295.50802| train_rmse: 18.167409896850586| valid_rmse: 17.970420837402344|  0:01:27s\n",
      "epoch 8  | loss: 277.24501| train_rmse: 16.766029357910156| valid_rmse: 16.620349884033203|  0:01:37s\n",
      "epoch 9  | loss: 266.63737| train_rmse: 16.389989852905273| valid_rmse: 16.27566909790039|  0:01:48s\n",
      "epoch 10 | loss: 262.33999| train_rmse: 16.310260772705078| valid_rmse: 16.182769775390625|  0:01:59s\n",
      "epoch 11 | loss: 259.99866| train_rmse: 16.117530822753906| valid_rmse: 16.02920913696289|  0:02:10s\n",
      "epoch 12 | loss: 259.24403| train_rmse: 16.079389572143555| valid_rmse: 15.995699882507324|  0:02:21s\n",
      "epoch 13 | loss: 257.71184| train_rmse: 16.01116943359375| valid_rmse: 15.95127010345459|  0:02:32s\n",
      "epoch 14 | loss: 257.4243| train_rmse: 16.093820571899414| valid_rmse: 16.032649993896484|  0:02:43s\n",
      "epoch 15 | loss: 256.99901| train_rmse: 15.939499855041504| valid_rmse: 15.938380241394043|  0:02:54s\n",
      "epoch 16 | loss: 255.21594| train_rmse: 15.911370277404785| valid_rmse: 15.894060134887695|  0:03:04s\n",
      "epoch 17 | loss: 255.27072| train_rmse: 15.898099899291992| valid_rmse: 15.917360305786133|  0:03:15s\n",
      "epoch 18 | loss: 254.21782| train_rmse: 15.870969772338867| valid_rmse: 15.85103988647461|  0:03:26s\n",
      "epoch 19 | loss: 253.31306| train_rmse: 15.833680152893066| valid_rmse: 15.814840316772461|  0:03:37s\n",
      "epoch 20 | loss: 253.1812| train_rmse: 15.879429817199707| valid_rmse: 15.843040466308594|  0:03:48s\n",
      "epoch 21 | loss: 252.81422| train_rmse: 15.816140174865723| valid_rmse: 15.801380157470703|  0:03:59s\n",
      "epoch 22 | loss: 251.66437| train_rmse: 15.813300132751465| valid_rmse: 15.781949996948242|  0:04:10s\n",
      "epoch 23 | loss: 251.8606| train_rmse: 15.808899879455566| valid_rmse: 15.79736042022705|  0:04:21s\n",
      "epoch 24 | loss: 251.3933| train_rmse: 15.79008960723877| valid_rmse: 15.78600025177002|  0:04:32s\n",
      "epoch 25 | loss: 251.11739| train_rmse: 15.76671028137207| valid_rmse: 15.748700141906738|  0:04:42s\n",
      "epoch 26 | loss: 251.48078| train_rmse: 15.772910118103027| valid_rmse: 15.761260032653809|  0:04:53s\n",
      "epoch 27 | loss: 250.89437| train_rmse: 15.769740104675293| valid_rmse: 15.766690254211426|  0:05:04s\n",
      "epoch 28 | loss: 250.60283| train_rmse: 15.753669738769531| valid_rmse: 15.782990455627441|  0:05:14s\n",
      "epoch 29 | loss: 249.74326| train_rmse: 15.746489524841309| valid_rmse: 15.762649536132812|  0:05:25s\n",
      "epoch 30 | loss: 249.92327| train_rmse: 15.745140075683594| valid_rmse: 15.744319915771484|  0:05:36s\n",
      "epoch 31 | loss: 249.74742| train_rmse: 15.742300033569336| valid_rmse: 15.747830390930176|  0:05:47s\n",
      "epoch 32 | loss: 250.00312| train_rmse: 15.745479583740234| valid_rmse: 15.758580207824707|  0:05:58s\n",
      "epoch 33 | loss: 249.84123| train_rmse: 15.780579566955566| valid_rmse: 15.881159782409668|  0:06:09s\n",
      "epoch 34 | loss: 249.33854| train_rmse: 15.729700088500977| valid_rmse: 15.760669708251953|  0:06:20s\n",
      "epoch 35 | loss: 248.90884| train_rmse: 15.723540306091309| valid_rmse: 15.771340370178223|  0:06:31s\n",
      "epoch 36 | loss: 248.57876| train_rmse: 15.688460350036621| valid_rmse: 15.712479591369629|  0:06:42s\n",
      "epoch 37 | loss: 248.72896| train_rmse: 15.724220275878906| valid_rmse: 15.79382038116455|  0:06:52s\n",
      "epoch 38 | loss: 248.03016| train_rmse: 15.685770034790039| valid_rmse: 15.728389739990234|  0:07:03s\n",
      "epoch 39 | loss: 248.34167| train_rmse: 15.692970275878906| valid_rmse: 15.736339569091797|  0:07:14s\n",
      "epoch 40 | loss: 247.91479| train_rmse: 15.708979606628418| valid_rmse: 15.775710105895996|  0:07:25s\n",
      "epoch 41 | loss: 247.90246| train_rmse: 15.701359748840332| valid_rmse: 15.758020401000977|  0:07:36s\n",
      "epoch 42 | loss: 248.00381| train_rmse: 15.688030242919922| valid_rmse: 15.74431037902832|  0:07:47s\n",
      "epoch 43 | loss: 248.39627| train_rmse: 15.689120292663574| valid_rmse: 15.731900215148926|  0:07:58s\n",
      "epoch 44 | loss: 247.77061| train_rmse: 15.676139831542969| valid_rmse: 15.744070053100586|  0:08:08s\n",
      "epoch 45 | loss: 247.46003| train_rmse: 15.670220375061035| valid_rmse: 15.726059913635254|  0:08:19s\n",
      "epoch 46 | loss: 246.92767| train_rmse: 15.676910400390625| valid_rmse: 15.726469993591309|  0:08:30s\n",
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_valid_rmse = 15.712479591369629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet Results for batch 2:\n",
      "  TabNet RMSE: 15.712477684020996\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4694.17349| train_rmse: 69.32987976074219| valid_rmse: 69.46102142333984|  0:00:10s\n",
      "epoch 1  | loss: 4150.66903| train_rmse: 66.9444580078125| valid_rmse: 67.07733917236328|  0:00:21s\n",
      "epoch 2  | loss: 3568.56265| train_rmse: 62.97517013549805| valid_rmse: 63.11336898803711|  0:00:32s\n",
      "epoch 3  | loss: 2694.43461| train_rmse: 55.01005172729492| valid_rmse: 55.16476058959961|  0:00:43s\n",
      "epoch 4  | loss: 1636.50753| train_rmse: 43.15644073486328| valid_rmse: 43.30733871459961|  0:00:54s\n",
      "epoch 5  | loss: 675.51798| train_rmse: 26.065689086914062| valid_rmse: 26.243730545043945|  0:01:04s\n",
      "epoch 6  | loss: 312.54431| train_rmse: 17.84300994873047| valid_rmse: 18.028860092163086|  0:01:15s\n",
      "epoch 7  | loss: 274.59765| train_rmse: 16.94028091430664| valid_rmse: 17.100439071655273|  0:01:26s\n",
      "epoch 8  | loss: 263.81314| train_rmse: 16.489580154418945| valid_rmse: 16.68147087097168|  0:01:37s\n",
      "epoch 9  | loss: 258.28864| train_rmse: 16.164430618286133| valid_rmse: 16.341350555419922|  0:01:48s\n",
      "epoch 10 | loss: 254.18861| train_rmse: 16.123300552368164| valid_rmse: 16.266969680786133|  0:01:58s\n",
      "epoch 11 | loss: 252.58341| train_rmse: 15.895110130310059| valid_rmse: 16.060020446777344|  0:02:09s\n",
      "epoch 12 | loss: 250.12355| train_rmse: 15.804180145263672| valid_rmse: 15.976940155029297|  0:02:20s\n",
      "epoch 13 | loss: 249.51142| train_rmse: 15.729920387268066| valid_rmse: 15.914139747619629|  0:02:31s\n",
      "epoch 14 | loss: 248.40285| train_rmse: 15.734649658203125| valid_rmse: 15.921819686889648|  0:02:42s\n",
      "epoch 15 | loss: 247.63261| train_rmse: 15.702659606933594| valid_rmse: 15.903989791870117|  0:02:52s\n",
      "epoch 16 | loss: 246.6617| train_rmse: 15.665780067443848| valid_rmse: 15.857049942016602|  0:03:03s\n",
      "epoch 17 | loss: 246.89869| train_rmse: 15.636619567871094| valid_rmse: 15.817500114440918|  0:03:14s\n",
      "epoch 18 | loss: 245.96963| train_rmse: 15.611260414123535| valid_rmse: 15.806659698486328|  0:03:25s\n",
      "epoch 19 | loss: 246.94217| train_rmse: 15.601019859313965| valid_rmse: 15.797300338745117|  0:03:35s\n",
      "epoch 20 | loss: 245.69912| train_rmse: 15.622420310974121| valid_rmse: 15.817469596862793|  0:03:46s\n",
      "epoch 21 | loss: 245.22368| train_rmse: 15.613140106201172| valid_rmse: 15.903200149536133|  0:03:58s\n",
      "epoch 22 | loss: 245.37751| train_rmse: 15.592619895935059| valid_rmse: 15.790590286254883|  0:04:09s\n",
      "epoch 23 | loss: 245.44919| train_rmse: 15.601469993591309| valid_rmse: 15.808170318603516|  0:04:20s\n",
      "epoch 24 | loss: 244.96117| train_rmse: 15.642040252685547| valid_rmse: 15.8645601272583|  0:04:31s\n",
      "epoch 25 | loss: 244.87019| train_rmse: 15.602009773254395| valid_rmse: 15.792590141296387|  0:04:42s\n",
      "epoch 26 | loss: 244.85266| train_rmse: 15.574509620666504| valid_rmse: 15.785240173339844|  0:04:53s\n",
      "epoch 27 | loss: 244.48426| train_rmse: 15.585089683532715| valid_rmse: 15.775569915771484|  0:05:03s\n",
      "epoch 28 | loss: 244.59634| train_rmse: 15.592989921569824| valid_rmse: 15.805560111999512|  0:05:14s\n",
      "epoch 29 | loss: 243.7681| train_rmse: 15.586600303649902| valid_rmse: 15.789380073547363|  0:05:25s\n",
      "epoch 30 | loss: 243.74495| train_rmse: 15.57252025604248| valid_rmse: 15.819000244140625|  0:05:36s\n",
      "epoch 31 | loss: 243.76949| train_rmse: 15.547920227050781| valid_rmse: 15.794099807739258|  0:05:47s\n",
      "epoch 32 | loss: 243.39952| train_rmse: 15.544830322265625| valid_rmse: 15.799989700317383|  0:05:58s\n",
      "epoch 33 | loss: 242.89883| train_rmse: 15.539290428161621| valid_rmse: 15.790539741516113|  0:06:09s\n",
      "epoch 34 | loss: 242.78383| train_rmse: 15.502690315246582| valid_rmse: 15.791080474853516|  0:06:20s\n",
      "epoch 35 | loss: 242.49555| train_rmse: 15.507439613342285| valid_rmse: 15.79638957977295|  0:06:30s\n",
      "epoch 36 | loss: 242.13455| train_rmse: 15.505419731140137| valid_rmse: 15.841650009155273|  0:06:41s\n",
      "epoch 37 | loss: 242.19364| train_rmse: 15.509639739990234| valid_rmse: 15.8372802734375|  0:06:52s\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_valid_rmse = 15.775569915771484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet Results for batch 3:\n",
      "  TabNet RMSE: 15.775568008422852\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4725.027| train_rmse: 69.4205093383789| valid_rmse: 69.01531219482422|  0:00:10s\n",
      "epoch 1  | loss: 4199.77499| train_rmse: 67.3227767944336| valid_rmse: 66.91776275634766|  0:00:21s\n",
      "epoch 2  | loss: 3653.50096| train_rmse: 63.66453170776367| valid_rmse: 63.253761291503906|  0:00:32s\n",
      "epoch 3  | loss: 2836.76145| train_rmse: 55.75054168701172| valid_rmse: 55.340118408203125|  0:00:43s\n",
      "epoch 4  | loss: 1812.49863| train_rmse: 44.72005081176758| valid_rmse: 44.32789993286133|  0:00:54s\n",
      "epoch 5  | loss: 749.84942| train_rmse: 26.152469635009766| valid_rmse: 25.838420867919922|  0:01:04s\n",
      "epoch 6  | loss: 304.05809| train_rmse: 18.226179122924805| valid_rmse: 18.144100189208984|  0:01:15s\n",
      "epoch 7  | loss: 279.39317| train_rmse: 17.19194984436035| valid_rmse: 17.219390869140625|  0:01:26s\n",
      "epoch 8  | loss: 267.48512| train_rmse: 16.652000427246094| valid_rmse: 16.746280670166016|  0:01:37s\n",
      "epoch 9  | loss: 260.7567| train_rmse: 16.17350959777832| valid_rmse: 16.299589157104492|  0:01:48s\n",
      "epoch 10 | loss: 257.50223| train_rmse: 16.149580001831055| valid_rmse: 16.293190002441406|  0:01:59s\n",
      "epoch 11 | loss: 255.32831| train_rmse: 16.000959396362305| valid_rmse: 16.1535701751709|  0:02:10s\n",
      "epoch 12 | loss: 255.67362| train_rmse: 15.909629821777344| valid_rmse: 16.092870712280273|  0:02:20s\n",
      "epoch 13 | loss: 253.9226| train_rmse: 15.875679969787598| valid_rmse: 16.09963035583496|  0:02:31s\n",
      "epoch 14 | loss: 253.40902| train_rmse: 15.867589950561523| valid_rmse: 16.116470336914062|  0:02:42s\n",
      "epoch 15 | loss: 252.73788| train_rmse: 15.832070350646973| valid_rmse: 16.03253936767578|  0:02:53s\n",
      "epoch 16 | loss: 251.96738| train_rmse: 15.802769660949707| valid_rmse: 16.036169052124023|  0:03:04s\n",
      "epoch 17 | loss: 251.72598| train_rmse: 15.829580307006836| valid_rmse: 16.077550888061523|  0:03:15s\n",
      "epoch 18 | loss: 251.69492| train_rmse: 15.785490036010742| valid_rmse: 16.079330444335938|  0:03:25s\n",
      "epoch 19 | loss: 251.49872| train_rmse: 15.757619857788086| valid_rmse: 16.03508949279785|  0:03:36s\n",
      "epoch 20 | loss: 251.13665| train_rmse: 15.779370307922363| valid_rmse: 16.093669891357422|  0:03:47s\n",
      "epoch 21 | loss: 250.55852| train_rmse: 15.777059555053711| valid_rmse: 16.10494041442871|  0:03:58s\n",
      "epoch 22 | loss: 250.47334| train_rmse: 15.746009826660156| valid_rmse: 16.04768943786621|  0:04:09s\n",
      "epoch 23 | loss: 249.82462| train_rmse: 15.748530387878418| valid_rmse: 16.045469284057617|  0:04:20s\n",
      "epoch 24 | loss: 249.83383| train_rmse: 15.78264045715332| valid_rmse: 16.112590789794922|  0:04:30s\n",
      "epoch 25 | loss: 249.4553| train_rmse: 15.759690284729004| valid_rmse: 16.10590934753418|  0:04:41s\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_valid_rmse = 16.03253936767578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet Results for batch 4:\n",
      "  TabNet RMSE: 16.03253936767578\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4703.59606| train_rmse: 69.30017852783203| valid_rmse: 69.27725982666016|  0:00:10s\n",
      "epoch 1  | loss: 4207.1515| train_rmse: 67.06858825683594| valid_rmse: 67.04049682617188|  0:00:21s\n",
      "epoch 2  | loss: 3632.07621| train_rmse: 63.89720153808594| valid_rmse: 63.85103988647461|  0:00:32s\n",
      "epoch 3  | loss: 2984.30748| train_rmse: 58.691131591796875| valid_rmse: 58.61349868774414|  0:00:43s\n",
      "epoch 4  | loss: 2158.72466| train_rmse: 49.08045959472656| valid_rmse: 48.95122146606445|  0:00:54s\n",
      "epoch 5  | loss: 1064.79223| train_rmse: 31.603580474853516| valid_rmse: 31.435489654541016|  0:01:05s\n",
      "epoch 6  | loss: 370.88373| train_rmse: 18.954980850219727| valid_rmse: 18.762399673461914|  0:01:16s\n",
      "epoch 7  | loss: 291.6201| train_rmse: 17.164770126342773| valid_rmse: 17.054410934448242|  0:01:26s\n",
      "epoch 8  | loss: 272.49225| train_rmse: 16.460840225219727| valid_rmse: 16.334239959716797|  0:01:37s\n",
      "epoch 9  | loss: 262.2008| train_rmse: 16.21870994567871| valid_rmse: 16.038490295410156|  0:01:48s\n",
      "epoch 10 | loss: 259.42806| train_rmse: 16.06987953186035| valid_rmse: 15.891039848327637|  0:02:00s\n",
      "epoch 11 | loss: 256.41133| train_rmse: 16.041770935058594| valid_rmse: 15.864109992980957|  0:02:10s\n",
      "epoch 12 | loss: 256.78209| train_rmse: 16.05381965637207| valid_rmse: 15.896499633789062|  0:02:21s\n",
      "epoch 13 | loss: 255.89953| train_rmse: 15.909560203552246| valid_rmse: 15.754650115966797|  0:02:32s\n",
      "epoch 14 | loss: 254.62857| train_rmse: 15.86635971069336| valid_rmse: 15.714249610900879|  0:02:43s\n",
      "epoch 15 | loss: 252.97376| train_rmse: 15.89363956451416| valid_rmse: 15.71222972869873|  0:02:54s\n",
      "epoch 16 | loss: 253.032 | train_rmse: 15.844380378723145| valid_rmse: 15.696629524230957|  0:03:05s\n",
      "epoch 17 | loss: 252.61207| train_rmse: 15.872739791870117| valid_rmse: 15.71422004699707|  0:03:15s\n",
      "epoch 18 | loss: 252.68678| train_rmse: 15.913260459899902| valid_rmse: 15.766249656677246|  0:03:27s\n",
      "epoch 19 | loss: 252.10089| train_rmse: 15.822420120239258| valid_rmse: 15.634690284729004|  0:03:37s\n",
      "epoch 20 | loss: 251.32077| train_rmse: 15.91963005065918| valid_rmse: 15.695159912109375|  0:03:48s\n",
      "epoch 21 | loss: 251.37092| train_rmse: 15.805130004882812| valid_rmse: 15.639619827270508|  0:03:59s\n",
      "epoch 22 | loss: 250.79852| train_rmse: 15.78827953338623| valid_rmse: 15.661470413208008|  0:04:10s\n",
      "epoch 23 | loss: 250.16204| train_rmse: 15.773099899291992| valid_rmse: 15.606980323791504|  0:04:21s\n",
      "epoch 24 | loss: 250.73794| train_rmse: 15.732170104980469| valid_rmse: 15.628840446472168|  0:04:31s\n",
      "epoch 25 | loss: 250.07351| train_rmse: 15.78419017791748| valid_rmse: 15.75432014465332|  0:04:42s\n",
      "epoch 26 | loss: 249.49673| train_rmse: 15.725069999694824| valid_rmse: 15.645210266113281|  0:04:53s\n",
      "epoch 27 | loss: 249.62568| train_rmse: 15.745100021362305| valid_rmse: 15.65447998046875|  0:05:04s\n",
      "epoch 28 | loss: 249.0097| train_rmse: 15.764260292053223| valid_rmse: 15.639470100402832|  0:05:15s\n",
      "epoch 29 | loss: 248.95733| train_rmse: 15.757439613342285| valid_rmse: 15.655070304870605|  0:05:26s\n",
      "epoch 30 | loss: 248.98274| train_rmse: 15.723600387573242| valid_rmse: 15.599510192871094|  0:05:37s\n",
      "epoch 31 | loss: 248.6986| train_rmse: 15.691760063171387| valid_rmse: 15.58965015411377|  0:05:47s\n",
      "epoch 32 | loss: 248.10019| train_rmse: 15.71265983581543| valid_rmse: 15.650030136108398|  0:05:58s\n",
      "epoch 33 | loss: 247.87302| train_rmse: 15.69629955291748| valid_rmse: 15.61635971069336|  0:06:09s\n",
      "epoch 34 | loss: 247.65742| train_rmse: 15.71802043914795| valid_rmse: 15.699999809265137|  0:06:20s\n",
      "epoch 35 | loss: 247.81645| train_rmse: 15.699430465698242| valid_rmse: 15.634900093078613|  0:06:31s\n",
      "epoch 36 | loss: 247.33881| train_rmse: 15.700200080871582| valid_rmse: 15.634650230407715|  0:06:42s\n",
      "epoch 37 | loss: 247.42827| train_rmse: 15.6868896484375| valid_rmse: 15.627320289611816|  0:06:53s\n",
      "epoch 38 | loss: 246.481 | train_rmse: 15.695150375366211| valid_rmse: 15.672590255737305|  0:07:04s\n",
      "epoch 39 | loss: 246.94244| train_rmse: 15.736499786376953| valid_rmse: 15.696829795837402|  0:07:15s\n",
      "epoch 40 | loss: 246.96832| train_rmse: 15.709159851074219| valid_rmse: 15.672280311584473|  0:07:26s\n",
      "epoch 41 | loss: 246.80928| train_rmse: 15.736490249633789| valid_rmse: 15.695460319519043|  0:07:37s\n",
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_valid_rmse = 15.58965015411377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet Results for batch 5:\n",
      "  TabNet RMSE: 15.589649200439453\n",
      "--------------------------------------------------\n",
      "\n",
      "Final aggregated TabNet results:\n",
      "[{'TabNet_RMSE': np.float32(15.725903), 'TabNet_Model': TabNetRegressor(n_d=64, n_a=64, n_steps=5, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=[], n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.001, seed=0, clip_value=1, verbose=1, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.001, 'weight_decay': 1e-05}, scheduler_fn=<class 'torch.optim.lr_scheduler.StepLR'>, scheduler_params={'step_size': 50, 'gamma': 0.9}, mask_type='entmax', input_dim=106, output_dim=1, device_name='auto', n_shared_decoder=1, n_indep_decoder=1, grouped_features=[])}, {'TabNet_RMSE': np.float32(15.712478), 'TabNet_Model': TabNetRegressor(n_d=64, n_a=64, n_steps=5, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=[], n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.001, seed=0, clip_value=1, verbose=1, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.001, 'weight_decay': 1e-05}, scheduler_fn=<class 'torch.optim.lr_scheduler.StepLR'>, scheduler_params={'step_size': 50, 'gamma': 0.9}, mask_type='entmax', input_dim=106, output_dim=1, device_name='auto', n_shared_decoder=1, n_indep_decoder=1, grouped_features=[])}, {'TabNet_RMSE': np.float32(15.775568), 'TabNet_Model': TabNetRegressor(n_d=64, n_a=64, n_steps=5, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=[], n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.001, seed=0, clip_value=1, verbose=1, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.001, 'weight_decay': 1e-05}, scheduler_fn=<class 'torch.optim.lr_scheduler.StepLR'>, scheduler_params={'step_size': 50, 'gamma': 0.9}, mask_type='entmax', input_dim=106, output_dim=1, device_name='auto', n_shared_decoder=1, n_indep_decoder=1, grouped_features=[])}, {'TabNet_RMSE': np.float32(16.03254), 'TabNet_Model': TabNetRegressor(n_d=64, n_a=64, n_steps=5, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=[], n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.001, seed=0, clip_value=1, verbose=1, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.001, 'weight_decay': 1e-05}, scheduler_fn=<class 'torch.optim.lr_scheduler.StepLR'>, scheduler_params={'step_size': 50, 'gamma': 0.9}, mask_type='entmax', input_dim=106, output_dim=1, device_name='auto', n_shared_decoder=1, n_indep_decoder=1, grouped_features=[])}, {'TabNet_RMSE': np.float32(15.589649), 'TabNet_Model': TabNetRegressor(n_d=64, n_a=64, n_steps=5, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=[], n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.001, seed=0, clip_value=1, verbose=1, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.001, 'weight_decay': 1e-05}, scheduler_fn=<class 'torch.optim.lr_scheduler.StepLR'>, scheduler_params={'step_size': 50, 'gamma': 0.9}, mask_type='entmax', input_dim=106, output_dim=1, device_name='auto', n_shared_decoder=1, n_indep_decoder=1, grouped_features=[])}]\n"
     ]
    }
   ],
   "source": [
    "tabnet_results = []\n",
    "for i in range(N_ITERATIONS):\n",
    "    seed = BASE_RANDOM_STATE + i\n",
    "    processed_batch = get_processed_batch(seed)\n",
    "    metrics_tabnet = train_and_evaluate_tabnet(processed_batch)\n",
    "    tabnet_results.append(metrics_tabnet)\n",
    "    print(f\"TabNet Results for batch {i+1}:\")\n",
    "    print(f\"  TabNet RMSE: {metrics_tabnet['TabNet_RMSE']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nFinal aggregated TabNet results:\")\n",
    "print(tabnet_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EBM Results for batch 1:\n",
      "  EBM RMSE: 15.646273252062988\n",
      "  EBM MAE: 12.264317407795623\n",
      "  EBM Global Importance:\n",
      "<interpret.glassbox._ebm._ebm.EBMExplanation object at 0x7498f23441a0>\n",
      "--------------------------------------------------\n",
      "EBM Results for batch 2:\n",
      "  EBM RMSE: 15.643480985579112\n",
      "  EBM MAE: 12.290968350439183\n",
      "  EBM Global Importance:\n",
      "<interpret.glassbox._ebm._ebm.EBMExplanation object at 0x7498fb83fe60>\n",
      "--------------------------------------------------\n",
      "EBM Results for batch 3:\n",
      "  EBM RMSE: 15.608702155144194\n",
      "  EBM MAE: 12.30231829810554\n",
      "  EBM Global Importance:\n",
      "<interpret.glassbox._ebm._ebm.EBMExplanation object at 0x7498f29fc1a0>\n",
      "--------------------------------------------------\n",
      "EBM Results for batch 4:\n",
      "  EBM RMSE: 15.8947362469418\n",
      "  EBM MAE: 12.421365897736841\n",
      "  EBM Global Importance:\n",
      "<interpret.glassbox._ebm._ebm.EBMExplanation object at 0x7498f88f80e0>\n",
      "--------------------------------------------------\n",
      "EBM Results for batch 5:\n",
      "  EBM RMSE: 15.522592774815852\n",
      "  EBM MAE: 12.168442685372431\n",
      "  EBM Global Importance:\n",
      "<interpret.glassbox._ebm._ebm.EBMExplanation object at 0x7498f2914740>\n",
      "--------------------------------------------------\n",
      "\n",
      "Final aggregated EBM results:\n",
      "[{'EBM_RMSE': np.float64(15.646273252062988), 'EBM_MAE': np.float64(12.264317407795623), 'EBM_Global_Importance': <interpret.glassbox._ebm._ebm.EBMExplanation object at 0x7498f23441a0>}, {'EBM_RMSE': np.float64(15.643480985579112), 'EBM_MAE': np.float64(12.290968350439183), 'EBM_Global_Importance': <interpret.glassbox._ebm._ebm.EBMExplanation object at 0x7498fb83fe60>}, {'EBM_RMSE': np.float64(15.608702155144194), 'EBM_MAE': np.float64(12.30231829810554), 'EBM_Global_Importance': <interpret.glassbox._ebm._ebm.EBMExplanation object at 0x7498f29fc1a0>}, {'EBM_RMSE': np.float64(15.8947362469418), 'EBM_MAE': np.float64(12.421365897736841), 'EBM_Global_Importance': <interpret.glassbox._ebm._ebm.EBMExplanation object at 0x7498f88f80e0>}, {'EBM_RMSE': np.float64(15.522592774815852), 'EBM_MAE': np.float64(12.168442685372431), 'EBM_Global_Importance': <interpret.glassbox._ebm._ebm.EBMExplanation object at 0x7498f2914740>}]\n"
     ]
    }
   ],
   "source": [
    "ebm_results = []\n",
    "for i in range(N_ITERATIONS):\n",
    "    seed = BASE_RANDOM_STATE + i\n",
    "    # Get a processed mini-batch\n",
    "    processed_batch = get_processed_batch(seed)\n",
    "    # Train and evaluate the EBM model on this batch\n",
    "    metrics_ebm = train_and_evaluate_ebm(processed_batch)\n",
    "    ebm_results.append(metrics_ebm)\n",
    "    \n",
    "    print(f\"EBM Results for batch {i+1}:\")\n",
    "    print(f\"  EBM RMSE: {metrics_ebm['EBM_RMSE']}\")\n",
    "    print(f\"  EBM MAE: {metrics_ebm['EBM_MAE']}\")\n",
    "    print(\"  EBM Global Importance:\")\n",
    "    print(metrics_ebm['EBM_Global_Importance'])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nFinal aggregated EBM results:\")\n",
    "print(ebm_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6181\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 69.298800\n",
      "LightGBM Results for batch 1:\n",
      "  LGBM RMSE: 15.648079999294255\n",
      "  LGBM MAE: 12.272282070625188\n",
      "  LGBM SHAP Feature Importance:\n",
      "                feature  shap_importance\n",
      "3          smoking_prev         1.692942\n",
      "1               avg_bmi         1.304523\n",
      "4       country_Alcohol         0.663493\n",
      "103            tfidf_97         0.656056\n",
      "2    heart_disease_risk         0.315724\n",
      "..                  ...              ...\n",
      "22             tfidf_16         0.000000\n",
      "9               tfidf_3         0.000000\n",
      "37             tfidf_31         0.000000\n",
      "87             tfidf_81         0.000000\n",
      "92             tfidf_86         0.000000\n",
      "\n",
      "[106 rows x 2 columns]\n",
      "--------------------------------------------------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6316\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 69.336550\n",
      "LightGBM Results for batch 2:\n",
      "  LGBM RMSE: 15.6488985823241\n",
      "  LGBM MAE: 12.287695398387969\n",
      "  LGBM SHAP Feature Importance:\n",
      "             feature  shap_importance\n",
      "1            avg_bmi         1.623732\n",
      "3       smoking_prev         1.593027\n",
      "4    country_Alcohol         0.643307\n",
      "103         tfidf_97         0.636145\n",
      "73          tfidf_67         0.311213\n",
      "..               ...              ...\n",
      "36          tfidf_30         0.000000\n",
      "52          tfidf_46         0.000000\n",
      "54          tfidf_48         0.000000\n",
      "84          tfidf_78         0.000000\n",
      "81          tfidf_75         0.000000\n",
      "\n",
      "[106 rows x 2 columns]\n",
      "--------------------------------------------------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6227\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 69.279750\n",
      "LightGBM Results for batch 3:\n",
      "  LGBM RMSE: 15.583237994790512\n",
      "  LGBM MAE: 12.29192698893458\n",
      "  LGBM SHAP Feature Importance:\n",
      "             feature  shap_importance\n",
      "3       smoking_prev         1.669437\n",
      "1            avg_bmi         1.172770\n",
      "103         tfidf_97         0.697017\n",
      "4    country_Alcohol         0.640212\n",
      "65          tfidf_59         0.365703\n",
      "..               ...              ...\n",
      "23          tfidf_17         0.000343\n",
      "36          tfidf_30         0.000000\n",
      "9            tfidf_3         0.000000\n",
      "87          tfidf_81         0.000000\n",
      "84          tfidf_78         0.000000\n",
      "\n",
      "[106 rows x 2 columns]\n",
      "--------------------------------------------------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6121\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 69.374475\n",
      "LightGBM Results for batch 4:\n",
      "  LGBM RMSE: 15.883069130735215\n",
      "  LGBM MAE: 12.39613782898784\n",
      "  LGBM SHAP Feature Importance:\n",
      "             feature  shap_importance\n",
      "3       smoking_prev         1.690676\n",
      "1            avg_bmi         1.338360\n",
      "103         tfidf_97         0.726470\n",
      "4    country_Alcohol         0.618073\n",
      "66          tfidf_60         0.241317\n",
      "..               ...              ...\n",
      "53          tfidf_47         0.000000\n",
      "71          tfidf_65         0.000000\n",
      "88          tfidf_82         0.000000\n",
      "80          tfidf_74         0.000000\n",
      "81          tfidf_75         0.000000\n",
      "\n",
      "[106 rows x 2 columns]\n",
      "--------------------------------------------------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6329\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 69.282625\n",
      "LightGBM Results for batch 5:\n",
      "  LGBM RMSE: 15.511480802519966\n",
      "  LGBM MAE: 12.164381494333979\n",
      "  LGBM SHAP Feature Importance:\n",
      "             feature  shap_importance\n",
      "3       smoking_prev         1.649906\n",
      "1            avg_bmi         1.405557\n",
      "4    country_Alcohol         0.640180\n",
      "103         tfidf_97         0.578583\n",
      "67          tfidf_61         0.321046\n",
      "..               ...              ...\n",
      "22          tfidf_16         0.000000\n",
      "72          tfidf_66         0.000000\n",
      "54          tfidf_48         0.000000\n",
      "83          tfidf_77         0.000000\n",
      "91          tfidf_85         0.000000\n",
      "\n",
      "[106 rows x 2 columns]\n",
      "--------------------------------------------------\n",
      "\n",
      "Final aggregated LightGBM results:\n",
      "[{'LGBM_RMSE': np.float64(15.648079999294255), 'LGBM_MAE': np.float64(12.272282070625188), 'LGBM_SHAP_Importance':                 feature  shap_importance\n",
      "3          smoking_prev         1.692942\n",
      "1               avg_bmi         1.304523\n",
      "4       country_Alcohol         0.663493\n",
      "103            tfidf_97         0.656056\n",
      "2    heart_disease_risk         0.315724\n",
      "..                  ...              ...\n",
      "22             tfidf_16         0.000000\n",
      "9               tfidf_3         0.000000\n",
      "37             tfidf_31         0.000000\n",
      "87             tfidf_81         0.000000\n",
      "92             tfidf_86         0.000000\n",
      "\n",
      "[106 rows x 2 columns]}, {'LGBM_RMSE': np.float64(15.6488985823241), 'LGBM_MAE': np.float64(12.287695398387969), 'LGBM_SHAP_Importance':              feature  shap_importance\n",
      "1            avg_bmi         1.623732\n",
      "3       smoking_prev         1.593027\n",
      "4    country_Alcohol         0.643307\n",
      "103         tfidf_97         0.636145\n",
      "73          tfidf_67         0.311213\n",
      "..               ...              ...\n",
      "36          tfidf_30         0.000000\n",
      "52          tfidf_46         0.000000\n",
      "54          tfidf_48         0.000000\n",
      "84          tfidf_78         0.000000\n",
      "81          tfidf_75         0.000000\n",
      "\n",
      "[106 rows x 2 columns]}, {'LGBM_RMSE': np.float64(15.583237994790512), 'LGBM_MAE': np.float64(12.29192698893458), 'LGBM_SHAP_Importance':              feature  shap_importance\n",
      "3       smoking_prev         1.669437\n",
      "1            avg_bmi         1.172770\n",
      "103         tfidf_97         0.697017\n",
      "4    country_Alcohol         0.640212\n",
      "65          tfidf_59         0.365703\n",
      "..               ...              ...\n",
      "23          tfidf_17         0.000343\n",
      "36          tfidf_30         0.000000\n",
      "9            tfidf_3         0.000000\n",
      "87          tfidf_81         0.000000\n",
      "84          tfidf_78         0.000000\n",
      "\n",
      "[106 rows x 2 columns]}, {'LGBM_RMSE': np.float64(15.883069130735215), 'LGBM_MAE': np.float64(12.39613782898784), 'LGBM_SHAP_Importance':              feature  shap_importance\n",
      "3       smoking_prev         1.690676\n",
      "1            avg_bmi         1.338360\n",
      "103         tfidf_97         0.726470\n",
      "4    country_Alcohol         0.618073\n",
      "66          tfidf_60         0.241317\n",
      "..               ...              ...\n",
      "53          tfidf_47         0.000000\n",
      "71          tfidf_65         0.000000\n",
      "88          tfidf_82         0.000000\n",
      "80          tfidf_74         0.000000\n",
      "81          tfidf_75         0.000000\n",
      "\n",
      "[106 rows x 2 columns]}, {'LGBM_RMSE': np.float64(15.511480802519966), 'LGBM_MAE': np.float64(12.164381494333979), 'LGBM_SHAP_Importance':              feature  shap_importance\n",
      "3       smoking_prev         1.649906\n",
      "1            avg_bmi         1.405557\n",
      "4    country_Alcohol         0.640180\n",
      "103         tfidf_97         0.578583\n",
      "67          tfidf_61         0.321046\n",
      "..               ...              ...\n",
      "22          tfidf_16         0.000000\n",
      "72          tfidf_66         0.000000\n",
      "54          tfidf_48         0.000000\n",
      "83          tfidf_77         0.000000\n",
      "91          tfidf_85         0.000000\n",
      "\n",
      "[106 rows x 2 columns]}]\n"
     ]
    }
   ],
   "source": [
    "lgbm_results = []\n",
    "for i in range(N_ITERATIONS):\n",
    "    seed = BASE_RANDOM_STATE + i\n",
    "    # Get a processed mini-batch\n",
    "    processed_batch = get_processed_batch(seed)\n",
    "    # Train and evaluate the LightGBM model with SHAP on this batch\n",
    "    metrics_lgbm = train_and_evaluate_lgbm_shap(processed_batch)\n",
    "    lgbm_results.append(metrics_lgbm)\n",
    "    \n",
    "    print(f\"LightGBM Results for batch {i+1}:\")\n",
    "    print(f\"  LGBM RMSE: {metrics_lgbm['LGBM_RMSE']}\")\n",
    "    print(f\"  LGBM MAE: {metrics_lgbm['LGBM_MAE']}\")\n",
    "    print(\"  LGBM SHAP Feature Importance:\")\n",
    "    print(metrics_lgbm['LGBM_SHAP_Importance'])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nFinal aggregated LightGBM results:\")\n",
    "print(lgbm_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NODE Results for batch 1:\n",
      "  NODE RMSE: 36.39453887939453\n",
      "  NODE MAE: 27.034534454345703\n",
      "--------------------------------------------------\n",
      "NODE Results for batch 2:\n",
      "  NODE RMSE: 36.5145149230957\n",
      "  NODE MAE: 27.120515823364258\n",
      "--------------------------------------------------\n",
      "NODE Results for batch 3:\n",
      "  NODE RMSE: 36.43730926513672\n",
      "  NODE MAE: 27.181415557861328\n",
      "--------------------------------------------------\n",
      "NODE Results for batch 4:\n",
      "  NODE RMSE: 36.799781799316406\n",
      "  NODE MAE: 27.504575729370117\n",
      "--------------------------------------------------\n",
      "NODE Results for batch 5:\n",
      "  NODE RMSE: 37.06364440917969\n",
      "  NODE MAE: 27.574878692626953\n",
      "--------------------------------------------------\n",
      "\n",
      "Final aggregated NODE results:\n",
      "[{'NODE_RMSE': np.float32(36.39454), 'NODE_MAE': np.float32(27.034534), 'NODE_Model': SimpleNODE(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=106, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")}, {'NODE_RMSE': np.float32(36.514515), 'NODE_MAE': np.float32(27.120516), 'NODE_Model': SimpleNODE(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=106, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")}, {'NODE_RMSE': np.float32(36.43731), 'NODE_MAE': np.float32(27.181416), 'NODE_Model': SimpleNODE(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=106, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")}, {'NODE_RMSE': np.float32(36.79978), 'NODE_MAE': np.float32(27.504576), 'NODE_Model': SimpleNODE(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=106, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")}, {'NODE_RMSE': np.float32(37.063644), 'NODE_MAE': np.float32(27.574879), 'NODE_Model': SimpleNODE(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=106, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")}]\n"
     ]
    }
   ],
   "source": [
    "node_results = []\n",
    "for i in range(N_ITERATIONS):\n",
    "    seed = BASE_RANDOM_STATE + i\n",
    "    # Get a processed mini-batch\n",
    "    processed_batch = get_processed_batch(seed)\n",
    "    # Train and evaluate the NODE model on this batch\n",
    "    metrics_node = train_and_evaluate_node(processed_batch)\n",
    "    node_results.append(metrics_node)\n",
    "    \n",
    "    print(f\"NODE Results for batch {i+1}:\")\n",
    "    print(f\"  NODE RMSE: {metrics_node['NODE_RMSE']}\")\n",
    "    print(f\"  NODE MAE: {metrics_node['NODE_MAE']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nFinal aggregated NODE results:\")\n",
    "print(node_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
